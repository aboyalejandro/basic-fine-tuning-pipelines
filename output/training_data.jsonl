{"messages": [{"role": "user", "content": "Write a technical overview of emerging data tools and their impact on data engineering workflows."}, {"role": "assistant", "content": "I have been working with Snowflake, dbt, PostgreSQL, Python, Airflow, and AWS tools for approximately 3 years.The most challenging part of this journey has been always feeling as though I was falling behind.The list (and FOMO) never ends:If it is not DuckDB, it is Polars. If it is not S3 conditional writes, it is a new API release by OpenAI to build pipelines with text-to-speech endpoints.Starting this journey, you are learning the basics of SQL and Python, while &#8220;the big bosses&#8221; discuss topics such as the competition between Databricks and Snowflake.It is easy to feel as though you have missed the season finale of a Netflix series that everyone is discussing.I will outline my reflections on what is important after all these years, along with my top pieces of advice for beginners, as well as for anyone who wants to thrive in this field.GPT asked me to give credits for this image.&#128218; Learn Timeless Skills: Python, SQL, and Data ModelingI have met (and at times been) many people who know a great deal about Pandas and dbt tricks, but they lack the ability to write a WINDOW FUNCTION in SQL or scale reusable code with Python classes.Trendy topics often overlap with fundamental concepts, which are essential to master if you want to remain adaptable in the field!Jeff Bezos offers a quote that underscores this idea: &#8220;Focus on the things that never change; people will always want the best price and fast shipment.&#8221;What can you do with these things that never change?Python: Become proficient in data engineering patterns, such as ETL flows, but also explore creative projects, like building APIs or applications unrelated to data engineering. This approach can shift your perspective from being a simple pipeline owner to a data product builder.SQL: If you have a query that takes five minutes to process 10k rows, try optimising it. Rather than focusing on new features in DuckDB, invest time in studying how indexes, CTEs, and joins can improve query efficiency.Data Modeling: failing to plan ahead and model strategically often results in ad hoc requests dictating your approach. Take the time to design models and identify the most scalable, long-term, and functional data modeling approach for your specific use case.Thanks for reading The Pipe & The Line! Subscribe for free to receive new posts and support my work.&#128230; Navigating the Tools TsunamiWhen learning a new tool, investigate its common features and foundational principles (even using ChatGPT). For example:You do not need to master every tool: it does not matter whether you choose Databricks, BigQuery, or AWS. Select one based on market demand or its prevalence in job postings, and focus on becoming proficient with its core features. Testing new tools becomes easier when you have transferable knowledge: if you are familiar with AWS S3 Buckets and Lambda functions, you will find it easier to work with GCP Storage and Cloud Functions. Many tools share similar functionalities but differ in terminology.Wait until you are in a real-life setup to learn a tool in depth: Avoid creating overly complex, simulated use cases in personal projects. Focus on studying and practicing foundational concepts, and be prepared to dive deeper when the opportunity arises.Avoid becoming a \"shiny-tool-driven\" engineer: many engineers recommend tools they are excited about rather than selecting the most appropriate tool for the task. This approach often fails, as it undermines objectivity.Do not take my word for it; conduct your own research to determine the best learning path when it comes to tools.&#127891; Avoid Being a Certification Machine: Choose WiselyI often see LinkedIn posts stating, &#8220;I want to share that I got certified at [Company Name].&#8221; There is nothing inherently wrong with self-promotion; after all, that is LinkedIn&#8217;s primary purpose.The biggest issue with certifications is that if you do not apply the knowledge in real-life situations immediately, it becomes outdated very quickly. Certifications should be pursued with a long-term goal in mind.I have worked at three companies with AWS-based infrastructure; obtaining an AWS certification would help me expand my skills in cloud services, as it is likely to remain beneficial over the next 1, 3, or 5 years. On the other hand, obtaining a certification in Airflow might be less advantageous. Not all companies use it, and open-source projects often evolve so rapidly that it is easy to fall behind. Also, some functionalities may not be transferable to other orchestration tools.Thanks for reading The Pipe & The Line! This post is public so feel free to share it.Share&#128336; Avoid Being Naive: 95% of Companies Lack Real-Time and Big Data SetupsI understand: the idea of real-time big data pipelines sounds sexy!However, such setups are limited to a small number of companies. Most of the real-world industry does not operate at that scale.Additionally, over time, it becomes clear that many companies over-engineer their solutions with setups far more complex than necessary. Setting up Spark to handle 1M rows is the same as using a nuclear missile to kill a mosquito.Do not assume that failing to learn Spark or Kafka will prevent you from securing a job in the near future. In your early career, you are most likely to work at a small data company.As the saying goes, &#8220;Sometimes PostgreSQL is all you need.&#8221;Final ThoughtsAlways remember that while technology evolves rapidly, it is essential to understand your timing and capabilities at every step of the journey.Shiny new tools may seem exciting, but not all of them are practical or valuable.Take your time, stay focused, and occasionally take a break to feed your curiosity by experimenting with something new. But please, avoid forcing yourself to adopt tools or trends that do not align with your goals or needs.Thanks for reading The Pipe & The Line! Subscribe for free to receive new posts and support my work.TL;DR&#128218; Learn Timeless Skills: Python, SQL, and Data Modeling: Focus on mastering Python patterns, SQL optimization, and strategic data modeling to build scalable and adaptable skills for your data engineering career.&#128230; Navigating the Tools Tsunami: prioritize learning what aligns with your career goals rather than chasing every new tool on the market.&#127891; Avoid Being a Certification Machine: Choose Wisely: Invest in certifications only when they align with your long-term goals and offer practical benefits in real-life work setups.&#128336; Avoid Being Naive: 95% of Companies Lack Real-Time and Big Data Setups: Don&#8217;t get overwhelmed by hype; most organizations operate on simpler scales than the flashy big data narratives.If you enjoyed the content, hit the like &#10084;&#65039; button, share, comment, repost, and all those nice things people do when like stuff these days. Glad to know you made it to this part!Hi, I am Alejandro Aboy. I am currently working as a Data Engineer. I started in digital marketing at 19. I gained experience in website tracking, advertising, and analytics. I also founded my agency. In 2021, I found my passion for data engineering. So, I shifted my career focus, despite lacking a CS degree. I'm now pursuing this path, leveraging my diverse experience and willingness to learn.Follow me on Linkedin"}]}
{"messages": [{"role": "user", "content": "Create a career advice piece emphasizing continuous learning strategies to combat FOMO in tech."}, {"role": "assistant", "content": "I have been working with Snowflake, dbt, PostgreSQL, Python, Airflow, and AWS tools for approximately 3 years.The most challenging part of this journey has been always feeling as though I was falling behind.The list (and FOMO) never ends:If it is not DuckDB, it is Polars. If it is not S3 conditional writes, it is a new API release by OpenAI to build pipelines with text-to-speech endpoints.Starting this journey, you are learning the basics of SQL and Python, while &#8220;the big bosses&#8221; discuss topics such as the competition between Databricks and Snowflake.It is easy to feel as though you have missed the season finale of a Netflix series that everyone is discussing.I will outline my reflections on what is important after all these years, along with my top pieces of advice for beginners, as well as for anyone who wants to thrive in this field.GPT asked me to give credits for this image.&#128218; Learn Timeless Skills: Python, SQL, and Data ModelingI have met (and at times been) many people who know a great deal about Pandas and dbt tricks, but they lack the ability to write a WINDOW FUNCTION in SQL or scale reusable code with Python classes.Trendy topics often overlap with fundamental concepts, which are essential to master if you want to remain adaptable in the field!Jeff Bezos offers a quote that underscores this idea: &#8220;Focus on the things that never change; people will always want the best price and fast shipment.&#8221;What can you do with these things that never change?Python: Become proficient in data engineering patterns, such as ETL flows, but also explore creative projects, like building APIs or applications unrelated to data engineering. This approach can shift your perspective from being a simple pipeline owner to a data product builder.SQL: If you have a query that takes five minutes to process 10k rows, try optimising it. Rather than focusing on new features in DuckDB, invest time in studying how indexes, CTEs, and joins can improve query efficiency.Data Modeling: failing to plan ahead and model strategically often results in ad hoc requests dictating your approach. Take the time to design models and identify the most scalable, long-term, and functional data modeling approach for your specific use case.Thanks for reading The Pipe & The Line! Subscribe for free to receive new posts and support my work.&#128230; Navigating the Tools TsunamiWhen learning a new tool, investigate its common features and foundational principles (even using ChatGPT). For example:You do not need to master every tool: it does not matter whether you choose Databricks, BigQuery, or AWS. Select one based on market demand or its prevalence in job postings, and focus on becoming proficient with its core features. Testing new tools becomes easier when you have transferable knowledge: if you are familiar with AWS S3 Buckets and Lambda functions, you will find it easier to work with GCP Storage and Cloud Functions. Many tools share similar functionalities but differ in terminology.Wait until you are in a real-life setup to learn a tool in depth: Avoid creating overly complex, simulated use cases in personal projects. Focus on studying and practicing foundational concepts, and be prepared to dive deeper when the opportunity arises.Avoid becoming a \"shiny-tool-driven\" engineer: many engineers recommend tools they are excited about rather than selecting the most appropriate tool for the task. This approach often fails, as it undermines objectivity.Do not take my word for it; conduct your own research to determine the best learning path when it comes to tools.&#127891; Avoid Being a Certification Machine: Choose WiselyI often see LinkedIn posts stating, &#8220;I want to share that I got certified at [Company Name].&#8221; There is nothing inherently wrong with self-promotion; after all, that is LinkedIn&#8217;s primary purpose.The biggest issue with certifications is that if you do not apply the knowledge in real-life situations immediately, it becomes outdated very quickly. Certifications should be pursued with a long-term goal in mind.I have worked at three companies with AWS-based infrastructure; obtaining an AWS certification would help me expand my skills in cloud services, as it is likely to remain beneficial over the next 1, 3, or 5 years. On the other hand, obtaining a certification in Airflow might be less advantageous. Not all companies use it, and open-source projects often evolve so rapidly that it is easy to fall behind. Also, some functionalities may not be transferable to other orchestration tools.Thanks for reading The Pipe & The Line! This post is public so feel free to share it.Share&#128336; Avoid Being Naive: 95% of Companies Lack Real-Time and Big Data SetupsI understand: the idea of real-time big data pipelines sounds sexy!However, such setups are limited to a small number of companies. Most of the real-world industry does not operate at that scale.Additionally, over time, it becomes clear that many companies over-engineer their solutions with setups far more complex than necessary. Setting up Spark to handle 1M rows is the same as using a nuclear missile to kill a mosquito.Do not assume that failing to learn Spark or Kafka will prevent you from securing a job in the near future. In your early career, you are most likely to work at a small data company.As the saying goes, &#8220;Sometimes PostgreSQL is all you need.&#8221;Final ThoughtsAlways remember that while technology evolves rapidly, it is essential to understand your timing and capabilities at every step of the journey.Shiny new tools may seem exciting, but not all of them are practical or valuable.Take your time, stay focused, and occasionally take a break to feed your curiosity by experimenting with something new. But please, avoid forcing yourself to adopt tools or trends that do not align with your goals or needs.Thanks for reading The Pipe & The Line! Subscribe for free to receive new posts and support my work.TL;DR&#128218; Learn Timeless Skills: Python, SQL, and Data Modeling: Focus on mastering Python patterns, SQL optimization, and strategic data modeling to build scalable and adaptable skills for your data engineering career.&#128230; Navigating the Tools Tsunami: prioritize learning what aligns with your career goals rather than chasing every new tool on the market.&#127891; Avoid Being a Certification Machine: Choose Wisely: Invest in certifications only when they align with your long-term goals and offer practical benefits in real-life work setups.&#128336; Avoid Being Naive: 95% of Companies Lack Real-Time and Big Data Setups: Don&#8217;t get overwhelmed by hype; most organizations operate on simpler scales than the flashy big data narratives.If you enjoyed the content, hit the like &#10084;&#65039; button, share, comment, repost, and all those nice things people do when like stuff these days. Glad to know you made it to this part!Hi, I am Alejandro Aboy. I am currently working as a Data Engineer. I started in digital marketing at 19. I gained experience in website tracking, advertising, and analytics. I also founded my agency. In 2021, I found my passion for data engineering. So, I shifted my career focus, despite lacking a CS degree. I'm now pursuing this path, leveraging my diverse experience and willingness to learn.Follow me on Linkedin"}]}
{"messages": [{"role": "user", "content": "Develop a practical guide on integrating new technologies into existing data pipelines for efficiency."}, {"role": "assistant", "content": "I have been working with Snowflake, dbt, PostgreSQL, Python, Airflow, and AWS tools for approximately 3 years.The most challenging part of this journey has been always feeling as though I was falling behind.The list (and FOMO) never ends:If it is not DuckDB, it is Polars. If it is not S3 conditional writes, it is a new API release by OpenAI to build pipelines with text-to-speech endpoints.Starting this journey, you are learning the basics of SQL and Python, while &#8220;the big bosses&#8221; discuss topics such as the competition between Databricks and Snowflake.It is easy to feel as though you have missed the season finale of a Netflix series that everyone is discussing.I will outline my reflections on what is important after all these years, along with my top pieces of advice for beginners, as well as for anyone who wants to thrive in this field.GPT asked me to give credits for this image.&#128218; Learn Timeless Skills: Python, SQL, and Data ModelingI have met (and at times been) many people who know a great deal about Pandas and dbt tricks, but they lack the ability to write a WINDOW FUNCTION in SQL or scale reusable code with Python classes.Trendy topics often overlap with fundamental concepts, which are essential to master if you want to remain adaptable in the field!Jeff Bezos offers a quote that underscores this idea: &#8220;Focus on the things that never change; people will always want the best price and fast shipment.&#8221;What can you do with these things that never change?Python: Become proficient in data engineering patterns, such as ETL flows, but also explore creative projects, like building APIs or applications unrelated to data engineering. This approach can shift your perspective from being a simple pipeline owner to a data product builder.SQL: If you have a query that takes five minutes to process 10k rows, try optimising it. Rather than focusing on new features in DuckDB, invest time in studying how indexes, CTEs, and joins can improve query efficiency.Data Modeling: failing to plan ahead and model strategically often results in ad hoc requests dictating your approach. Take the time to design models and identify the most scalable, long-term, and functional data modeling approach for your specific use case.Thanks for reading The Pipe & The Line! Subscribe for free to receive new posts and support my work.&#128230; Navigating the Tools TsunamiWhen learning a new tool, investigate its common features and foundational principles (even using ChatGPT). For example:You do not need to master every tool: it does not matter whether you choose Databricks, BigQuery, or AWS. Select one based on market demand or its prevalence in job postings, and focus on becoming proficient with its core features. Testing new tools becomes easier when you have transferable knowledge: if you are familiar with AWS S3 Buckets and Lambda functions, you will find it easier to work with GCP Storage and Cloud Functions. Many tools share similar functionalities but differ in terminology.Wait until you are in a real-life setup to learn a tool in depth: Avoid creating overly complex, simulated use cases in personal projects. Focus on studying and practicing foundational concepts, and be prepared to dive deeper when the opportunity arises.Avoid becoming a \"shiny-tool-driven\" engineer: many engineers recommend tools they are excited about rather than selecting the most appropriate tool for the task. This approach often fails, as it undermines objectivity.Do not take my word for it; conduct your own research to determine the best learning path when it comes to tools.&#127891; Avoid Being a Certification Machine: Choose WiselyI often see LinkedIn posts stating, &#8220;I want to share that I got certified at [Company Name].&#8221; There is nothing inherently wrong with self-promotion; after all, that is LinkedIn&#8217;s primary purpose.The biggest issue with certifications is that if you do not apply the knowledge in real-life situations immediately, it becomes outdated very quickly. Certifications should be pursued with a long-term goal in mind.I have worked at three companies with AWS-based infrastructure; obtaining an AWS certification would help me expand my skills in cloud services, as it is likely to remain beneficial over the next 1, 3, or 5 years. On the other hand, obtaining a certification in Airflow might be less advantageous. Not all companies use it, and open-source projects often evolve so rapidly that it is easy to fall behind. Also, some functionalities may not be transferable to other orchestration tools.Thanks for reading The Pipe & The Line! This post is public so feel free to share it.Share&#128336; Avoid Being Naive: 95% of Companies Lack Real-Time and Big Data SetupsI understand: the idea of real-time big data pipelines sounds sexy!However, such setups are limited to a small number of companies. Most of the real-world industry does not operate at that scale.Additionally, over time, it becomes clear that many companies over-engineer their solutions with setups far more complex than necessary. Setting up Spark to handle 1M rows is the same as using a nuclear missile to kill a mosquito.Do not assume that failing to learn Spark or Kafka will prevent you from securing a job in the near future. In your early career, you are most likely to work at a small data company.As the saying goes, &#8220;Sometimes PostgreSQL is all you need.&#8221;Final ThoughtsAlways remember that while technology evolves rapidly, it is essential to understand your timing and capabilities at every step of the journey.Shiny new tools may seem exciting, but not all of them are practical or valuable.Take your time, stay focused, and occasionally take a break to feed your curiosity by experimenting with something new. But please, avoid forcing yourself to adopt tools or trends that do not align with your goals or needs.Thanks for reading The Pipe & The Line! Subscribe for free to receive new posts and support my work.TL;DR&#128218; Learn Timeless Skills: Python, SQL, and Data Modeling: Focus on mastering Python patterns, SQL optimization, and strategic data modeling to build scalable and adaptable skills for your data engineering career.&#128230; Navigating the Tools Tsunami: prioritize learning what aligns with your career goals rather than chasing every new tool on the market.&#127891; Avoid Being a Certification Machine: Choose Wisely: Invest in certifications only when they align with your long-term goals and offer practical benefits in real-life work setups.&#128336; Avoid Being Naive: 95% of Companies Lack Real-Time and Big Data Setups: Don&#8217;t get overwhelmed by hype; most organizations operate on simpler scales than the flashy big data narratives.If you enjoyed the content, hit the like &#10084;&#65039; button, share, comment, repost, and all those nice things people do when like stuff these days. Glad to know you made it to this part!Hi, I am Alejandro Aboy. I am currently working as a Data Engineer. I started in digital marketing at 19. I gained experience in website tracking, advertising, and analytics. I also founded my agency. In 2021, I found my passion for data engineering. So, I shifted my career focus, despite lacking a CS degree. I'm now pursuing this path, leveraging my diverse experience and willingness to learn.Follow me on Linkedin"}]}
{"messages": [{"role": "user", "content": "Explain how to automate data extraction from Google Sheets using Python scripts for data engineering tasks."}, {"role": "assistant", "content": "Google Sheets might be one of the most debated tools in the field of Data Engineering. The main reason is as follows:&#8220;I create tons of ETL scripts and Dashboards for stakeholders to ask for spreadsheets exports.&#8221; Not everyone knows Python or SQL to process data at scale, so good all Google Sheets makes it easier for them.Luckily, we have great tools to collect whatever data is being created within those spreadsheets and process it without much effort.We&#8217;ll explore 3 most popular ways to leverage Google Sheets data in your workflows. AI generated image to make it epic and visual.Thanks for reading The Pipe & The Line! Subscribe for free to receive new posts and support my work.&#128736;&#65039; Google Sheets APIFor full control, directly interacting with the Google Sheets API is a powerful option.For all Google Sheets API scripts in the docs, you can identify this pattern:Only the specific actions (e.g., read, write, append) will vary in the code, but its overall structure remains familiar.All actions require:A spreadsheet id (e.g. SPREADSHEET_ID = &#8216;Jasw321K29182_12389HsaswdA&#8221;)A range name (e.g. RANGE_NAME = &#8216;data!A1:E12&#8221;)Values to update, append, write, etc (If applicable)For each action you want to run, you will execute the service HTTP request to Google Sheets API. Whenever you see '...', it indicates a placeholder for the previously described pattern.You can read a spreadsheet like this:Update values like this:Appending values uses the same code snippet but replaces 'update' with 'append.'You might wonder why someone would use this if Python wrappers exist? Flexibility. Python wrappers cover the more typical use cases, but if you need something specific for an edge case, you might want to enhance them with the plain API.Learn more about the possibilities in the Google Sheets API docs. &#128013; Python &#8216;gspread&#8217; packageProbably the best choice for Data Engineers: Remove the pain from plain Google Sheets API and use a wrapper. It has a quick learning curve and can be easily extended.The usual workflow to read a spreadsheet looks like this:Then, you can make further processing with Pandas or DuckDB, see example:If you want to expand functionalities, you can use gspread-pandas or gspread-dataframe. Here&#8217;s an example to save a dataframe into a worksheet or update a range of cells:Learn more about the possibilities in the gspread pypi docs.&#128450;&#65039; How does Google Sheets expect to receive data?Writing has its challenges, because the Google Sheets API requires a list of lists and is not compatible with DataFrames, needing additional formatting. This is how it looks in gspread:Keep in mind that empty strings (nulls), infinite values, and other caveats must be addressed, as Google Sheets handles data types differently from DataFrames.The previous example might not be enough, depending on the structure of your data.Here&#8217;s an example of that required processing:Many of these edge cases can be addressed using the built-in df_to_sheet method from the gspread-pandas package.&#129414; DuckDB, keep making things easier!A couple weeks ago, I mentioned that some things were cooking for DuckDBAnd now it&#8217;s for real: DuckDB Google Sheets Extension. The greatest thing? it&#8217;s SQL friendly, it&#8217;s much easier because there&#8217;s no learning curve! The main highlight is code reduction: we went from 20-30 lines with previous examples to 1 line for the same result. Writing it is also easier, you don&#8217;t need to worry about formatting because it&#8217;s handled by the extension. Learn more about the possibilities in the DuckDB Sheets extension docs.. &#128272; Authentication with Service Accounts Sometimes, the main blocker to get this workflow up and running is getting authentication right. There are many options, such as OAuth, Service Account and others, but I&#8217;ll talk about the most powerful in my opinion.Here&#8217;s a quick guide to set up a Service Account:Head to Google Cloud Console and create a new project.Navigate to APIs & Services &#8594; Credentials and create a Service Account.Download the JSON file for the service account and add it to your project.Share the Service Account Email with the Google Sheets you want to access. This also works for the folder where the Spreadsheets are saved. If you have a one shot use-case that doesn&#8217;t require an automation workflow and you don&#8217;t want/need to create a service account, you can just use Google Collab. This is how gspread looks without the authentication hassle:You can keep using gspread or DuckDB to extend your workflows within the notebook environment.If at some point you need to automate this, you will need to move away from Collab, install required packages and create your service account to handle things properly. Thanks for reading The Pipe & The Line! This post is public so feel free to share it.Share&#128221; TL;DR &#128736;&#65039; Google Sheets API: full flexibility to extend methods as you please.&#128013; Python &#8216;gspread&#8217; package: simple python wrapper to speed up your development. &#128450;&#65039; How does Google Sheets expects to receive data?: Formatting tips and considerations.&#129414; DuckDB, keep making things easier!: SQL in memory&#8212;best for handling and querying large datasets within Sheets.&#128272; Authentication with Service Accounts: step-by-step guide to create your credentials for Google Sheets API.If you enjoyed the content, hit the like &#10084;&#65039; button, share, comment, repost, and all those nice things people do when like stuff these days. Glad to know you made it to this part!Hi, I am Alejandro Aboy. I am currently working as a Data Engineer. I started in digital marketing at 19. I gained experience in website tracking, advertising, and analytics. I also founded my agency. In 2021, I found my passion for data engineering. So, I shifted my career focus, despite lacking a CS degree. I'm now pursuing this path, leveraging my diverse experience and willingness to learn.Follow me on Linkedin"}]}
{"messages": [{"role": "user", "content": "Discuss the importance of mastering data visualization tools for enhancing stakeholder communication in data projects."}, {"role": "assistant", "content": "Google Sheets might be one of the most debated tools in the field of Data Engineering. The main reason is as follows:&#8220;I create tons of ETL scripts and Dashboards for stakeholders to ask for spreadsheets exports.&#8221; Not everyone knows Python or SQL to process data at scale, so good all Google Sheets makes it easier for them.Luckily, we have great tools to collect whatever data is being created within those spreadsheets and process it without much effort.We&#8217;ll explore 3 most popular ways to leverage Google Sheets data in your workflows. AI generated image to make it epic and visual.Thanks for reading The Pipe & The Line! Subscribe for free to receive new posts and support my work.&#128736;&#65039; Google Sheets APIFor full control, directly interacting with the Google Sheets API is a powerful option.For all Google Sheets API scripts in the docs, you can identify this pattern:Only the specific actions (e.g., read, write, append) will vary in the code, but its overall structure remains familiar.All actions require:A spreadsheet id (e.g. SPREADSHEET_ID = &#8216;Jasw321K29182_12389HsaswdA&#8221;)A range name (e.g. RANGE_NAME = &#8216;data!A1:E12&#8221;)Values to update, append, write, etc (If applicable)For each action you want to run, you will execute the service HTTP request to Google Sheets API. Whenever you see '...', it indicates a placeholder for the previously described pattern.You can read a spreadsheet like this:Update values like this:Appending values uses the same code snippet but replaces 'update' with 'append.'You might wonder why someone would use this if Python wrappers exist? Flexibility. Python wrappers cover the more typical use cases, but if you need something specific for an edge case, you might want to enhance them with the plain API.Learn more about the possibilities in the Google Sheets API docs. &#128013; Python &#8216;gspread&#8217; packageProbably the best choice for Data Engineers: Remove the pain from plain Google Sheets API and use a wrapper. It has a quick learning curve and can be easily extended.The usual workflow to read a spreadsheet looks like this:Then, you can make further processing with Pandas or DuckDB, see example:If you want to expand functionalities, you can use gspread-pandas or gspread-dataframe. Here&#8217;s an example to save a dataframe into a worksheet or update a range of cells:Learn more about the possibilities in the gspread pypi docs.&#128450;&#65039; How does Google Sheets expect to receive data?Writing has its challenges, because the Google Sheets API requires a list of lists and is not compatible with DataFrames, needing additional formatting. This is how it looks in gspread:Keep in mind that empty strings (nulls), infinite values, and other caveats must be addressed, as Google Sheets handles data types differently from DataFrames.The previous example might not be enough, depending on the structure of your data.Here&#8217;s an example of that required processing:Many of these edge cases can be addressed using the built-in df_to_sheet method from the gspread-pandas package.&#129414; DuckDB, keep making things easier!A couple weeks ago, I mentioned that some things were cooking for DuckDBAnd now it&#8217;s for real: DuckDB Google Sheets Extension. The greatest thing? it&#8217;s SQL friendly, it&#8217;s much easier because there&#8217;s no learning curve! The main highlight is code reduction: we went from 20-30 lines with previous examples to 1 line for the same result. Writing it is also easier, you don&#8217;t need to worry about formatting because it&#8217;s handled by the extension. Learn more about the possibilities in the DuckDB Sheets extension docs.. &#128272; Authentication with Service Accounts Sometimes, the main blocker to get this workflow up and running is getting authentication right. There are many options, such as OAuth, Service Account and others, but I&#8217;ll talk about the most powerful in my opinion.Here&#8217;s a quick guide to set up a Service Account:Head to Google Cloud Console and create a new project.Navigate to APIs & Services &#8594; Credentials and create a Service Account.Download the JSON file for the service account and add it to your project.Share the Service Account Email with the Google Sheets you want to access. This also works for the folder where the Spreadsheets are saved. If you have a one shot use-case that doesn&#8217;t require an automation workflow and you don&#8217;t want/need to create a service account, you can just use Google Collab. This is how gspread looks without the authentication hassle:You can keep using gspread or DuckDB to extend your workflows within the notebook environment.If at some point you need to automate this, you will need to move away from Collab, install required packages and create your service account to handle things properly. Thanks for reading The Pipe & The Line! This post is public so feel free to share it.Share&#128221; TL;DR &#128736;&#65039; Google Sheets API: full flexibility to extend methods as you please.&#128013; Python &#8216;gspread&#8217; package: simple python wrapper to speed up your development. &#128450;&#65039; How does Google Sheets expects to receive data?: Formatting tips and considerations.&#129414; DuckDB, keep making things easier!: SQL in memory&#8212;best for handling and querying large datasets within Sheets.&#128272; Authentication with Service Accounts: step-by-step guide to create your credentials for Google Sheets API.If you enjoyed the content, hit the like &#10084;&#65039; button, share, comment, repost, and all those nice things people do when like stuff these days. Glad to know you made it to this part!Hi, I am Alejandro Aboy. I am currently working as a Data Engineer. I started in digital marketing at 19. I gained experience in website tracking, advertising, and analytics. I also founded my agency. In 2021, I found my passion for data engineering. So, I shifted my career focus, despite lacking a CS degree. I'm now pursuing this path, leveraging my diverse experience and willingness to learn.Follow me on Linkedin"}]}
{"messages": [{"role": "user", "content": "Create a step-by-step guide on integrating Google Sheets with ETL processes for seamless data management."}, {"role": "assistant", "content": "Google Sheets might be one of the most debated tools in the field of Data Engineering. The main reason is as follows:&#8220;I create tons of ETL scripts and Dashboards for stakeholders to ask for spreadsheets exports.&#8221; Not everyone knows Python or SQL to process data at scale, so good all Google Sheets makes it easier for them.Luckily, we have great tools to collect whatever data is being created within those spreadsheets and process it without much effort.We&#8217;ll explore 3 most popular ways to leverage Google Sheets data in your workflows. AI generated image to make it epic and visual.Thanks for reading The Pipe & The Line! Subscribe for free to receive new posts and support my work.&#128736;&#65039; Google Sheets APIFor full control, directly interacting with the Google Sheets API is a powerful option.For all Google Sheets API scripts in the docs, you can identify this pattern:Only the specific actions (e.g., read, write, append) will vary in the code, but its overall structure remains familiar.All actions require:A spreadsheet id (e.g. SPREADSHEET_ID = &#8216;Jasw321K29182_12389HsaswdA&#8221;)A range name (e.g. RANGE_NAME = &#8216;data!A1:E12&#8221;)Values to update, append, write, etc (If applicable)For each action you want to run, you will execute the service HTTP request to Google Sheets API. Whenever you see '...', it indicates a placeholder for the previously described pattern.You can read a spreadsheet like this:Update values like this:Appending values uses the same code snippet but replaces 'update' with 'append.'You might wonder why someone would use this if Python wrappers exist? Flexibility. Python wrappers cover the more typical use cases, but if you need something specific for an edge case, you might want to enhance them with the plain API.Learn more about the possibilities in the Google Sheets API docs. &#128013; Python &#8216;gspread&#8217; packageProbably the best choice for Data Engineers: Remove the pain from plain Google Sheets API and use a wrapper. It has a quick learning curve and can be easily extended.The usual workflow to read a spreadsheet looks like this:Then, you can make further processing with Pandas or DuckDB, see example:If you want to expand functionalities, you can use gspread-pandas or gspread-dataframe. Here&#8217;s an example to save a dataframe into a worksheet or update a range of cells:Learn more about the possibilities in the gspread pypi docs.&#128450;&#65039; How does Google Sheets expect to receive data?Writing has its challenges, because the Google Sheets API requires a list of lists and is not compatible with DataFrames, needing additional formatting. This is how it looks in gspread:Keep in mind that empty strings (nulls), infinite values, and other caveats must be addressed, as Google Sheets handles data types differently from DataFrames.The previous example might not be enough, depending on the structure of your data.Here&#8217;s an example of that required processing:Many of these edge cases can be addressed using the built-in df_to_sheet method from the gspread-pandas package.&#129414; DuckDB, keep making things easier!A couple weeks ago, I mentioned that some things were cooking for DuckDBAnd now it&#8217;s for real: DuckDB Google Sheets Extension. The greatest thing? it&#8217;s SQL friendly, it&#8217;s much easier because there&#8217;s no learning curve! The main highlight is code reduction: we went from 20-30 lines with previous examples to 1 line for the same result. Writing it is also easier, you don&#8217;t need to worry about formatting because it&#8217;s handled by the extension. Learn more about the possibilities in the DuckDB Sheets extension docs.. &#128272; Authentication with Service Accounts Sometimes, the main blocker to get this workflow up and running is getting authentication right. There are many options, such as OAuth, Service Account and others, but I&#8217;ll talk about the most powerful in my opinion.Here&#8217;s a quick guide to set up a Service Account:Head to Google Cloud Console and create a new project.Navigate to APIs & Services &#8594; Credentials and create a Service Account.Download the JSON file for the service account and add it to your project.Share the Service Account Email with the Google Sheets you want to access. This also works for the folder where the Spreadsheets are saved. If you have a one shot use-case that doesn&#8217;t require an automation workflow and you don&#8217;t want/need to create a service account, you can just use Google Collab. This is how gspread looks without the authentication hassle:You can keep using gspread or DuckDB to extend your workflows within the notebook environment.If at some point you need to automate this, you will need to move away from Collab, install required packages and create your service account to handle things properly. Thanks for reading The Pipe & The Line! This post is public so feel free to share it.Share&#128221; TL;DR &#128736;&#65039; Google Sheets API: full flexibility to extend methods as you please.&#128013; Python &#8216;gspread&#8217; package: simple python wrapper to speed up your development. &#128450;&#65039; How does Google Sheets expects to receive data?: Formatting tips and considerations.&#129414; DuckDB, keep making things easier!: SQL in memory&#8212;best for handling and querying large datasets within Sheets.&#128272; Authentication with Service Accounts: step-by-step guide to create your credentials for Google Sheets API.If you enjoyed the content, hit the like &#10084;&#65039; button, share, comment, repost, and all those nice things people do when like stuff these days. Glad to know you made it to this part!Hi, I am Alejandro Aboy. I am currently working as a Data Engineer. I started in digital marketing at 19. I gained experience in website tracking, advertising, and analytics. I also founded my agency. In 2021, I found my passion for data engineering. So, I shifted my career focus, despite lacking a CS degree. I'm now pursuing this path, leveraging my diverse experience and willingness to learn.Follow me on Linkedin"}]}
{"messages": [{"role": "user", "content": "Explain the architecture and performance enhancements of FireDucks compared to traditional data processing tools."}, {"role": "assistant", "content": "I saw this post from . I&#8217;ve been reading the article to become more familiar with this tool.I thought this could be a good opportunity to extend one of my articles with additional testing:&#128736;&#65039; Setting up the environment&#128064; MacOS or Windows versions were not released at the time of writing this article. See note:Windows and MacOS versions are under development. We will soon be releasing the beta-version of FireDucks for these platforms. At this moment, if you are working on Windows system, you may like to consider using FireDucks from WSL (Windows Subsystem for Linux).Please make sure you are using Python &gt;=3.8, &lt;=3.12, otherwise you may encounter error saying: &#8220;No matching distribution found for fireducks&#8221;.You can add the Fireducks package by installing it or loading the extension:pip install fireducks%load_ext fireducks.pandas import pandas as pdAlso, you can do an explicit import: import fireducks.pandas as pdThanks for reading The Pipe & The Line! Subscribe for free to receive new posts and support my work.&#9888;&#65039; Disclaimer:All tests ran on Mac M1 Pro 16GB 2021. The presented conclusions might vary from other experiments.I created this Collab notebook to test 50k, 100k, 200k, 1M and 2M. I&#8217;ve stopped at 5M since it crashed &#128514;The code wraps some methods to generate fake data with Faker and processes it with standard Pandas and Fireducks Pandas.Now, let&#8217;s run some tests. &#129514; Is Fireducks 20-30x faster than Pandas?The answer is yes, particularly for CSV read times in this benchmark:For the CSV read times on 2 million rows:Pandas: 10.30 secondsFireducks: 0.01 seconds963 times faster &#128514; For CSV write times with 2 million rows:Pandas: 15.35 secondsFireducks: 2.63 seconds6 times faster &#127919;In other file formats like Parquet and Arrow, the difference tends to be around 10-20x faster for read operations and about 2-3x faster for write operations.Based on the numbers in my previous post, it&#8217;s even faster than DuckDB and Polars on CSV files reading performance.A representative AI generated image.&#128221; Last WordsThis could be an excellent option for use cases that need a boost in processing power without a steep learning curve.Just by changing the import statement, you can significantly improve your workflows, and you don&#8217;t need to learn Polars or move to DuckDB.I always recommend finding the right tool for your context instead of being drawn to the newest option.Sometimes, we only need plain Pandas to get through the day, and that&#8217;s perfectly fine.If you enjoyed the content, hit the like &#10084;&#65039; button, share, comment, repost, and all those nice things people do when like stuff these days. Glad to know you made it to this part!Hi, I am Alejandro Aboy. I am currently working as a Data Engineer. I started in digital marketing at 19. I gained experience in website tracking, advertising, and analytics. I also founded my agency. In 2021, I found my passion for data engineering. So, I shifted my career focus, despite lacking a CS degree. I'm now pursuing this path, leveraging my diverse experience and willingness to learn.Follow me on Linkedin"}]}
{"messages": [{"role": "user", "content": "Discuss how mastering FireDucks can boost your career prospects in data engineering."}, {"role": "assistant", "content": "I saw this post from . I&#8217;ve been reading the article to become more familiar with this tool.I thought this could be a good opportunity to extend one of my articles with additional testing:&#128736;&#65039; Setting up the environment&#128064; MacOS or Windows versions were not released at the time of writing this article. See note:Windows and MacOS versions are under development. We will soon be releasing the beta-version of FireDucks for these platforms. At this moment, if you are working on Windows system, you may like to consider using FireDucks from WSL (Windows Subsystem for Linux).Please make sure you are using Python &gt;=3.8, &lt;=3.12, otherwise you may encounter error saying: &#8220;No matching distribution found for fireducks&#8221;.You can add the Fireducks package by installing it or loading the extension:pip install fireducks%load_ext fireducks.pandas import pandas as pdAlso, you can do an explicit import: import fireducks.pandas as pdThanks for reading The Pipe & The Line! Subscribe for free to receive new posts and support my work.&#9888;&#65039; Disclaimer:All tests ran on Mac M1 Pro 16GB 2021. The presented conclusions might vary from other experiments.I created this Collab notebook to test 50k, 100k, 200k, 1M and 2M. I&#8217;ve stopped at 5M since it crashed &#128514;The code wraps some methods to generate fake data with Faker and processes it with standard Pandas and Fireducks Pandas.Now, let&#8217;s run some tests. &#129514; Is Fireducks 20-30x faster than Pandas?The answer is yes, particularly for CSV read times in this benchmark:For the CSV read times on 2 million rows:Pandas: 10.30 secondsFireducks: 0.01 seconds963 times faster &#128514; For CSV write times with 2 million rows:Pandas: 15.35 secondsFireducks: 2.63 seconds6 times faster &#127919;In other file formats like Parquet and Arrow, the difference tends to be around 10-20x faster for read operations and about 2-3x faster for write operations.Based on the numbers in my previous post, it&#8217;s even faster than DuckDB and Polars on CSV files reading performance.A representative AI generated image.&#128221; Last WordsThis could be an excellent option for use cases that need a boost in processing power without a steep learning curve.Just by changing the import statement, you can significantly improve your workflows, and you don&#8217;t need to learn Polars or move to DuckDB.I always recommend finding the right tool for your context instead of being drawn to the newest option.Sometimes, we only need plain Pandas to get through the day, and that&#8217;s perfectly fine.If you enjoyed the content, hit the like &#10084;&#65039; button, share, comment, repost, and all those nice things people do when like stuff these days. Glad to know you made it to this part!Hi, I am Alejandro Aboy. I am currently working as a Data Engineer. I started in digital marketing at 19. I gained experience in website tracking, advertising, and analytics. I also founded my agency. In 2021, I found my passion for data engineering. So, I shifted my career focus, despite lacking a CS degree. I'm now pursuing this path, leveraging my diverse experience and willingness to learn.Follow me on Linkedin"}]}
{"messages": [{"role": "user", "content": "Create a step-by-step guide for setting up FireDucks in a Linux environment for data analysis."}, {"role": "assistant", "content": "I saw this post from . I&#8217;ve been reading the article to become more familiar with this tool.I thought this could be a good opportunity to extend one of my articles with additional testing:&#128736;&#65039; Setting up the environment&#128064; MacOS or Windows versions were not released at the time of writing this article. See note:Windows and MacOS versions are under development. We will soon be releasing the beta-version of FireDucks for these platforms. At this moment, if you are working on Windows system, you may like to consider using FireDucks from WSL (Windows Subsystem for Linux).Please make sure you are using Python &gt;=3.8, &lt;=3.12, otherwise you may encounter error saying: &#8220;No matching distribution found for fireducks&#8221;.You can add the Fireducks package by installing it or loading the extension:pip install fireducks%load_ext fireducks.pandas import pandas as pdAlso, you can do an explicit import: import fireducks.pandas as pdThanks for reading The Pipe & The Line! Subscribe for free to receive new posts and support my work.&#9888;&#65039; Disclaimer:All tests ran on Mac M1 Pro 16GB 2021. The presented conclusions might vary from other experiments.I created this Collab notebook to test 50k, 100k, 200k, 1M and 2M. I&#8217;ve stopped at 5M since it crashed &#128514;The code wraps some methods to generate fake data with Faker and processes it with standard Pandas and Fireducks Pandas.Now, let&#8217;s run some tests. &#129514; Is Fireducks 20-30x faster than Pandas?The answer is yes, particularly for CSV read times in this benchmark:For the CSV read times on 2 million rows:Pandas: 10.30 secondsFireducks: 0.01 seconds963 times faster &#128514; For CSV write times with 2 million rows:Pandas: 15.35 secondsFireducks: 2.63 seconds6 times faster &#127919;In other file formats like Parquet and Arrow, the difference tends to be around 10-20x faster for read operations and about 2-3x faster for write operations.Based on the numbers in my previous post, it&#8217;s even faster than DuckDB and Polars on CSV files reading performance.A representative AI generated image.&#128221; Last WordsThis could be an excellent option for use cases that need a boost in processing power without a steep learning curve.Just by changing the import statement, you can significantly improve your workflows, and you don&#8217;t need to learn Polars or move to DuckDB.I always recommend finding the right tool for your context instead of being drawn to the newest option.Sometimes, we only need plain Pandas to get through the day, and that&#8217;s perfectly fine.If you enjoyed the content, hit the like &#10084;&#65039; button, share, comment, repost, and all those nice things people do when like stuff these days. Glad to know you made it to this part!Hi, I am Alejandro Aboy. I am currently working as a Data Engineer. I started in digital marketing at 19. I gained experience in website tracking, advertising, and analytics. I also founded my agency. In 2021, I found my passion for data engineering. So, I shifted my career focus, despite lacking a CS degree. I'm now pursuing this path, leveraging my diverse experience and willingness to learn.Follow me on Linkedin"}]}
{"messages": [{"role": "user", "content": "Explain how pglogical implements Change Data Capture in PostgreSQL with examples and use cases."}, {"role": "assistant", "content": "There is another version of this post for native Logical Replication in PostgreSQL that I would suggest check beforehand:This repo has been adapted with some improvements to handle some edge cases and enhance the replication process!&#128104;&#127995;&#8205;&#128187; The environment I&#8217;ve created a repo to help me speed up my testing. &#128073; Github Repo: &#128259; Change Data Capture (CDC) using pglogical in PostgreSQL Graphic representation.In this context, I&#8217;ve enabled 2 PostgreSQL services with Docker to simulate CDC with a fake data generator I&#8217;ve adapted for this scenario. With some simple commands, you can simulate a bunch of scenarios such as INSERT, DELETE, UPDATE, DROP, TRUNCATE.Using Dbeaver or psql in your terminal can help you validate the changes afterwards. By default, the repo will create the 2 PostgreSQL instances with everything you need.For this to work, we need:pglogical extension. A Publisher node (Source)A Subscriber node (Target)A Replication set for the tables to replication.A Subscription to the Publisher. Tables with primary keys. PostgreSQL config with wal_level = &#8216;logical&#8217;Also, this repo has some adjustments as backup, since these actions are not executed by default and would break the replication process: Scripts will add/remove columns to tables on source and target databases.Scripts will add new tables to the replication and to both databses.Scripts will remove tables from the replication set and drop tables on both databases.Thanks for reading The Pipe & The Line! Subscribe for free to receive new posts and support my work.&#128279; Starting the replicationTo get started, you can run:make build NUM_RECORDS=10000 make runThis will spin up both PostgreSQL services, and create source tables with 10K rows of fake data. Then, you run: make cdc-pglogicalSource (a.k.a provider/publisher):Create pglogical extension.Create a publisher node.Add tables to the replication set.By default, it will create a replication slot. Target (a.k.a subscriber):Create pglogical extension.Create a subscription to the publisher.&#128258; Sending changesYou can start sending changes, these commands will apply INSERT, DELETE, TRUNCATE or UPDATE on a single random table. You can decide on how many rows you apply the changes:NUM_RECORDS=5 make insert-data NUM_RECORDS=5 make delete-data NUM_RECORDS=5 make update-data make truncateThis is the same as doing (e.g. products table):Thanks for reading The Pipe & The Line! Subscribe to get useful content for your data career!&#128064; Try breaking the replication! If it wasn&#8217;t for the improvements to the scripts, all these upcoming actions would break the pglogical replication. Good thing about to pglogical compared to native PostgreSQL replication is that replication sets can cover entire schemas. If you add a new table for an already included schema, it will add it automatically, but you will still need to create it on the target database. If you need to drop a table, you need to remove it from the replication and also drop on the target database. This edge cases are covered by the scripts:You can test this scenario by running these commands in the repo. You will notice tables are created and drop in both databases: make create-table make drop-table Translates as (e.g. products table):Schema changes is another scenario, you can also test it:make add-column make drop-columnTranslates as (e.g. products table):The enhanced scripts are applying changes in both databases, thing that wouldn&#8217;t happen by default. As always, you can run these commands to validate all changes:&#128221; Conclusions & NotesBe default, pglogical will copy an initial snapshot when enabled.You need PRIMARY KEYs on the TABLES you want to replicate. Not VIEWS.This feature doesn&#8217;t replicate sequences, DDLs or schemas.Schema changes are ignored. Adding or dropping columns will do nothing.If you have set pglogical.replication_set_add_all_tables('default', ARRAY['public']); and run CREATE TABLE, you need to also add it on the target database, otherwise it will break the replication.The previous point works similarly to the DROP TABLE, you need to remove it from the replication set and then drop it on both sides.If you enjoyed the content, hit the like &#10084;&#65039; button, share, comment, repost, and all those nice things people do when like stuff these days. Glad to know you made it to this part!Hi, I am Alejandro Aboy. I am currently working as a Data Engineer. I started in digital marketing at 19. I gained experience in website tracking, advertising, and analytics. I also founded my agency. In 2021, I found my passion for data engineering. So, I shifted my career focus, despite lacking a CS degree. I'm now pursuing this path, leveraging my diverse experience and willingness to learn.Follow me on Linkedin"}]}
{"messages": [{"role": "user", "content": "Discuss the career benefits of mastering logical replication technologies like pglogical for data engineers."}, {"role": "assistant", "content": "There is another version of this post for native Logical Replication in PostgreSQL that I would suggest check beforehand:This repo has been adapted with some improvements to handle some edge cases and enhance the replication process!&#128104;&#127995;&#8205;&#128187; The environment I&#8217;ve created a repo to help me speed up my testing. &#128073; Github Repo: &#128259; Change Data Capture (CDC) using pglogical in PostgreSQL Graphic representation.In this context, I&#8217;ve enabled 2 PostgreSQL services with Docker to simulate CDC with a fake data generator I&#8217;ve adapted for this scenario. With some simple commands, you can simulate a bunch of scenarios such as INSERT, DELETE, UPDATE, DROP, TRUNCATE.Using Dbeaver or psql in your terminal can help you validate the changes afterwards. By default, the repo will create the 2 PostgreSQL instances with everything you need.For this to work, we need:pglogical extension. A Publisher node (Source)A Subscriber node (Target)A Replication set for the tables to replication.A Subscription to the Publisher. Tables with primary keys. PostgreSQL config with wal_level = &#8216;logical&#8217;Also, this repo has some adjustments as backup, since these actions are not executed by default and would break the replication process: Scripts will add/remove columns to tables on source and target databases.Scripts will add new tables to the replication and to both databses.Scripts will remove tables from the replication set and drop tables on both databases.Thanks for reading The Pipe & The Line! Subscribe for free to receive new posts and support my work.&#128279; Starting the replicationTo get started, you can run:make build NUM_RECORDS=10000 make runThis will spin up both PostgreSQL services, and create source tables with 10K rows of fake data. Then, you run: make cdc-pglogicalSource (a.k.a provider/publisher):Create pglogical extension.Create a publisher node.Add tables to the replication set.By default, it will create a replication slot. Target (a.k.a subscriber):Create pglogical extension.Create a subscription to the publisher.&#128258; Sending changesYou can start sending changes, these commands will apply INSERT, DELETE, TRUNCATE or UPDATE on a single random table. You can decide on how many rows you apply the changes:NUM_RECORDS=5 make insert-data NUM_RECORDS=5 make delete-data NUM_RECORDS=5 make update-data make truncateThis is the same as doing (e.g. products table):Thanks for reading The Pipe & The Line! Subscribe to get useful content for your data career!&#128064; Try breaking the replication! If it wasn&#8217;t for the improvements to the scripts, all these upcoming actions would break the pglogical replication. Good thing about to pglogical compared to native PostgreSQL replication is that replication sets can cover entire schemas. If you add a new table for an already included schema, it will add it automatically, but you will still need to create it on the target database. If you need to drop a table, you need to remove it from the replication and also drop on the target database. This edge cases are covered by the scripts:You can test this scenario by running these commands in the repo. You will notice tables are created and drop in both databases: make create-table make drop-table Translates as (e.g. products table):Schema changes is another scenario, you can also test it:make add-column make drop-columnTranslates as (e.g. products table):The enhanced scripts are applying changes in both databases, thing that wouldn&#8217;t happen by default. As always, you can run these commands to validate all changes:&#128221; Conclusions & NotesBe default, pglogical will copy an initial snapshot when enabled.You need PRIMARY KEYs on the TABLES you want to replicate. Not VIEWS.This feature doesn&#8217;t replicate sequences, DDLs or schemas.Schema changes are ignored. Adding or dropping columns will do nothing.If you have set pglogical.replication_set_add_all_tables('default', ARRAY['public']); and run CREATE TABLE, you need to also add it on the target database, otherwise it will break the replication.The previous point works similarly to the DROP TABLE, you need to remove it from the replication set and then drop it on both sides.If you enjoyed the content, hit the like &#10084;&#65039; button, share, comment, repost, and all those nice things people do when like stuff these days. Glad to know you made it to this part!Hi, I am Alejandro Aboy. I am currently working as a Data Engineer. I started in digital marketing at 19. I gained experience in website tracking, advertising, and analytics. I also founded my agency. In 2021, I found my passion for data engineering. So, I shifted my career focus, despite lacking a CS degree. I'm now pursuing this path, leveraging my diverse experience and willingness to learn.Follow me on Linkedin"}]}
{"messages": [{"role": "user", "content": "Create a step-by-step guide for setting up pglogical in a PostgreSQL environment with troubleshooting tips."}, {"role": "assistant", "content": "There is another version of this post for native Logical Replication in PostgreSQL that I would suggest check beforehand:This repo has been adapted with some improvements to handle some edge cases and enhance the replication process!&#128104;&#127995;&#8205;&#128187; The environment I&#8217;ve created a repo to help me speed up my testing. &#128073; Github Repo: &#128259; Change Data Capture (CDC) using pglogical in PostgreSQL Graphic representation.In this context, I&#8217;ve enabled 2 PostgreSQL services with Docker to simulate CDC with a fake data generator I&#8217;ve adapted for this scenario. With some simple commands, you can simulate a bunch of scenarios such as INSERT, DELETE, UPDATE, DROP, TRUNCATE.Using Dbeaver or psql in your terminal can help you validate the changes afterwards. By default, the repo will create the 2 PostgreSQL instances with everything you need.For this to work, we need:pglogical extension. A Publisher node (Source)A Subscriber node (Target)A Replication set for the tables to replication.A Subscription to the Publisher. Tables with primary keys. PostgreSQL config with wal_level = &#8216;logical&#8217;Also, this repo has some adjustments as backup, since these actions are not executed by default and would break the replication process: Scripts will add/remove columns to tables on source and target databases.Scripts will add new tables to the replication and to both databses.Scripts will remove tables from the replication set and drop tables on both databases.Thanks for reading The Pipe & The Line! Subscribe for free to receive new posts and support my work.&#128279; Starting the replicationTo get started, you can run:make build NUM_RECORDS=10000 make runThis will spin up both PostgreSQL services, and create source tables with 10K rows of fake data. Then, you run: make cdc-pglogicalSource (a.k.a provider/publisher):Create pglogical extension.Create a publisher node.Add tables to the replication set.By default, it will create a replication slot. Target (a.k.a subscriber):Create pglogical extension.Create a subscription to the publisher.&#128258; Sending changesYou can start sending changes, these commands will apply INSERT, DELETE, TRUNCATE or UPDATE on a single random table. You can decide on how many rows you apply the changes:NUM_RECORDS=5 make insert-data NUM_RECORDS=5 make delete-data NUM_RECORDS=5 make update-data make truncateThis is the same as doing (e.g. products table):Thanks for reading The Pipe & The Line! Subscribe to get useful content for your data career!&#128064; Try breaking the replication! If it wasn&#8217;t for the improvements to the scripts, all these upcoming actions would break the pglogical replication. Good thing about to pglogical compared to native PostgreSQL replication is that replication sets can cover entire schemas. If you add a new table for an already included schema, it will add it automatically, but you will still need to create it on the target database. If you need to drop a table, you need to remove it from the replication and also drop on the target database. This edge cases are covered by the scripts:You can test this scenario by running these commands in the repo. You will notice tables are created and drop in both databases: make create-table make drop-table Translates as (e.g. products table):Schema changes is another scenario, you can also test it:make add-column make drop-columnTranslates as (e.g. products table):The enhanced scripts are applying changes in both databases, thing that wouldn&#8217;t happen by default. As always, you can run these commands to validate all changes:&#128221; Conclusions & NotesBe default, pglogical will copy an initial snapshot when enabled.You need PRIMARY KEYs on the TABLES you want to replicate. Not VIEWS.This feature doesn&#8217;t replicate sequences, DDLs or schemas.Schema changes are ignored. Adding or dropping columns will do nothing.If you have set pglogical.replication_set_add_all_tables('default', ARRAY['public']); and run CREATE TABLE, you need to also add it on the target database, otherwise it will break the replication.The previous point works similarly to the DROP TABLE, you need to remove it from the replication set and then drop it on both sides.If you enjoyed the content, hit the like &#10084;&#65039; button, share, comment, repost, and all those nice things people do when like stuff these days. Glad to know you made it to this part!Hi, I am Alejandro Aboy. I am currently working as a Data Engineer. I started in digital marketing at 19. I gained experience in website tracking, advertising, and analytics. I also founded my agency. In 2021, I found my passion for data engineering. So, I shifted my career focus, despite lacking a CS degree. I'm now pursuing this path, leveraging my diverse experience and willingness to learn.Follow me on Linkedin"}]}
{"messages": [{"role": "user", "content": "Explain how to prioritize learning tech concepts based on real-world applications in data engineering."}, {"role": "assistant", "content": "I will start this article with a Linkedin post that emphasizes an important mindset that I strongly believe in:Often, we feel overwhelmed by the endless array of tech concepts.There&#8217;s much to learn, read, and experiment with before achieving a decent level of expertise.And that&#8217;s where challenges arise:You are compelled to learn out of necessity.You misunderstand the contextual needs, leading you to learn the wrong things.You lack real use cases to apply your new skills.But there is a way forward.Learning To LearnThroughout your learning journey, three components are essential:What motivates you to learn.What you need to learn professionally.What the current situation requires.Inspired Ikigai diagram.Matching these three circles in perfect sync is challenging.Why?Consider an example of a less-than-ideal situation:Your company&#8217;s website might be down due to server network issues.Servers can be boring and uninteresting, but you should know enough to tackle the problem.Or, here&#8217;s a better-matching scenario:You&#8217;re highly interested in learning about data orchestration.Your company is struggling with multiple cron jobs that are difficult to maintain.You initiate a migration project to Apache Airflow and gain valuable experience in the process.When employees face repeated scenarios like the first one, they may eventually leave their companies.The main reason is a lack of alignment between motivation, professional experience, and the company&#8217;s context.Scenarios like the second one are harder to find but foster balanced engagement, allowing employees to maintain a growth mindset without endless boredom.Thanks for reading The Pipe & The Line! Subscribe for free to receive new posts and support my work.What You Want (Motivation)Sometimes, you end up learning the wrong things just because they seem cool or new.Exciting tools and shiny objects can be distractions if not properly balanced.Done wisely, however, these can accelerate your learning and make it enjoyable!The biggest challenge is finding a relevant problem (or &#8220;excuse&#8221;) to learn what motivates you.One helpful approach is to ask yourself:&#8220;Can I spend hours on this topic without noticing the time?&#8221;This is pure hands-on practice, without roadmaps, guidelines, or manuals.What You Need (Experience)You may often find advice on LinkedIn infographics with roadmaps. From these, determine your ideal next step.For example, learning Git will help you collaborate in any tech team you join in your career. It might not be exciting, but it&#8217;s a useful skill.Conversely, if you plan to work at small data companies, learning tools like Spark may be unnecessary, even if it seems exciting.Ask yourself:&#8220;What knowledge will make a difference in my career in the coming months or years?&#8221;You can pursue this through courses, bootcamps, or books. Linking it with motivation will speed up your skill development!Thanks for reading The Pipe & The Line! This post is public so feel free to share it.ShareWhat the Situation Requires (Problems)Simply observe your surroundings. If you join a company with tech assets you&#8217;re unfamiliar with, it&#8217;s usually clear where to start.Good managers or project managers can be helpful, but if you&#8217;re on your own, context can be a double-edged sword.Regardless of the environment, learn to prioritize issues and avoid creating problems unnecessarily.The key takeaway is this:Learn when doing nothing is the right choice.At times, you may receive random requirements&#8212;to automate processes, create dashboards, or build apps that may be irrelevant by the time they&#8217;re delivered.Learning to filter noise is a complex skill that you&#8217;ll develop over time.By understanding WHEN not to act, you&#8217;ll gain clarity on WHAT actions to take when they matter.Solve Your Own (Stupid) ProblemsApplying these principles to personal issues can help you decide what to learn next.This approach is often driven by motivation, but it also enhances your ability to manage context.Recently, some of my personal projects have included:A water tracker, created because I was tired of watching one-minute ads on Candy Crush.A portable data stack concept to learn about dbt-core, DuckDB, and Docker Compose.An RSS feed AI summarizer, so I can quickly decide which newsletters to read without them piling up in my inbox.Some of these tools are useful in my current company, allowing me to make more informed decisions.Others save me a few hours each day but are unrelated to my job&#8217;s main tools.You know your challenges best, and if not, it&#8217;s time to sharpen that skill!ConsiderationsA few important points to keep in mind:Not all topics are suitable for this personal approach: (e.g.) Learning about infrastructure is pretty hard with personal stuff. I wouldn&#8217;t recommend (even to my worst enemy) setting up Kubernetes Cluster to host a water tracker. Use the right amount of effort: If a personal project like a water tracker takes you three weeks, something may be off. When projects from scratch take me more than 2 days (or 2 hours if using LLMs), I start re-evaluating my approach.Define the problem before choosing tools&#8212;never skip this step. You&#8217;ll save a lot of time by clarifying the scope first. Deadlines here are flexible, but setting artificial limits can help you build the right thing at the right time.Find the optimal intersection between motivation, professional experience, and context to keep encountering rewarding challenges.Remember to enjoy technology, keep learning, and have fun!&#128221; TL;DRLearning to learnWhat you want (Motivation)What you need (Experience)What the situation requires (Problems)Solve your own (stupid) problemsConsiderationsIf you enjoyed the content, hit the like &#10084;&#65039; button, share, comment, repost, and all those nice things people do when like stuff these days. Glad to know you made it to this part!Hi, I am Alejandro Aboy. I am currently working as a Data Engineer. I started in digital marketing at 19. I gained experience in website tracking, advertising, and analytics. I also founded my agency. In 2021, I found my passion for data engineering. So, I shifted my career focus, despite lacking a CS degree. I'm now pursuing this path, leveraging my diverse experience and willingness to learn.Follow me on Linkedin"}]}
{"messages": [{"role": "user", "content": "Provide actionable career advice for tech professionals to navigate skill gaps and industry demands."}, {"role": "assistant", "content": "I will start this article with a Linkedin post that emphasizes an important mindset that I strongly believe in:Often, we feel overwhelmed by the endless array of tech concepts.There&#8217;s much to learn, read, and experiment with before achieving a decent level of expertise.And that&#8217;s where challenges arise:You are compelled to learn out of necessity.You misunderstand the contextual needs, leading you to learn the wrong things.You lack real use cases to apply your new skills.But there is a way forward.Learning To LearnThroughout your learning journey, three components are essential:What motivates you to learn.What you need to learn professionally.What the current situation requires.Inspired Ikigai diagram.Matching these three circles in perfect sync is challenging.Why?Consider an example of a less-than-ideal situation:Your company&#8217;s website might be down due to server network issues.Servers can be boring and uninteresting, but you should know enough to tackle the problem.Or, here&#8217;s a better-matching scenario:You&#8217;re highly interested in learning about data orchestration.Your company is struggling with multiple cron jobs that are difficult to maintain.You initiate a migration project to Apache Airflow and gain valuable experience in the process.When employees face repeated scenarios like the first one, they may eventually leave their companies.The main reason is a lack of alignment between motivation, professional experience, and the company&#8217;s context.Scenarios like the second one are harder to find but foster balanced engagement, allowing employees to maintain a growth mindset without endless boredom.Thanks for reading The Pipe & The Line! Subscribe for free to receive new posts and support my work.What You Want (Motivation)Sometimes, you end up learning the wrong things just because they seem cool or new.Exciting tools and shiny objects can be distractions if not properly balanced.Done wisely, however, these can accelerate your learning and make it enjoyable!The biggest challenge is finding a relevant problem (or &#8220;excuse&#8221;) to learn what motivates you.One helpful approach is to ask yourself:&#8220;Can I spend hours on this topic without noticing the time?&#8221;This is pure hands-on practice, without roadmaps, guidelines, or manuals.What You Need (Experience)You may often find advice on LinkedIn infographics with roadmaps. From these, determine your ideal next step.For example, learning Git will help you collaborate in any tech team you join in your career. It might not be exciting, but it&#8217;s a useful skill.Conversely, if you plan to work at small data companies, learning tools like Spark may be unnecessary, even if it seems exciting.Ask yourself:&#8220;What knowledge will make a difference in my career in the coming months or years?&#8221;You can pursue this through courses, bootcamps, or books. Linking it with motivation will speed up your skill development!Thanks for reading The Pipe & The Line! This post is public so feel free to share it.ShareWhat the Situation Requires (Problems)Simply observe your surroundings. If you join a company with tech assets you&#8217;re unfamiliar with, it&#8217;s usually clear where to start.Good managers or project managers can be helpful, but if you&#8217;re on your own, context can be a double-edged sword.Regardless of the environment, learn to prioritize issues and avoid creating problems unnecessarily.The key takeaway is this:Learn when doing nothing is the right choice.At times, you may receive random requirements&#8212;to automate processes, create dashboards, or build apps that may be irrelevant by the time they&#8217;re delivered.Learning to filter noise is a complex skill that you&#8217;ll develop over time.By understanding WHEN not to act, you&#8217;ll gain clarity on WHAT actions to take when they matter.Solve Your Own (Stupid) ProblemsApplying these principles to personal issues can help you decide what to learn next.This approach is often driven by motivation, but it also enhances your ability to manage context.Recently, some of my personal projects have included:A water tracker, created because I was tired of watching one-minute ads on Candy Crush.A portable data stack concept to learn about dbt-core, DuckDB, and Docker Compose.An RSS feed AI summarizer, so I can quickly decide which newsletters to read without them piling up in my inbox.Some of these tools are useful in my current company, allowing me to make more informed decisions.Others save me a few hours each day but are unrelated to my job&#8217;s main tools.You know your challenges best, and if not, it&#8217;s time to sharpen that skill!ConsiderationsA few important points to keep in mind:Not all topics are suitable for this personal approach: (e.g.) Learning about infrastructure is pretty hard with personal stuff. I wouldn&#8217;t recommend (even to my worst enemy) setting up Kubernetes Cluster to host a water tracker. Use the right amount of effort: If a personal project like a water tracker takes you three weeks, something may be off. When projects from scratch take me more than 2 days (or 2 hours if using LLMs), I start re-evaluating my approach.Define the problem before choosing tools&#8212;never skip this step. You&#8217;ll save a lot of time by clarifying the scope first. Deadlines here are flexible, but setting artificial limits can help you build the right thing at the right time.Find the optimal intersection between motivation, professional experience, and context to keep encountering rewarding challenges.Remember to enjoy technology, keep learning, and have fun!&#128221; TL;DRLearning to learnWhat you want (Motivation)What you need (Experience)What the situation requires (Problems)Solve your own (stupid) problemsConsiderationsIf you enjoyed the content, hit the like &#10084;&#65039; button, share, comment, repost, and all those nice things people do when like stuff these days. Glad to know you made it to this part!Hi, I am Alejandro Aboy. I am currently working as a Data Engineer. I started in digital marketing at 19. I gained experience in website tracking, advertising, and analytics. I also founded my agency. In 2021, I found my passion for data engineering. So, I shifted my career focus, despite lacking a CS degree. I'm now pursuing this path, leveraging my diverse experience and willingness to learn.Follow me on Linkedin"}]}
{"messages": [{"role": "user", "content": "Outline a practical guide for setting up personal projects to reinforce new technical skills effectively."}, {"role": "assistant", "content": "I will start this article with a Linkedin post that emphasizes an important mindset that I strongly believe in:Often, we feel overwhelmed by the endless array of tech concepts.There&#8217;s much to learn, read, and experiment with before achieving a decent level of expertise.And that&#8217;s where challenges arise:You are compelled to learn out of necessity.You misunderstand the contextual needs, leading you to learn the wrong things.You lack real use cases to apply your new skills.But there is a way forward.Learning To LearnThroughout your learning journey, three components are essential:What motivates you to learn.What you need to learn professionally.What the current situation requires.Inspired Ikigai diagram.Matching these three circles in perfect sync is challenging.Why?Consider an example of a less-than-ideal situation:Your company&#8217;s website might be down due to server network issues.Servers can be boring and uninteresting, but you should know enough to tackle the problem.Or, here&#8217;s a better-matching scenario:You&#8217;re highly interested in learning about data orchestration.Your company is struggling with multiple cron jobs that are difficult to maintain.You initiate a migration project to Apache Airflow and gain valuable experience in the process.When employees face repeated scenarios like the first one, they may eventually leave their companies.The main reason is a lack of alignment between motivation, professional experience, and the company&#8217;s context.Scenarios like the second one are harder to find but foster balanced engagement, allowing employees to maintain a growth mindset without endless boredom.Thanks for reading The Pipe & The Line! Subscribe for free to receive new posts and support my work.What You Want (Motivation)Sometimes, you end up learning the wrong things just because they seem cool or new.Exciting tools and shiny objects can be distractions if not properly balanced.Done wisely, however, these can accelerate your learning and make it enjoyable!The biggest challenge is finding a relevant problem (or &#8220;excuse&#8221;) to learn what motivates you.One helpful approach is to ask yourself:&#8220;Can I spend hours on this topic without noticing the time?&#8221;This is pure hands-on practice, without roadmaps, guidelines, or manuals.What You Need (Experience)You may often find advice on LinkedIn infographics with roadmaps. From these, determine your ideal next step.For example, learning Git will help you collaborate in any tech team you join in your career. It might not be exciting, but it&#8217;s a useful skill.Conversely, if you plan to work at small data companies, learning tools like Spark may be unnecessary, even if it seems exciting.Ask yourself:&#8220;What knowledge will make a difference in my career in the coming months or years?&#8221;You can pursue this through courses, bootcamps, or books. Linking it with motivation will speed up your skill development!Thanks for reading The Pipe & The Line! This post is public so feel free to share it.ShareWhat the Situation Requires (Problems)Simply observe your surroundings. If you join a company with tech assets you&#8217;re unfamiliar with, it&#8217;s usually clear where to start.Good managers or project managers can be helpful, but if you&#8217;re on your own, context can be a double-edged sword.Regardless of the environment, learn to prioritize issues and avoid creating problems unnecessarily.The key takeaway is this:Learn when doing nothing is the right choice.At times, you may receive random requirements&#8212;to automate processes, create dashboards, or build apps that may be irrelevant by the time they&#8217;re delivered.Learning to filter noise is a complex skill that you&#8217;ll develop over time.By understanding WHEN not to act, you&#8217;ll gain clarity on WHAT actions to take when they matter.Solve Your Own (Stupid) ProblemsApplying these principles to personal issues can help you decide what to learn next.This approach is often driven by motivation, but it also enhances your ability to manage context.Recently, some of my personal projects have included:A water tracker, created because I was tired of watching one-minute ads on Candy Crush.A portable data stack concept to learn about dbt-core, DuckDB, and Docker Compose.An RSS feed AI summarizer, so I can quickly decide which newsletters to read without them piling up in my inbox.Some of these tools are useful in my current company, allowing me to make more informed decisions.Others save me a few hours each day but are unrelated to my job&#8217;s main tools.You know your challenges best, and if not, it&#8217;s time to sharpen that skill!ConsiderationsA few important points to keep in mind:Not all topics are suitable for this personal approach: (e.g.) Learning about infrastructure is pretty hard with personal stuff. I wouldn&#8217;t recommend (even to my worst enemy) setting up Kubernetes Cluster to host a water tracker. Use the right amount of effort: If a personal project like a water tracker takes you three weeks, something may be off. When projects from scratch take me more than 2 days (or 2 hours if using LLMs), I start re-evaluating my approach.Define the problem before choosing tools&#8212;never skip this step. You&#8217;ll save a lot of time by clarifying the scope first. Deadlines here are flexible, but setting artificial limits can help you build the right thing at the right time.Find the optimal intersection between motivation, professional experience, and context to keep encountering rewarding challenges.Remember to enjoy technology, keep learning, and have fun!&#128221; TL;DRLearning to learnWhat you want (Motivation)What you need (Experience)What the situation requires (Problems)Solve your own (stupid) problemsConsiderationsIf you enjoyed the content, hit the like &#10084;&#65039; button, share, comment, repost, and all those nice things people do when like stuff these days. Glad to know you made it to this part!Hi, I am Alejandro Aboy. I am currently working as a Data Engineer. I started in digital marketing at 19. I gained experience in website tracking, advertising, and analytics. I also founded my agency. In 2021, I found my passion for data engineering. So, I shifted my career focus, despite lacking a CS degree. I'm now pursuing this path, leveraging my diverse experience and willingness to learn.Follow me on Linkedin"}]}
{"messages": [{"role": "user", "content": "Explain how to implement Change Data Capture in PostgreSQL using built-in logical replication features."}, {"role": "assistant", "content": "As one of my learning goals, I&#8217;ve been searching for Change Data Capture (CDC) alternatives for PostgreSQL recently.If you are new to CDC, you might want to check this post first: While conducting my research, I found that most use cases involve taking data from a source (e.g., PostgreSQL) and moving the payload to an object storage target (e.g., AWS S3).However, it has been difficult to find a more basic use case, such as PostgreSQL-to-PostgreSQL replication.As usual, there are major players in this space, such as Fivetran, Airbyte, and Estuary, among others.The downside is that costs escalate quickly, often without notice.If you really want to learn, try experiencing the challenges of progressively setting up and maintaining open-source tools.Starting small always allows you to gain perspective before relying on black-box tools to handle everything for you.&#128104;&#127995;&#8205;&#128187; The environment I&#8217;ve created a repo to help me speed up my testing. &#128073; Github Repo: &#128259; Change Data Capture (CDC) using Logical Replication in PostgreSQL Graphic representation.In this context, I&#8217;ve enabled 2 PostgreSQL services with Docker to simulate CDC with a fake data generator I&#8217;ve adapted for this scenario. With some simple commands, you can simulate a bunch of scenarios such as INSERT, DELETE, UPDATE, DROP, TRUNCATE.Using Dbeaver or psql in your terminal can help you validate the changes afterwards. By default, the repo will create the 2 PostgreSQL instances with everything you need.For this to work, we need:A Replication Slot in the Source Database.A Publication with a set tables to replicate.A Subscription to receive those changes into the Target Database.Tables with primary keys. PostgreSQL config with wal_level = &#8216;logical&#8217;Thanks for reading The Pipe & The Line! Subscribe for free to receive new posts and support my work.&#128279; Starting the replicationTo get started, you can clone the repo and run:make build NUM_RECORDS=10000 make run This will spin up both PostgreSQL services, and create source tables with 10K rows of fake data. Then, you run: make cdc-logical-replicationThis will enable the CDC process between both services. We will see that command does below.Source (a.k.a provider/publisher):Create a replication slot.Create a publication for specific tables or ALL TABLES.Verify that the slot has been created.Verify the tables you&#8217;ve chosen are listed.Target (a.k.a subscriber):Create the subscription.Validate that the subscription was created.&#128258; Sending changesYou can start sending changes, these commands will apply INSERT, DELETE, TRUNCATE or UPDATE on random tables. You can decide on how many rows you apply the changes:NUM_RECORDS=500 make insert-data NUM_RECORDS=500 make delete-data NUM_RECORDS=500 make update-data make truncateThis is the same as doing (e.g. products table):Thanks for reading The Pipe & The Line! Subscribe to get useful content for your data career!&#128557; Break the replication!If you haven&#8217;t set the publication to run for ALL TABLES, adding a new table won&#8217;t change a thing, because you need to add it or remove it explicitly to/from the replication. Bear in mind that if you create a table in the source, you will need to the same in the target, it won&#8217;t be created by the replication.If you drop a table that was included in the replication, it will break the replication process and you will need to recreate it.You can test this scenario by running these commands in the repo: make create-table make drop-table Translates as (e.g. products table):Schema changes is another scenario, you can also test it:make add-column make drop-columnTranslates as (e.g. products table):You will notice this is completely ignored by the replication process. As always, you can run these commands to validate all changes:&#128221; Conclusions & NotesFor a one-shot replication, this might be an amazing solution.If you don&#8217;t create new schema often, the workflow is pretty simple.You should automate addition of new tables to the replication sets. You need PRIMARY KEYs on the TABLES you want to replicate. Not views.By default, it will do a full snapshot or the current table. You can disable this by using copy_data=false when creating the subscription on the target database. This feature doesn&#8217;t replicate sequences, DDLs or schemas.Schema changes are ignored. Adding or dropping columns will do nothing. Afterwards the replication will be broken.If you have ALL TABLES included in your publication and you create a table, you need to also add it on the target database, otherwise it will just ignore it.If you have ALL TABLES included in your publication and you drop a table, replication will be broken.If you enjoyed the content, hit the like &#10084;&#65039; button, share, comment, repost, and all those nice things people do when like stuff these days. Glad to know you made it to this part!Hi, I am Alejandro Aboy. I am currently working as a Data Engineer. I started in digital marketing at 19. I gained experience in website tracking, advertising, and analytics. I also founded my agency. In 2021, I found my passion for data engineering. So, I shifted my career focus, despite lacking a CS degree. I'm now pursuing this path, leveraging my diverse experience and willingness to learn.Follow me on Linkedin"}]}
{"messages": [{"role": "user", "content": "Discuss career opportunities in data engineering focusing on skills related to CDC and data replication technologies."}, {"role": "assistant", "content": "As one of my learning goals, I&#8217;ve been searching for Change Data Capture (CDC) alternatives for PostgreSQL recently.If you are new to CDC, you might want to check this post first: While conducting my research, I found that most use cases involve taking data from a source (e.g., PostgreSQL) and moving the payload to an object storage target (e.g., AWS S3).However, it has been difficult to find a more basic use case, such as PostgreSQL-to-PostgreSQL replication.As usual, there are major players in this space, such as Fivetran, Airbyte, and Estuary, among others.The downside is that costs escalate quickly, often without notice.If you really want to learn, try experiencing the challenges of progressively setting up and maintaining open-source tools.Starting small always allows you to gain perspective before relying on black-box tools to handle everything for you.&#128104;&#127995;&#8205;&#128187; The environment I&#8217;ve created a repo to help me speed up my testing. &#128073; Github Repo: &#128259; Change Data Capture (CDC) using Logical Replication in PostgreSQL Graphic representation.In this context, I&#8217;ve enabled 2 PostgreSQL services with Docker to simulate CDC with a fake data generator I&#8217;ve adapted for this scenario. With some simple commands, you can simulate a bunch of scenarios such as INSERT, DELETE, UPDATE, DROP, TRUNCATE.Using Dbeaver or psql in your terminal can help you validate the changes afterwards. By default, the repo will create the 2 PostgreSQL instances with everything you need.For this to work, we need:A Replication Slot in the Source Database.A Publication with a set tables to replicate.A Subscription to receive those changes into the Target Database.Tables with primary keys. PostgreSQL config with wal_level = &#8216;logical&#8217;Thanks for reading The Pipe & The Line! Subscribe for free to receive new posts and support my work.&#128279; Starting the replicationTo get started, you can clone the repo and run:make build NUM_RECORDS=10000 make run This will spin up both PostgreSQL services, and create source tables with 10K rows of fake data. Then, you run: make cdc-logical-replicationThis will enable the CDC process between both services. We will see that command does below.Source (a.k.a provider/publisher):Create a replication slot.Create a publication for specific tables or ALL TABLES.Verify that the slot has been created.Verify the tables you&#8217;ve chosen are listed.Target (a.k.a subscriber):Create the subscription.Validate that the subscription was created.&#128258; Sending changesYou can start sending changes, these commands will apply INSERT, DELETE, TRUNCATE or UPDATE on random tables. You can decide on how many rows you apply the changes:NUM_RECORDS=500 make insert-data NUM_RECORDS=500 make delete-data NUM_RECORDS=500 make update-data make truncateThis is the same as doing (e.g. products table):Thanks for reading The Pipe & The Line! Subscribe to get useful content for your data career!&#128557; Break the replication!If you haven&#8217;t set the publication to run for ALL TABLES, adding a new table won&#8217;t change a thing, because you need to add it or remove it explicitly to/from the replication. Bear in mind that if you create a table in the source, you will need to the same in the target, it won&#8217;t be created by the replication.If you drop a table that was included in the replication, it will break the replication process and you will need to recreate it.You can test this scenario by running these commands in the repo: make create-table make drop-table Translates as (e.g. products table):Schema changes is another scenario, you can also test it:make add-column make drop-columnTranslates as (e.g. products table):You will notice this is completely ignored by the replication process. As always, you can run these commands to validate all changes:&#128221; Conclusions & NotesFor a one-shot replication, this might be an amazing solution.If you don&#8217;t create new schema often, the workflow is pretty simple.You should automate addition of new tables to the replication sets. You need PRIMARY KEYs on the TABLES you want to replicate. Not views.By default, it will do a full snapshot or the current table. You can disable this by using copy_data=false when creating the subscription on the target database. This feature doesn&#8217;t replicate sequences, DDLs or schemas.Schema changes are ignored. Adding or dropping columns will do nothing. Afterwards the replication will be broken.If you have ALL TABLES included in your publication and you create a table, you need to also add it on the target database, otherwise it will just ignore it.If you have ALL TABLES included in your publication and you drop a table, replication will be broken.If you enjoyed the content, hit the like &#10084;&#65039; button, share, comment, repost, and all those nice things people do when like stuff these days. Glad to know you made it to this part!Hi, I am Alejandro Aboy. I am currently working as a Data Engineer. I started in digital marketing at 19. I gained experience in website tracking, advertising, and analytics. I also founded my agency. In 2021, I found my passion for data engineering. So, I shifted my career focus, despite lacking a CS degree. I'm now pursuing this path, leveraging my diverse experience and willingness to learn.Follow me on Linkedin"}]}
{"messages": [{"role": "user", "content": "Create a practical guide for setting up PostgreSQL-to-PostgreSQL replication, including configuration and best practices."}, {"role": "assistant", "content": "As one of my learning goals, I&#8217;ve been searching for Change Data Capture (CDC) alternatives for PostgreSQL recently.If you are new to CDC, you might want to check this post first: While conducting my research, I found that most use cases involve taking data from a source (e.g., PostgreSQL) and moving the payload to an object storage target (e.g., AWS S3).However, it has been difficult to find a more basic use case, such as PostgreSQL-to-PostgreSQL replication.As usual, there are major players in this space, such as Fivetran, Airbyte, and Estuary, among others.The downside is that costs escalate quickly, often without notice.If you really want to learn, try experiencing the challenges of progressively setting up and maintaining open-source tools.Starting small always allows you to gain perspective before relying on black-box tools to handle everything for you.&#128104;&#127995;&#8205;&#128187; The environment I&#8217;ve created a repo to help me speed up my testing. &#128073; Github Repo: &#128259; Change Data Capture (CDC) using Logical Replication in PostgreSQL Graphic representation.In this context, I&#8217;ve enabled 2 PostgreSQL services with Docker to simulate CDC with a fake data generator I&#8217;ve adapted for this scenario. With some simple commands, you can simulate a bunch of scenarios such as INSERT, DELETE, UPDATE, DROP, TRUNCATE.Using Dbeaver or psql in your terminal can help you validate the changes afterwards. By default, the repo will create the 2 PostgreSQL instances with everything you need.For this to work, we need:A Replication Slot in the Source Database.A Publication with a set tables to replicate.A Subscription to receive those changes into the Target Database.Tables with primary keys. PostgreSQL config with wal_level = &#8216;logical&#8217;Thanks for reading The Pipe & The Line! Subscribe for free to receive new posts and support my work.&#128279; Starting the replicationTo get started, you can clone the repo and run:make build NUM_RECORDS=10000 make run This will spin up both PostgreSQL services, and create source tables with 10K rows of fake data. Then, you run: make cdc-logical-replicationThis will enable the CDC process between both services. We will see that command does below.Source (a.k.a provider/publisher):Create a replication slot.Create a publication for specific tables or ALL TABLES.Verify that the slot has been created.Verify the tables you&#8217;ve chosen are listed.Target (a.k.a subscriber):Create the subscription.Validate that the subscription was created.&#128258; Sending changesYou can start sending changes, these commands will apply INSERT, DELETE, TRUNCATE or UPDATE on random tables. You can decide on how many rows you apply the changes:NUM_RECORDS=500 make insert-data NUM_RECORDS=500 make delete-data NUM_RECORDS=500 make update-data make truncateThis is the same as doing (e.g. products table):Thanks for reading The Pipe & The Line! Subscribe to get useful content for your data career!&#128557; Break the replication!If you haven&#8217;t set the publication to run for ALL TABLES, adding a new table won&#8217;t change a thing, because you need to add it or remove it explicitly to/from the replication. Bear in mind that if you create a table in the source, you will need to the same in the target, it won&#8217;t be created by the replication.If you drop a table that was included in the replication, it will break the replication process and you will need to recreate it.You can test this scenario by running these commands in the repo: make create-table make drop-table Translates as (e.g. products table):Schema changes is another scenario, you can also test it:make add-column make drop-columnTranslates as (e.g. products table):You will notice this is completely ignored by the replication process. As always, you can run these commands to validate all changes:&#128221; Conclusions & NotesFor a one-shot replication, this might be an amazing solution.If you don&#8217;t create new schema often, the workflow is pretty simple.You should automate addition of new tables to the replication sets. You need PRIMARY KEYs on the TABLES you want to replicate. Not views.By default, it will do a full snapshot or the current table. You can disable this by using copy_data=false when creating the subscription on the target database. This feature doesn&#8217;t replicate sequences, DDLs or schemas.Schema changes are ignored. Adding or dropping columns will do nothing. Afterwards the replication will be broken.If you have ALL TABLES included in your publication and you create a table, you need to also add it on the target database, otherwise it will just ignore it.If you have ALL TABLES included in your publication and you drop a table, replication will be broken.If you enjoyed the content, hit the like &#10084;&#65039; button, share, comment, repost, and all those nice things people do when like stuff these days. Glad to know you made it to this part!Hi, I am Alejandro Aboy. I am currently working as a Data Engineer. I started in digital marketing at 19. I gained experience in website tracking, advertising, and analytics. I also founded my agency. In 2021, I found my passion for data engineering. So, I shifted my career focus, despite lacking a CS degree. I'm now pursuing this path, leveraging my diverse experience and willingness to learn.Follow me on Linkedin"}]}
{"messages": [{"role": "user", "content": "Discuss the importance of choosing the right data tools for small projects and avoiding overcomplication."}, {"role": "assistant", "content": "There are lots of shiny objects we encounter that make us forget about simplicity and the principle of &#8220;the right tool for the job&#8221;.Most of the time, small startups or even companies with small data volumes fall for these new tools.In the end, they are simply overkill.Then, the domino effect starts cascading into:Expensive Fivetran setupsHuge bills from BigQueryNumerous dbt models with SELECT * FROM table;And much more.Sometimes, the best way to start is less sexy.&#128230; The Power of Portability Over the past few months, I&#8217;ve been learning in-depth concepts about Docker and portability.If you are also interested, you can read:There are tons of companies overkilling data stacks with huge setups that end up leveraging 1% of those features.The funny thing is, we are living in amazing times where you can achieve incredible results with tiny but powerful tools.You can utilize a 'data stack in a box' concept and use it on demand and be able to recreate it without all the painful maintenance when its not needed.Also, it can help you save tons of unnecessary costs even if you use this kind of approaches on top of your modern data stack tools.&#128104;&#127995;&#8205;&#128187; Fundamentals: take only what you needMost of the time, data stacks consist of these pieces:ETL workflowsDatabaseData visualizationOrchestrationThis is the most common pattern that you will find if you work in 95% of data teams around the world.There&#8217;s a lot of hype around real-time with Kafka & Flink, big data processing with Spark and open table format with Iceberg or Delta Lake.But those use cases are exclusive to a subset of companies that have a suitable volume to make it worth it. At the end of the day, you need to move data from A to B, add some logic, and automate it so you don&#8217;t hit the button every day.That&#8217;s it. Don&#8217;t kill mosquitos with a bazooka! Simple representation of the usual data engineering pattern.Thanks for reading The Pipe & The Line! Subscribe for free to receive new posts and support my work.&#128270; Proof of ConceptTo test this concept, I&#8217;ve created a repo:&#128073; Github Repo Data Stack To Go With DuckDB & dbtGraphic view of the repository.This repo is calling 3 endpoints from PiwikPRO API to get traffic analytics data. Every day; Github Actions will build the Docker container on its runner.Call the APIs to get the data and save it into DuckDB database, all with pure Python. dbt-core will build the models and run all test validations.Again, with pure Python, we will get the dbt output in DuckDB and export it to Google Sheets.Google Sheets is connected to a Looker Studio to build quick and nice dashboard to answer business questions. In the upcoming sections I will go through each tool used! &#128736;&#65039; The right tool for the jobWe&#8217;ll go through the tools one by one to understand what is the bare minimum that can do a huge difference in your portable stack: &#128013; Pure Python (Extraction and Loading)Nothing much to say.No need to learn Airflow decorators or random syntax, just call APIs with pure python code!&#129414; DuckDB (In memory Database)In this other post, I&#8217;ve compared Pandas, Polars and DuckDB based on file types and performance.What DuckDB can do is beyond comprehension.You can query millions of rows in a couple of seconds without major complications.You need to come a really long way before thinking that you need another tool to achieve what you want. And, the best part is you can work with SQL when using it!DuckDB reading times based on different files.&#128258; dbt-core (Transformation)At the moment, the main character in the whole data space covering the &#8220;T&#8221; in the ELT.It&#8217;s pretty easy to build data models with custom logic and let the tool connect the dots with sequential jinja templating. Back in the day, doing this with stored procedures was hell compared to this current approach. But, options such as SQL-mesh or SDF are starting to compete with interesting solutions. Your model results after running!&#128354; Github Actions (Orchestration) It&#8217;s free, it has no learning curve. And it works as a simple CRON. You don&#8217;t need to go with Airflow right away. Forget about it!Example of your Github Actions view.&#128196; Google Sheets (Database)Data people are formatted to hate GSheets because stakeholders ask them to export reports to work with the data on a spreadsheet.If you can beat them, join them. Yes, you will need to become familiar with setting up service accounts to interact with the Google Sheets API, but that&#8217;s straightforward with proper documentation.Besides, really cool things are being cooked around DuckDB and spreadsheets! Archie has been working on a DuckDB extensio that will change lots of workflows.&#128202; Looker Studio (Data Viz)Make easy dashboards without weird customization and logics. As you update your Google Sheets, the native connection will do the rest.Is more than you need to get started, PowerBI or Tableau can wait. Built this in 15&#8217;.Thanks for reading The Pipe & The Line! Subscribe to receive content for your data career!&#128064; OutroYou need to experiment to find the most suitable tools for your use-case and think in the long-term.You always need to study the context of your company and be mindful about all scenarios possible.For example, Google Sheets has a limit of 10M cells at the moment. This number can be retrieved if you multiply rows by columns. That said, if you are worried about it, your data volume might need another solution!Practice, research, and make the most out of all the tools at your disposal! However, don&#8217;t take shortcuts, as the quickest solution is not always the best.&#128221; TL;DR&#128230; The Power of Portability &#128104;&#127995;&#8205;&#128187; Fundamentals: take only what you need&#128270; Proof of Concept&#128736;&#65039; The right tool for the jobIf you enjoyed the content, hit the like &#10084;&#65039; button, share, comment, repost, and all those nice things people do when like stuff these days. Glad to know you made it to this part!Hi, I am Alejandro Aboy. I am currently working as a Data Engineer. I started in digital marketing at 19. I gained experience in website tracking, advertising, and analytics. I also founded my agency. In 2021, I found my passion for data engineering. So, I shifted my career focus, despite lacking a CS degree. I'm now pursuing this path, leveraging my diverse experience and willingness to learn.Follow me on Linkedin"}]}
{"messages": [{"role": "user", "content": "Create a practical guide on building a cost-effective data stack using open-source tools for startups."}, {"role": "assistant", "content": "There are lots of shiny objects we encounter that make us forget about simplicity and the principle of &#8220;the right tool for the job&#8221;.Most of the time, small startups or even companies with small data volumes fall for these new tools.In the end, they are simply overkill.Then, the domino effect starts cascading into:Expensive Fivetran setupsHuge bills from BigQueryNumerous dbt models with SELECT * FROM table;And much more.Sometimes, the best way to start is less sexy.&#128230; The Power of Portability Over the past few months, I&#8217;ve been learning in-depth concepts about Docker and portability.If you are also interested, you can read:There are tons of companies overkilling data stacks with huge setups that end up leveraging 1% of those features.The funny thing is, we are living in amazing times where you can achieve incredible results with tiny but powerful tools.You can utilize a 'data stack in a box' concept and use it on demand and be able to recreate it without all the painful maintenance when its not needed.Also, it can help you save tons of unnecessary costs even if you use this kind of approaches on top of your modern data stack tools.&#128104;&#127995;&#8205;&#128187; Fundamentals: take only what you needMost of the time, data stacks consist of these pieces:ETL workflowsDatabaseData visualizationOrchestrationThis is the most common pattern that you will find if you work in 95% of data teams around the world.There&#8217;s a lot of hype around real-time with Kafka & Flink, big data processing with Spark and open table format with Iceberg or Delta Lake.But those use cases are exclusive to a subset of companies that have a suitable volume to make it worth it. At the end of the day, you need to move data from A to B, add some logic, and automate it so you don&#8217;t hit the button every day.That&#8217;s it. Don&#8217;t kill mosquitos with a bazooka! Simple representation of the usual data engineering pattern.Thanks for reading The Pipe & The Line! Subscribe for free to receive new posts and support my work.&#128270; Proof of ConceptTo test this concept, I&#8217;ve created a repo:&#128073; Github Repo Data Stack To Go With DuckDB & dbtGraphic view of the repository.This repo is calling 3 endpoints from PiwikPRO API to get traffic analytics data. Every day; Github Actions will build the Docker container on its runner.Call the APIs to get the data and save it into DuckDB database, all with pure Python. dbt-core will build the models and run all test validations.Again, with pure Python, we will get the dbt output in DuckDB and export it to Google Sheets.Google Sheets is connected to a Looker Studio to build quick and nice dashboard to answer business questions. In the upcoming sections I will go through each tool used! &#128736;&#65039; The right tool for the jobWe&#8217;ll go through the tools one by one to understand what is the bare minimum that can do a huge difference in your portable stack: &#128013; Pure Python (Extraction and Loading)Nothing much to say.No need to learn Airflow decorators or random syntax, just call APIs with pure python code!&#129414; DuckDB (In memory Database)In this other post, I&#8217;ve compared Pandas, Polars and DuckDB based on file types and performance.What DuckDB can do is beyond comprehension.You can query millions of rows in a couple of seconds without major complications.You need to come a really long way before thinking that you need another tool to achieve what you want. And, the best part is you can work with SQL when using it!DuckDB reading times based on different files.&#128258; dbt-core (Transformation)At the moment, the main character in the whole data space covering the &#8220;T&#8221; in the ELT.It&#8217;s pretty easy to build data models with custom logic and let the tool connect the dots with sequential jinja templating. Back in the day, doing this with stored procedures was hell compared to this current approach. But, options such as SQL-mesh or SDF are starting to compete with interesting solutions. Your model results after running!&#128354; Github Actions (Orchestration) It&#8217;s free, it has no learning curve. And it works as a simple CRON. You don&#8217;t need to go with Airflow right away. Forget about it!Example of your Github Actions view.&#128196; Google Sheets (Database)Data people are formatted to hate GSheets because stakeholders ask them to export reports to work with the data on a spreadsheet.If you can beat them, join them. Yes, you will need to become familiar with setting up service accounts to interact with the Google Sheets API, but that&#8217;s straightforward with proper documentation.Besides, really cool things are being cooked around DuckDB and spreadsheets! Archie has been working on a DuckDB extensio that will change lots of workflows.&#128202; Looker Studio (Data Viz)Make easy dashboards without weird customization and logics. As you update your Google Sheets, the native connection will do the rest.Is more than you need to get started, PowerBI or Tableau can wait. Built this in 15&#8217;.Thanks for reading The Pipe & The Line! Subscribe to receive content for your data career!&#128064; OutroYou need to experiment to find the most suitable tools for your use-case and think in the long-term.You always need to study the context of your company and be mindful about all scenarios possible.For example, Google Sheets has a limit of 10M cells at the moment. This number can be retrieved if you multiply rows by columns. That said, if you are worried about it, your data volume might need another solution!Practice, research, and make the most out of all the tools at your disposal! However, don&#8217;t take shortcuts, as the quickest solution is not always the best.&#128221; TL;DR&#128230; The Power of Portability &#128104;&#127995;&#8205;&#128187; Fundamentals: take only what you need&#128270; Proof of Concept&#128736;&#65039; The right tool for the jobIf you enjoyed the content, hit the like &#10084;&#65039; button, share, comment, repost, and all those nice things people do when like stuff these days. Glad to know you made it to this part!Hi, I am Alejandro Aboy. I am currently working as a Data Engineer. I started in digital marketing at 19. I gained experience in website tracking, advertising, and analytics. I also founded my agency. In 2021, I found my passion for data engineering. So, I shifted my career focus, despite lacking a CS degree. I'm now pursuing this path, leveraging my diverse experience and willingness to learn.Follow me on Linkedin"}]}
{"messages": [{"role": "user", "content": "Explain how data professionals can balance innovation with simplicity in their technology choices to avoid unnecessary expenses."}, {"role": "assistant", "content": "There are lots of shiny objects we encounter that make us forget about simplicity and the principle of &#8220;the right tool for the job&#8221;.Most of the time, small startups or even companies with small data volumes fall for these new tools.In the end, they are simply overkill.Then, the domino effect starts cascading into:Expensive Fivetran setupsHuge bills from BigQueryNumerous dbt models with SELECT * FROM table;And much more.Sometimes, the best way to start is less sexy.&#128230; The Power of Portability Over the past few months, I&#8217;ve been learning in-depth concepts about Docker and portability.If you are also interested, you can read:There are tons of companies overkilling data stacks with huge setups that end up leveraging 1% of those features.The funny thing is, we are living in amazing times where you can achieve incredible results with tiny but powerful tools.You can utilize a 'data stack in a box' concept and use it on demand and be able to recreate it without all the painful maintenance when its not needed.Also, it can help you save tons of unnecessary costs even if you use this kind of approaches on top of your modern data stack tools.&#128104;&#127995;&#8205;&#128187; Fundamentals: take only what you needMost of the time, data stacks consist of these pieces:ETL workflowsDatabaseData visualizationOrchestrationThis is the most common pattern that you will find if you work in 95% of data teams around the world.There&#8217;s a lot of hype around real-time with Kafka & Flink, big data processing with Spark and open table format with Iceberg or Delta Lake.But those use cases are exclusive to a subset of companies that have a suitable volume to make it worth it. At the end of the day, you need to move data from A to B, add some logic, and automate it so you don&#8217;t hit the button every day.That&#8217;s it. Don&#8217;t kill mosquitos with a bazooka! Simple representation of the usual data engineering pattern.Thanks for reading The Pipe & The Line! Subscribe for free to receive new posts and support my work.&#128270; Proof of ConceptTo test this concept, I&#8217;ve created a repo:&#128073; Github Repo Data Stack To Go With DuckDB & dbtGraphic view of the repository.This repo is calling 3 endpoints from PiwikPRO API to get traffic analytics data. Every day; Github Actions will build the Docker container on its runner.Call the APIs to get the data and save it into DuckDB database, all with pure Python. dbt-core will build the models and run all test validations.Again, with pure Python, we will get the dbt output in DuckDB and export it to Google Sheets.Google Sheets is connected to a Looker Studio to build quick and nice dashboard to answer business questions. In the upcoming sections I will go through each tool used! &#128736;&#65039; The right tool for the jobWe&#8217;ll go through the tools one by one to understand what is the bare minimum that can do a huge difference in your portable stack: &#128013; Pure Python (Extraction and Loading)Nothing much to say.No need to learn Airflow decorators or random syntax, just call APIs with pure python code!&#129414; DuckDB (In memory Database)In this other post, I&#8217;ve compared Pandas, Polars and DuckDB based on file types and performance.What DuckDB can do is beyond comprehension.You can query millions of rows in a couple of seconds without major complications.You need to come a really long way before thinking that you need another tool to achieve what you want. And, the best part is you can work with SQL when using it!DuckDB reading times based on different files.&#128258; dbt-core (Transformation)At the moment, the main character in the whole data space covering the &#8220;T&#8221; in the ELT.It&#8217;s pretty easy to build data models with custom logic and let the tool connect the dots with sequential jinja templating. Back in the day, doing this with stored procedures was hell compared to this current approach. But, options such as SQL-mesh or SDF are starting to compete with interesting solutions. Your model results after running!&#128354; Github Actions (Orchestration) It&#8217;s free, it has no learning curve. And it works as a simple CRON. You don&#8217;t need to go with Airflow right away. Forget about it!Example of your Github Actions view.&#128196; Google Sheets (Database)Data people are formatted to hate GSheets because stakeholders ask them to export reports to work with the data on a spreadsheet.If you can beat them, join them. Yes, you will need to become familiar with setting up service accounts to interact with the Google Sheets API, but that&#8217;s straightforward with proper documentation.Besides, really cool things are being cooked around DuckDB and spreadsheets! Archie has been working on a DuckDB extensio that will change lots of workflows.&#128202; Looker Studio (Data Viz)Make easy dashboards without weird customization and logics. As you update your Google Sheets, the native connection will do the rest.Is more than you need to get started, PowerBI or Tableau can wait. Built this in 15&#8217;.Thanks for reading The Pipe & The Line! Subscribe to receive content for your data career!&#128064; OutroYou need to experiment to find the most suitable tools for your use-case and think in the long-term.You always need to study the context of your company and be mindful about all scenarios possible.For example, Google Sheets has a limit of 10M cells at the moment. This number can be retrieved if you multiply rows by columns. That said, if you are worried about it, your data volume might need another solution!Practice, research, and make the most out of all the tools at your disposal! However, don&#8217;t take shortcuts, as the quickest solution is not always the best.&#128221; TL;DR&#128230; The Power of Portability &#128104;&#127995;&#8205;&#128187; Fundamentals: take only what you need&#128270; Proof of Concept&#128736;&#65039; The right tool for the jobIf you enjoyed the content, hit the like &#10084;&#65039; button, share, comment, repost, and all those nice things people do when like stuff these days. Glad to know you made it to this part!Hi, I am Alejandro Aboy. I am currently working as a Data Engineer. I started in digital marketing at 19. I gained experience in website tracking, advertising, and analytics. I also founded my agency. In 2021, I found my passion for data engineering. So, I shifted my career focus, despite lacking a CS degree. I'm now pursuing this path, leveraging my diverse experience and willingness to learn.Follow me on Linkedin"}]}
{"messages": [{"role": "user", "content": "Share a personal story about a challenging data engineering problem and how you creatively solved it."}, {"role": "assistant", "content": "This post is different from the others.The main reasons?1. I am bored on a 4-hour flight with a phone. &#128517;2. It's just personal experience, no research. 3. I am dissecting one of my top kryptonites.Let&#8217;s go.&#128161;Techies love solving problemsSolving problems is unlocking whatever we need to go from A to B.Problems are not necessarily bugs or broken pipelines; they could be missing features or assets for end users.The main issue: sometimes we jump to fast into the technicalities.Going to fast might be useful in the short term, but it will mostly end up pilling up toxic technical debt.The \"quick fix\" or \"quick POC\" we think will save us from the apocalypse is actually the time bomb that will trigger it.&#128561; Beating Anxiety, one step at a timeOver the last few months, I've been digging into different ways of approaching conflicts from a technical perspective. It's easy to get anxious because stakeholders are putting pressure on you or because you overlook complexity.But doing the right exploration before pulling the trigger will give you great effectiveness when you tackle the problem.These are crucial concepts to this topic: \"Exploring\" is different from \"Solving\"Facts vs Hyphothesis: Don't get confused! Don't fight fire with gasolineKnowing when you DON'T need to interveneThanks for reading The Pipe & The Line! Subscribe to get new content and learnings for your data career!&#128373;&#127995; \"Exploring\" is different from \"Solving\"Be observant, take notes, pay attention. You don't have to be the fastest \"PR merger\" alive&#8212;take it easy.Start with the 5Ws (Who, When, Where, What, Why).It doesn't have to be strict and might sound redundant, but it gives clarity!&#128073; For example, an Airflow task fails due to a SQL query timeout. Who: Airflow Task Where: SQL Query What: failure due to query timeout When: task schedule Why: ...? When you start exploring, you might not know the reason, but you know all the other pieces of the puzzle!The longer you spend exploring, the less you will spend on trial and error.&#129335;&#127995;&#8205;&#9794;&#65039; Facts vs Hyphothesis: Don't get confused!Be careful with how you present your ideas when tackling issues.Normally, when you're in too deep with an issue, you start losing perspective.We might have the tendency of assuming things as facts without solid evidence, but they are just ideas.Think of it as a homicide investigation: you have suspects, motives, alibis, and murder weapons.But you don't know who did it or why. You formulate hypotheses.This approach allows you to generate starting points to check and discard as you go.The &#8220;Debug-tective&#8221; by DALL-E.The list can be endless, but you can validate those ideas by testing them in the field.&#128073; For example, I think our failing task is timimg out due to: A long & complex join between big tablea.Airflow Network issues with PostgresSQL.RDS instance running out of memory. If you identify that there is a long and complex subquery within the main SQL query, but you don't see logs about network or memory issues, you have your main suspect!When writing post-mortem documents or summaries for other technical coworkers, be clear about facts and ideas you have tackled through experiments.You will gain better feedback to beat the final boss!I cover some of these points within this post:&#128293; Don't fight fire with gasolineThere are tons of solutions for the same problem. But not all of them are suitable.Back to our example:If there is a SQL subquery that is taking forever, you can refactor it into smaller pieces, filter it, or even remove it if it's suitable for the context.Depending on what solution you choose, you might be opening the door for another problem, or just putting them to sleep for a while.&#128073; Some examples:\"Will the introduced SQL code be intuitive to read for new team members?\"\"Will this filter remove business-critical data and affect decision-making processes?\"You should always have this question in mind: \"How will this solution impact us in 3-6-12 months?\"If you can answer, you're on the right path.If you can't, seek an unbiased opinion from a coworker that will provide you with more clarity.If you can't do the latter, just let it rest for a day or two if possible&#8212;it will give you some perspective.Thanks for reading The Pipe & The Line! This post is public so feel free to share it.Share&#128721; Knowing when you DON'T need to intervene\"The best code is the one you don't write\" is the most poetic quote I've heard.It sounds like the perfect combo of laziness and wisdom.&#128073; Let's look at our recurring example from a different standpoint:You might find that the SQL subquery that takes forever is getting stuck due to another simultaneous query to the same table.Remove overlapping processes by changing schedules!We are biased towards the fact that we can only solve things by merging PRs, and sometimes a couple of clicks can make a huge difference.&#129309; Last wordsI always recommend coming back to all the implemented solutions from time to time and taking a fresh look.You might find more effective ways of approaching and documenting them.You might also find that the issue was tackled from the wrong angle.If that's the case, read this post again!&#128221; TL;DR&#128161; Techies love solving problems, but jump too fast into technical approaches&#128561; Beating Anxiety, one step at a time&#128373;&#127995; \"Exploring\" is different from \"Solving\". Pay attention to context before jumping.&#129335;&#127995;&#8205;&#9794;&#65039; Facts vs Hyphothesis: Don't get confused! What&#8217;s happening is not what you think is happening, validate and discard with experiments! &#128293; Don't fight fire with gasoline. Every decision can improve your future or put the problem to sleep, be wise!&#128721; Knowing when you DON'T need to intervene. You don&#8217;t always need to code, less is more. &#129309; Last words. Come back and check the problems with a fresher view, maybe there is a better angle! If you enjoyed the content, hit the like &#10084;&#65039; button, share, comment, repost, and all those nice things people do when like stuff these days. Glad to know you made it to this part!Hi, I am Alejandro Aboy. I am currently working as a Data Engineer. I started in digital marketing at 19. I gained experience in website tracking, advertising, and analytics. I also founded my agency. In 2021, I found my passion for data engineering. So, I shifted my career focus, despite lacking a CS degree. I'm now pursuing this path, leveraging my diverse experience and willingness to learn.Follow me on Linkedin"}]}
{"messages": [{"role": "user", "content": "Discuss the importance of understanding user needs before jumping into technical solutions in tech projects."}, {"role": "assistant", "content": "This post is different from the others.The main reasons?1. I am bored on a 4-hour flight with a phone. &#128517;2. It's just personal experience, no research. 3. I am dissecting one of my top kryptonites.Let&#8217;s go.&#128161;Techies love solving problemsSolving problems is unlocking whatever we need to go from A to B.Problems are not necessarily bugs or broken pipelines; they could be missing features or assets for end users.The main issue: sometimes we jump to fast into the technicalities.Going to fast might be useful in the short term, but it will mostly end up pilling up toxic technical debt.The \"quick fix\" or \"quick POC\" we think will save us from the apocalypse is actually the time bomb that will trigger it.&#128561; Beating Anxiety, one step at a timeOver the last few months, I've been digging into different ways of approaching conflicts from a technical perspective. It's easy to get anxious because stakeholders are putting pressure on you or because you overlook complexity.But doing the right exploration before pulling the trigger will give you great effectiveness when you tackle the problem.These are crucial concepts to this topic: \"Exploring\" is different from \"Solving\"Facts vs Hyphothesis: Don't get confused! Don't fight fire with gasolineKnowing when you DON'T need to interveneThanks for reading The Pipe & The Line! Subscribe to get new content and learnings for your data career!&#128373;&#127995; \"Exploring\" is different from \"Solving\"Be observant, take notes, pay attention. You don't have to be the fastest \"PR merger\" alive&#8212;take it easy.Start with the 5Ws (Who, When, Where, What, Why).It doesn't have to be strict and might sound redundant, but it gives clarity!&#128073; For example, an Airflow task fails due to a SQL query timeout. Who: Airflow Task Where: SQL Query What: failure due to query timeout When: task schedule Why: ...? When you start exploring, you might not know the reason, but you know all the other pieces of the puzzle!The longer you spend exploring, the less you will spend on trial and error.&#129335;&#127995;&#8205;&#9794;&#65039; Facts vs Hyphothesis: Don't get confused!Be careful with how you present your ideas when tackling issues.Normally, when you're in too deep with an issue, you start losing perspective.We might have the tendency of assuming things as facts without solid evidence, but they are just ideas.Think of it as a homicide investigation: you have suspects, motives, alibis, and murder weapons.But you don't know who did it or why. You formulate hypotheses.This approach allows you to generate starting points to check and discard as you go.The &#8220;Debug-tective&#8221; by DALL-E.The list can be endless, but you can validate those ideas by testing them in the field.&#128073; For example, I think our failing task is timimg out due to: A long & complex join between big tablea.Airflow Network issues with PostgresSQL.RDS instance running out of memory. If you identify that there is a long and complex subquery within the main SQL query, but you don't see logs about network or memory issues, you have your main suspect!When writing post-mortem documents or summaries for other technical coworkers, be clear about facts and ideas you have tackled through experiments.You will gain better feedback to beat the final boss!I cover some of these points within this post:&#128293; Don't fight fire with gasolineThere are tons of solutions for the same problem. But not all of them are suitable.Back to our example:If there is a SQL subquery that is taking forever, you can refactor it into smaller pieces, filter it, or even remove it if it's suitable for the context.Depending on what solution you choose, you might be opening the door for another problem, or just putting them to sleep for a while.&#128073; Some examples:\"Will the introduced SQL code be intuitive to read for new team members?\"\"Will this filter remove business-critical data and affect decision-making processes?\"You should always have this question in mind: \"How will this solution impact us in 3-6-12 months?\"If you can answer, you're on the right path.If you can't, seek an unbiased opinion from a coworker that will provide you with more clarity.If you can't do the latter, just let it rest for a day or two if possible&#8212;it will give you some perspective.Thanks for reading The Pipe & The Line! This post is public so feel free to share it.Share&#128721; Knowing when you DON'T need to intervene\"The best code is the one you don't write\" is the most poetic quote I've heard.It sounds like the perfect combo of laziness and wisdom.&#128073; Let's look at our recurring example from a different standpoint:You might find that the SQL subquery that takes forever is getting stuck due to another simultaneous query to the same table.Remove overlapping processes by changing schedules!We are biased towards the fact that we can only solve things by merging PRs, and sometimes a couple of clicks can make a huge difference.&#129309; Last wordsI always recommend coming back to all the implemented solutions from time to time and taking a fresh look.You might find more effective ways of approaching and documenting them.You might also find that the issue was tackled from the wrong angle.If that's the case, read this post again!&#128221; TL;DR&#128161; Techies love solving problems, but jump too fast into technical approaches&#128561; Beating Anxiety, one step at a time&#128373;&#127995; \"Exploring\" is different from \"Solving\". Pay attention to context before jumping.&#129335;&#127995;&#8205;&#9794;&#65039; Facts vs Hyphothesis: Don't get confused! What&#8217;s happening is not what you think is happening, validate and discard with experiments! &#128293; Don't fight fire with gasoline. Every decision can improve your future or put the problem to sleep, be wise!&#128721; Knowing when you DON'T need to intervene. You don&#8217;t always need to code, less is more. &#129309; Last words. Come back and check the problems with a fresher view, maybe there is a better angle! If you enjoyed the content, hit the like &#10084;&#65039; button, share, comment, repost, and all those nice things people do when like stuff these days. Glad to know you made it to this part!Hi, I am Alejandro Aboy. I am currently working as a Data Engineer. I started in digital marketing at 19. I gained experience in website tracking, advertising, and analytics. I also founded my agency. In 2021, I found my passion for data engineering. So, I shifted my career focus, despite lacking a CS degree. I'm now pursuing this path, leveraging my diverse experience and willingness to learn.Follow me on Linkedin"}]}
{"messages": [{"role": "user", "content": "Create a practical guide for data engineers on effective problem-solving techniques without diving into code immediately."}, {"role": "assistant", "content": "This post is different from the others.The main reasons?1. I am bored on a 4-hour flight with a phone. &#128517;2. It's just personal experience, no research. 3. I am dissecting one of my top kryptonites.Let&#8217;s go.&#128161;Techies love solving problemsSolving problems is unlocking whatever we need to go from A to B.Problems are not necessarily bugs or broken pipelines; they could be missing features or assets for end users.The main issue: sometimes we jump to fast into the technicalities.Going to fast might be useful in the short term, but it will mostly end up pilling up toxic technical debt.The \"quick fix\" or \"quick POC\" we think will save us from the apocalypse is actually the time bomb that will trigger it.&#128561; Beating Anxiety, one step at a timeOver the last few months, I've been digging into different ways of approaching conflicts from a technical perspective. It's easy to get anxious because stakeholders are putting pressure on you or because you overlook complexity.But doing the right exploration before pulling the trigger will give you great effectiveness when you tackle the problem.These are crucial concepts to this topic: \"Exploring\" is different from \"Solving\"Facts vs Hyphothesis: Don't get confused! Don't fight fire with gasolineKnowing when you DON'T need to interveneThanks for reading The Pipe & The Line! Subscribe to get new content and learnings for your data career!&#128373;&#127995; \"Exploring\" is different from \"Solving\"Be observant, take notes, pay attention. You don't have to be the fastest \"PR merger\" alive&#8212;take it easy.Start with the 5Ws (Who, When, Where, What, Why).It doesn't have to be strict and might sound redundant, but it gives clarity!&#128073; For example, an Airflow task fails due to a SQL query timeout. Who: Airflow Task Where: SQL Query What: failure due to query timeout When: task schedule Why: ...? When you start exploring, you might not know the reason, but you know all the other pieces of the puzzle!The longer you spend exploring, the less you will spend on trial and error.&#129335;&#127995;&#8205;&#9794;&#65039; Facts vs Hyphothesis: Don't get confused!Be careful with how you present your ideas when tackling issues.Normally, when you're in too deep with an issue, you start losing perspective.We might have the tendency of assuming things as facts without solid evidence, but they are just ideas.Think of it as a homicide investigation: you have suspects, motives, alibis, and murder weapons.But you don't know who did it or why. You formulate hypotheses.This approach allows you to generate starting points to check and discard as you go.The &#8220;Debug-tective&#8221; by DALL-E.The list can be endless, but you can validate those ideas by testing them in the field.&#128073; For example, I think our failing task is timimg out due to: A long & complex join between big tablea.Airflow Network issues with PostgresSQL.RDS instance running out of memory. If you identify that there is a long and complex subquery within the main SQL query, but you don't see logs about network or memory issues, you have your main suspect!When writing post-mortem documents or summaries for other technical coworkers, be clear about facts and ideas you have tackled through experiments.You will gain better feedback to beat the final boss!I cover some of these points within this post:&#128293; Don't fight fire with gasolineThere are tons of solutions for the same problem. But not all of them are suitable.Back to our example:If there is a SQL subquery that is taking forever, you can refactor it into smaller pieces, filter it, or even remove it if it's suitable for the context.Depending on what solution you choose, you might be opening the door for another problem, or just putting them to sleep for a while.&#128073; Some examples:\"Will the introduced SQL code be intuitive to read for new team members?\"\"Will this filter remove business-critical data and affect decision-making processes?\"You should always have this question in mind: \"How will this solution impact us in 3-6-12 months?\"If you can answer, you're on the right path.If you can't, seek an unbiased opinion from a coworker that will provide you with more clarity.If you can't do the latter, just let it rest for a day or two if possible&#8212;it will give you some perspective.Thanks for reading The Pipe & The Line! This post is public so feel free to share it.Share&#128721; Knowing when you DON'T need to intervene\"The best code is the one you don't write\" is the most poetic quote I've heard.It sounds like the perfect combo of laziness and wisdom.&#128073; Let's look at our recurring example from a different standpoint:You might find that the SQL subquery that takes forever is getting stuck due to another simultaneous query to the same table.Remove overlapping processes by changing schedules!We are biased towards the fact that we can only solve things by merging PRs, and sometimes a couple of clicks can make a huge difference.&#129309; Last wordsI always recommend coming back to all the implemented solutions from time to time and taking a fresh look.You might find more effective ways of approaching and documenting them.You might also find that the issue was tackled from the wrong angle.If that's the case, read this post again!&#128221; TL;DR&#128161; Techies love solving problems, but jump too fast into technical approaches&#128561; Beating Anxiety, one step at a time&#128373;&#127995; \"Exploring\" is different from \"Solving\". Pay attention to context before jumping.&#129335;&#127995;&#8205;&#9794;&#65039; Facts vs Hyphothesis: Don't get confused! What&#8217;s happening is not what you think is happening, validate and discard with experiments! &#128293; Don't fight fire with gasoline. Every decision can improve your future or put the problem to sleep, be wise!&#128721; Knowing when you DON'T need to intervene. You don&#8217;t always need to code, less is more. &#129309; Last words. Come back and check the problems with a fresher view, maybe there is a better angle! If you enjoyed the content, hit the like &#10084;&#65039; button, share, comment, repost, and all those nice things people do when like stuff these days. Glad to know you made it to this part!Hi, I am Alejandro Aboy. I am currently working as a Data Engineer. I started in digital marketing at 19. I gained experience in website tracking, advertising, and analytics. I also founded my agency. In 2021, I found my passion for data engineering. So, I shifted my career focus, despite lacking a CS degree. I'm now pursuing this path, leveraging my diverse experience and willingness to learn.Follow me on Linkedin"}]}
{"messages": [{"role": "user", "content": "Explain the fundamentals of a specific technology stack, including key components and their interactions in data engineering."}, {"role": "assistant", "content": "Not so long ago, I had no idea on how to use tech like Docker.To be honest, it seemed very complex.There are too many keywords and concepts that can confuse you pretty easily. But now, I cannot do anything without it.In this article, I will do a desmytified explanation about:ContainersImages and VolumesDockerfilesDocker Compose&#9888;&#65039; Disclaimer: all the examples apply to Data tools, but the explanation overall is generic.&#128736;&#65039; What is Docker?Imagine you're an amateur chef that crafted an amazing recipe you want to show to your friends.A fish recipe won&#8217;t make sense if your friend only has special knives for chicken.You will have to bring as much as you can or invite them home, where you have those utils available. The meal won&#8217;t be the same wherever you go. Docker is like your personal kitchen-in-a-box. No matter where you cook, everything you need for your kitchen will always be available, the way you need it:Kitchen-in-a-box analogy graph.Docker is a platform that uses containers, images and volumes with everything your application needs to run.This includes: code, packages, libraries and everything you use to run your container:Technical explanation graph.&#127859; Docker Is Your Personal KitchenHere&#8217;s how Docker keeps your data kitchen in order:Same Recipe, Same Results: With Docker, your container is like taking your oven, stove, and pans with you. Run the code on your laptop, your friend&#8217;s machine, or a cloud server, the results are always the same.Clean Kitchen, Every Time: Without Docker, your kitchen often has leftovers from previous meals, which makes it really messy. Run the code without conflicts, doesn&#8217;t matter if you computer has different or even older dependencies.Try New Recipes Without Worry: Whenever you want to try something new, the kitchen appliances you have might not be suitable for the job. You might fear breaking them and having to buy new ones. You can recreate containers as many times as you want, but nothing will be lost or broken since it&#8217;s isolated.&#128073; Example: You want to test an ETL pipeline using the latest PostgreSQL. Instead of upgrading your local installation, you can spin up a Docker container with the newest version and experiment inside it.The Kitchen Analogy.Thanks for reading The Pipe & The Line! Subscribe for free to receive new posts and support my work.&#128450;&#65039; The Blueprint and the Fridge: Images and VolumesEvery time you use Docker, you need:The blueprint of the kitchen you are going to setup &#8594; ImageThe fridge to save your preparations while cooking &#8594; Volumes&#128444;&#65039; What&#8217;s an Image?It has all the details that you need for the kitchen to exist: utensils, ingredients, ovens, pans, etc.But, it&#8217;s not actually a working kitchen yet. &#128230; What&#8217;s a Volume?When you shut down your kitchen (container), you can take it home in a bag (local) or store stuff in the fridge (volume). A volume allows you to save important data or files so that they persist, even if you tear down the container.&#127829; Breaking Down a DockerfileThink of a Dockerfile as a recipe for your perfect kitchen setup. It will have the base configuration for your Image.Each part of the Dockerfile sets up a different piece of your kitchen. Let&#8217;s break down the key components:Dockerfile example.FROM: This defines your base environment. Pick whether you want a pizza or sushi kitchen. COPY: Copies files from your local machine into the container. Bring ingredients into your kitchen. WORKDIR: Sets your working directory, so you know where everything happens. Choose your dedicated spot in the kitchen.RUN: Installs dependencies and prepares your environment. Chop and prepare your ingredients before cooking. ENV: Sets environment variables to control behavior. Set kitchen rules, such as &#8220;wash hands before cooking.&#8221;CMD: Defines the command that starts your application when the container runs. Turn on the oven and start the cooking.Thanks for reading The Pipe & The Line! This post is public so feel free to share it.Share&#128268; Docker vs. Docker ComposeIf Docker is your personal kitchen, Docker Compose is like a group of food trucks.It allows you to spin up multiple containers at once. Need a PostgreSQL database, an Airflow server, and a Jupyter notebook? Docker Compose lets you handle all these tools easily, you can also use multiple Dockerfiles if you want:Docker compose example.&#129335;&#8205;&#9794;&#65039; Why Not Just Install Locally?Sure, you can install everything on your local machine.It works... until it doesn&#8217;t.It&#8217;s pretty likely that you will always miss something, which would be the same as fetching the ingredients one by one or realising that your knives are not sharpened when you need them.Local Installation Headaches:Different tools need different versions of libraries.Need to tweak settings to match a standard setup.Removing all traces of unused tools is a pain.With Docker, you create an isolated environment that&#8217;s consistent, clean, and portable.&#128073; Example: Let's say you have an ETL pipeline using Python, Airflow, and PostgreSQL. On your local machine, versions might differ and libraries might be missing. In Docker, all of these dependencies live together inside the container.Thanks for reading The Pipe & The Line! Subscribe to get new content and learnings for your data career!&#9888;&#65039; Disclaimer: This will only apply for Python users. There is a good way to validate if someone uses Docker or not. Run this command on your terminal:pip freezeWhen I started my career in Data, running that command would have retrieved an output like this:beautifulsoup4==4.10.0 boto3==1.18.65 botocore==1.21.65 elasticsearch==7.13.4 fastapi==0.68.2 gunicorn==20.1.0 jsonschema==3.2.0 matplotlib==3.4.3 mlflow==1.19.0 numpy==1.21.2 pandas==1.3.3 ... (the list continues)It meant that my computer was flooded with packages that you could need updates to match the requirements of newer projects.And that&#8217;s the type of chaos Docker avoids!&#128640; Why You Need DockerConsistency: you stop fixing different issues across environments.Portability: you ship your projects faster without handling unnecessary conflicts.Isolation: don&#8217;t be afraid if something fails in one container, it won&#8217;t affect others. Next time you see this: Image - Container - VolumeThink of this: Blueprint - Kitchen - Fridge&#128221; TL;DR&#128736;&#65039; What is Docker? - Your personal kitchen-in-a-box, ensuring consistency wherever you run your data projects.&#127859; Docker Is Your Personal Kitchen - Control, cleanliness, and freedom to experiment without breaking anything.&#128450;&#65039; The Blueprint and the Fridge: Images and Volumes - The instructions to build the kitchen you need, and the storage to save preparations you cook. &#127829; Breaking Down a Dockerfile - The recipe for your container, setting up everything from the base environment to starting the app.&#128268; Docker vs. Docker Compose - Orchestrate multiple services with one command.&#129335;&#8205;&#9794;&#65039; Why Not Just Install Locally? - Avoid dependency hell and inconsistent environments!&#128640; Why You Need Docker - Ensures consistency, portability, and isolation for your data engineering projects.If you enjoyed the content, hit the like &#10084;&#65039; button, share, comment, repost, and all those nice things people do when like stuff these days. Glad to know you made it to this part!Hi, I am Alejandro Aboy. I am currently working as a Data Engineer. I started in digital marketing at 19. I gained experience in website tracking, advertising, and analytics. I also founded my agency. In 2021, I found my passion for data engineering. So, I shifted my career focus, despite lacking a CS degree. I'm now pursuing this path, leveraging my diverse experience and willingness to learn.Follow me on Linkedin"}]}
{"messages": [{"role": "user", "content": "Share insights on how mastering Docker can enhance your career opportunities in data engineering and DevOps roles."}, {"role": "assistant", "content": "Not so long ago, I had no idea on how to use tech like Docker.To be honest, it seemed very complex.There are too many keywords and concepts that can confuse you pretty easily. But now, I cannot do anything without it.In this article, I will do a desmytified explanation about:ContainersImages and VolumesDockerfilesDocker Compose&#9888;&#65039; Disclaimer: all the examples apply to Data tools, but the explanation overall is generic.&#128736;&#65039; What is Docker?Imagine you're an amateur chef that crafted an amazing recipe you want to show to your friends.A fish recipe won&#8217;t make sense if your friend only has special knives for chicken.You will have to bring as much as you can or invite them home, where you have those utils available. The meal won&#8217;t be the same wherever you go. Docker is like your personal kitchen-in-a-box. No matter where you cook, everything you need for your kitchen will always be available, the way you need it:Kitchen-in-a-box analogy graph.Docker is a platform that uses containers, images and volumes with everything your application needs to run.This includes: code, packages, libraries and everything you use to run your container:Technical explanation graph.&#127859; Docker Is Your Personal KitchenHere&#8217;s how Docker keeps your data kitchen in order:Same Recipe, Same Results: With Docker, your container is like taking your oven, stove, and pans with you. Run the code on your laptop, your friend&#8217;s machine, or a cloud server, the results are always the same.Clean Kitchen, Every Time: Without Docker, your kitchen often has leftovers from previous meals, which makes it really messy. Run the code without conflicts, doesn&#8217;t matter if you computer has different or even older dependencies.Try New Recipes Without Worry: Whenever you want to try something new, the kitchen appliances you have might not be suitable for the job. You might fear breaking them and having to buy new ones. You can recreate containers as many times as you want, but nothing will be lost or broken since it&#8217;s isolated.&#128073; Example: You want to test an ETL pipeline using the latest PostgreSQL. Instead of upgrading your local installation, you can spin up a Docker container with the newest version and experiment inside it.The Kitchen Analogy.Thanks for reading The Pipe & The Line! Subscribe for free to receive new posts and support my work.&#128450;&#65039; The Blueprint and the Fridge: Images and VolumesEvery time you use Docker, you need:The blueprint of the kitchen you are going to setup &#8594; ImageThe fridge to save your preparations while cooking &#8594; Volumes&#128444;&#65039; What&#8217;s an Image?It has all the details that you need for the kitchen to exist: utensils, ingredients, ovens, pans, etc.But, it&#8217;s not actually a working kitchen yet. &#128230; What&#8217;s a Volume?When you shut down your kitchen (container), you can take it home in a bag (local) or store stuff in the fridge (volume). A volume allows you to save important data or files so that they persist, even if you tear down the container.&#127829; Breaking Down a DockerfileThink of a Dockerfile as a recipe for your perfect kitchen setup. It will have the base configuration for your Image.Each part of the Dockerfile sets up a different piece of your kitchen. Let&#8217;s break down the key components:Dockerfile example.FROM: This defines your base environment. Pick whether you want a pizza or sushi kitchen. COPY: Copies files from your local machine into the container. Bring ingredients into your kitchen. WORKDIR: Sets your working directory, so you know where everything happens. Choose your dedicated spot in the kitchen.RUN: Installs dependencies and prepares your environment. Chop and prepare your ingredients before cooking. ENV: Sets environment variables to control behavior. Set kitchen rules, such as &#8220;wash hands before cooking.&#8221;CMD: Defines the command that starts your application when the container runs. Turn on the oven and start the cooking.Thanks for reading The Pipe & The Line! This post is public so feel free to share it.Share&#128268; Docker vs. Docker ComposeIf Docker is your personal kitchen, Docker Compose is like a group of food trucks.It allows you to spin up multiple containers at once. Need a PostgreSQL database, an Airflow server, and a Jupyter notebook? Docker Compose lets you handle all these tools easily, you can also use multiple Dockerfiles if you want:Docker compose example.&#129335;&#8205;&#9794;&#65039; Why Not Just Install Locally?Sure, you can install everything on your local machine.It works... until it doesn&#8217;t.It&#8217;s pretty likely that you will always miss something, which would be the same as fetching the ingredients one by one or realising that your knives are not sharpened when you need them.Local Installation Headaches:Different tools need different versions of libraries.Need to tweak settings to match a standard setup.Removing all traces of unused tools is a pain.With Docker, you create an isolated environment that&#8217;s consistent, clean, and portable.&#128073; Example: Let's say you have an ETL pipeline using Python, Airflow, and PostgreSQL. On your local machine, versions might differ and libraries might be missing. In Docker, all of these dependencies live together inside the container.Thanks for reading The Pipe & The Line! Subscribe to get new content and learnings for your data career!&#9888;&#65039; Disclaimer: This will only apply for Python users. There is a good way to validate if someone uses Docker or not. Run this command on your terminal:pip freezeWhen I started my career in Data, running that command would have retrieved an output like this:beautifulsoup4==4.10.0 boto3==1.18.65 botocore==1.21.65 elasticsearch==7.13.4 fastapi==0.68.2 gunicorn==20.1.0 jsonschema==3.2.0 matplotlib==3.4.3 mlflow==1.19.0 numpy==1.21.2 pandas==1.3.3 ... (the list continues)It meant that my computer was flooded with packages that you could need updates to match the requirements of newer projects.And that&#8217;s the type of chaos Docker avoids!&#128640; Why You Need DockerConsistency: you stop fixing different issues across environments.Portability: you ship your projects faster without handling unnecessary conflicts.Isolation: don&#8217;t be afraid if something fails in one container, it won&#8217;t affect others. Next time you see this: Image - Container - VolumeThink of this: Blueprint - Kitchen - Fridge&#128221; TL;DR&#128736;&#65039; What is Docker? - Your personal kitchen-in-a-box, ensuring consistency wherever you run your data projects.&#127859; Docker Is Your Personal Kitchen - Control, cleanliness, and freedom to experiment without breaking anything.&#128450;&#65039; The Blueprint and the Fridge: Images and Volumes - The instructions to build the kitchen you need, and the storage to save preparations you cook. &#127829; Breaking Down a Dockerfile - The recipe for your container, setting up everything from the base environment to starting the app.&#128268; Docker vs. Docker Compose - Orchestrate multiple services with one command.&#129335;&#8205;&#9794;&#65039; Why Not Just Install Locally? - Avoid dependency hell and inconsistent environments!&#128640; Why You Need Docker - Ensures consistency, portability, and isolation for your data engineering projects.If you enjoyed the content, hit the like &#10084;&#65039; button, share, comment, repost, and all those nice things people do when like stuff these days. Glad to know you made it to this part!Hi, I am Alejandro Aboy. I am currently working as a Data Engineer. I started in digital marketing at 19. I gained experience in website tracking, advertising, and analytics. I also founded my agency. In 2021, I found my passion for data engineering. So, I shifted my career focus, despite lacking a CS degree. I'm now pursuing this path, leveraging my diverse experience and willingness to learn.Follow me on Linkedin"}]}
{"messages": [{"role": "user", "content": "Create a step-by-step practical guide for setting up a Docker environment for data analysis projects."}, {"role": "assistant", "content": "Not so long ago, I had no idea on how to use tech like Docker.To be honest, it seemed very complex.There are too many keywords and concepts that can confuse you pretty easily. But now, I cannot do anything without it.In this article, I will do a desmytified explanation about:ContainersImages and VolumesDockerfilesDocker Compose&#9888;&#65039; Disclaimer: all the examples apply to Data tools, but the explanation overall is generic.&#128736;&#65039; What is Docker?Imagine you're an amateur chef that crafted an amazing recipe you want to show to your friends.A fish recipe won&#8217;t make sense if your friend only has special knives for chicken.You will have to bring as much as you can or invite them home, where you have those utils available. The meal won&#8217;t be the same wherever you go. Docker is like your personal kitchen-in-a-box. No matter where you cook, everything you need for your kitchen will always be available, the way you need it:Kitchen-in-a-box analogy graph.Docker is a platform that uses containers, images and volumes with everything your application needs to run.This includes: code, packages, libraries and everything you use to run your container:Technical explanation graph.&#127859; Docker Is Your Personal KitchenHere&#8217;s how Docker keeps your data kitchen in order:Same Recipe, Same Results: With Docker, your container is like taking your oven, stove, and pans with you. Run the code on your laptop, your friend&#8217;s machine, or a cloud server, the results are always the same.Clean Kitchen, Every Time: Without Docker, your kitchen often has leftovers from previous meals, which makes it really messy. Run the code without conflicts, doesn&#8217;t matter if you computer has different or even older dependencies.Try New Recipes Without Worry: Whenever you want to try something new, the kitchen appliances you have might not be suitable for the job. You might fear breaking them and having to buy new ones. You can recreate containers as many times as you want, but nothing will be lost or broken since it&#8217;s isolated.&#128073; Example: You want to test an ETL pipeline using the latest PostgreSQL. Instead of upgrading your local installation, you can spin up a Docker container with the newest version and experiment inside it.The Kitchen Analogy.Thanks for reading The Pipe & The Line! Subscribe for free to receive new posts and support my work.&#128450;&#65039; The Blueprint and the Fridge: Images and VolumesEvery time you use Docker, you need:The blueprint of the kitchen you are going to setup &#8594; ImageThe fridge to save your preparations while cooking &#8594; Volumes&#128444;&#65039; What&#8217;s an Image?It has all the details that you need for the kitchen to exist: utensils, ingredients, ovens, pans, etc.But, it&#8217;s not actually a working kitchen yet. &#128230; What&#8217;s a Volume?When you shut down your kitchen (container), you can take it home in a bag (local) or store stuff in the fridge (volume). A volume allows you to save important data or files so that they persist, even if you tear down the container.&#127829; Breaking Down a DockerfileThink of a Dockerfile as a recipe for your perfect kitchen setup. It will have the base configuration for your Image.Each part of the Dockerfile sets up a different piece of your kitchen. Let&#8217;s break down the key components:Dockerfile example.FROM: This defines your base environment. Pick whether you want a pizza or sushi kitchen. COPY: Copies files from your local machine into the container. Bring ingredients into your kitchen. WORKDIR: Sets your working directory, so you know where everything happens. Choose your dedicated spot in the kitchen.RUN: Installs dependencies and prepares your environment. Chop and prepare your ingredients before cooking. ENV: Sets environment variables to control behavior. Set kitchen rules, such as &#8220;wash hands before cooking.&#8221;CMD: Defines the command that starts your application when the container runs. Turn on the oven and start the cooking.Thanks for reading The Pipe & The Line! This post is public so feel free to share it.Share&#128268; Docker vs. Docker ComposeIf Docker is your personal kitchen, Docker Compose is like a group of food trucks.It allows you to spin up multiple containers at once. Need a PostgreSQL database, an Airflow server, and a Jupyter notebook? Docker Compose lets you handle all these tools easily, you can also use multiple Dockerfiles if you want:Docker compose example.&#129335;&#8205;&#9794;&#65039; Why Not Just Install Locally?Sure, you can install everything on your local machine.It works... until it doesn&#8217;t.It&#8217;s pretty likely that you will always miss something, which would be the same as fetching the ingredients one by one or realising that your knives are not sharpened when you need them.Local Installation Headaches:Different tools need different versions of libraries.Need to tweak settings to match a standard setup.Removing all traces of unused tools is a pain.With Docker, you create an isolated environment that&#8217;s consistent, clean, and portable.&#128073; Example: Let's say you have an ETL pipeline using Python, Airflow, and PostgreSQL. On your local machine, versions might differ and libraries might be missing. In Docker, all of these dependencies live together inside the container.Thanks for reading The Pipe & The Line! Subscribe to get new content and learnings for your data career!&#9888;&#65039; Disclaimer: This will only apply for Python users. There is a good way to validate if someone uses Docker or not. Run this command on your terminal:pip freezeWhen I started my career in Data, running that command would have retrieved an output like this:beautifulsoup4==4.10.0 boto3==1.18.65 botocore==1.21.65 elasticsearch==7.13.4 fastapi==0.68.2 gunicorn==20.1.0 jsonschema==3.2.0 matplotlib==3.4.3 mlflow==1.19.0 numpy==1.21.2 pandas==1.3.3 ... (the list continues)It meant that my computer was flooded with packages that you could need updates to match the requirements of newer projects.And that&#8217;s the type of chaos Docker avoids!&#128640; Why You Need DockerConsistency: you stop fixing different issues across environments.Portability: you ship your projects faster without handling unnecessary conflicts.Isolation: don&#8217;t be afraid if something fails in one container, it won&#8217;t affect others. Next time you see this: Image - Container - VolumeThink of this: Blueprint - Kitchen - Fridge&#128221; TL;DR&#128736;&#65039; What is Docker? - Your personal kitchen-in-a-box, ensuring consistency wherever you run your data projects.&#127859; Docker Is Your Personal Kitchen - Control, cleanliness, and freedom to experiment without breaking anything.&#128450;&#65039; The Blueprint and the Fridge: Images and Volumes - The instructions to build the kitchen you need, and the storage to save preparations you cook. &#127829; Breaking Down a Dockerfile - The recipe for your container, setting up everything from the base environment to starting the app.&#128268; Docker vs. Docker Compose - Orchestrate multiple services with one command.&#129335;&#8205;&#9794;&#65039; Why Not Just Install Locally? - Avoid dependency hell and inconsistent environments!&#128640; Why You Need Docker - Ensures consistency, portability, and isolation for your data engineering projects.If you enjoyed the content, hit the like &#10084;&#65039; button, share, comment, repost, and all those nice things people do when like stuff these days. Glad to know you made it to this part!Hi, I am Alejandro Aboy. I am currently working as a Data Engineer. I started in digital marketing at 19. I gained experience in website tracking, advertising, and analytics. I also founded my agency. In 2021, I found my passion for data engineering. So, I shifted my career focus, despite lacking a CS degree. I'm now pursuing this path, leveraging my diverse experience and willingness to learn.Follow me on Linkedin"}]}
{"messages": [{"role": "user", "content": "Compare the performance of popular data processing libraries in Python, focusing on speed and memory usage."}, {"role": "assistant", "content": "Let&#8217;s do some quick history:Pandas: released in January 2008Polars: released in October 2020DuckDB: released in July 2019It&#8217;s been 16 years of Pandas dominance until new options have appeared on the scene. Nowadays, we can seamlessly run SQL through python and process more data without jumping into Spark or Cloud Warehouses. I wanted to understand the difference between the 3 of them and how they perform across different file types.I&#8217;ve created this Github Repo: &#128270; File Processing Benchmark&#9888;&#65039; Disclaimer: All tests ran on Mac M1 Pro 16GB 2021. The presented conclusions might vary from other experiments.Let&#8217;s get started! &#128060; PandasPandas is often the first tool data analysts learn for Python. It's easy to use, versatile and helps you work with structured data easily. It's great for:Loading data from various sources (CSV, Excel, Databases).Cleaning and organizing messy data.Performing calculations and statistical analyses.Creating simple visualizations.This is how the syntax looks:import pandas as pd df = pd.read_csv('sales_data.csv') print(df.head()) print(df.describe()) product_summary = df.groupby('product').agg({ 'quantity': 'sum', 'price': ['mean', 'sum'], 'customer_id': 'nunique' }).reset_index() print(product_summary.sort_values(('price', 'sum'), ascending=False).head()) df['date'] = pd.to_datetime(df['date']) daily_sales = df.groupby('date')['price'].sum().plot(figsize=(10, 5), title='Daily Sales') &#129514; Test results: Slowest for CSV: Pandas is the slowest for both reading and writing CSV files, especially with larger datasets. Moderate Parquet Performance: It handles Parquet files well for small datasets. But, it slows down with larger ones.Inconsistent with Arrow: Generally faster than CSV but slower than Parquet. Performance varies widely across different dataset sizes.Good for Data Exploration: It suits users who value ease of use and flexibility over speed. But, it lags behind other tools for high-performance tasks, especially with large datasets.Thanks for reading The Pipe & The Line! This post is public so feel free to share it.Share&#128059;&#8205;&#10052;&#65039; PolarsPolars is designed to be lightning-fast and memory-efficient. Think of it as a turbocharged version of Pandas. It's great for:Working with very large datasets that may be slow in Pandas.Performing complex data manipulations quickly.Handling time-series data efficiently.This is how the syntax looks:import polars as pl df = pl.read_csv('sales_data.csv') print(df.head()) print(df.describe()) product_summary = df.groupby('product').agg([ pl.col('quantity').sum().alias('total_quantity'), pl.col('price').mean().alias('avg_price'), (pl.col('quantity') * pl.col('price')).sum().alias('total_sales'), pl.col('customer_id').n_unique().alias('unique_customers') ]).sort('total_sales', descending=True) print(product_summary.head()) daily_sales = df.select( pl.col('date').str.strptime(pl.Date, fmt='%Y-%m-%d'), (pl.col('quantity') * pl.col('price')).alias('sales') ).groupby('date').sum() print(daily_sales.head())&#129514; Test results: Superior CSV Performance: The fastest at reading and writing CSV files, for any dataset size.Consistent Arrow Performance: It has a fast, consistent speed for reading and writing Arrow files.Efficient Parquet Handling: Very fast in reading Parquet files, though write times increase more noticeably with larger datasets.Consistent Across File Types: For read and write operations, Polars performs well on all file types, especially for small and medium datasets.Ideal for Large-Scale Operations: Best choice for performance-critical tasks, especially when dealing with large datasets in various formats. High dataset writing highlight: When writing bigger Parque/Arrow datasets, it was slightly outperformed by Pandas. Overhead of Polars compared to Pandas might be higher, or this could be an outlier that can be invalidated with more tests.Thanks for reading The Pipe & The Line! Subscribe to get new content and learnings for your data career!&#129414; DuckDB DuckDB is a super-fast in-memory database engine right inside your Python program. It's designed to be:Easy to set up. Lightning-fast for data analysis tasks.Capable of handling larger-than-memory datasets efficiently.The best thing is that it lets you use SQL &#10084;&#65039; right in your Python code. You can quickly analyze large amounts of data without learning a whole new language.This is how the syntax looks:import duckdb con = duckdb.connect(database=':memory:') con.execute(\"CREATE TABLE sales AS SELECT * FROM read_csv_auto('sales_data.csv')\") print(con.execute(\"SELECT * FROM sales LIMIT 5\").df()) print(con.execute(\"DESCRIBE sales\").df()) product_summary = con.execute(\"\"\" SELECT product, SUM(quantity) as total_quantity, AVG(price) as avg_price, SUM(quantity * price) as total_sales, COUNT(DISTINCT customer_id) as unique_customers FROM sales GROUP BY product ORDER BY total_sales DESC LIMIT 5 \"\"\").df() print(product_summary)&#129514; Test results: Excellent Parquet Performance: It reads Parquet files super quickly even with bigger datasets. It performs similarly to the other when writing Parquet. Efficient CSV Performance: DuckDB excels at reading/writing CSV files for all dataset sizes. It outpaces Pandas and remains competitive with Polars with large datasets.Inconsistent Arrow Performance: It's fast for small datasets. But, it degrades for larger ones, especially in reading.Balanced Performance: It suits medium to large data operations and emonstrates good scalability across all file formats.&#129514; General conclusionsFile Format Efficiency: Parquet and Arrow are better than CSV for larger datasets.Scalability: Polars and DuckDB scale better than Pandas. They are better for larger datasets.Use Case Dependence: Polars is faster. But, DuckDB is good for exploratory analysis, it can handle huge workloads!&#128221; TL;DR&#128060; Pandas: use it for small explorations,& analysis. If possible, prioritze Parquet over CSV.&#129414; DuckDB: An amazing option to run EDAs for big datasets and use SQL without further effort. Use Parquet or even CSV over Arrow. &#128059;&#8205;&#10052;&#65039; Polars: it can take huge workloads easily, specially with Arrow and Parquet. If you enjoyed the content, hit the like &#10084;&#65039; button, share, comment, repost, and all those nice things people do when like stuff these days. Glad to know you made it to this part!Hi, I am Alejandro Aboy. I am currently working as a Data Engineer. I started in digital marketing at 19. I gained experience in website tracking, advertising, and analytics. I also founded my agency. In 2021, I found my passion for data engineering. So, I shifted my career focus, despite lacking a CS degree. I'm now pursuing this path, leveraging my diverse experience and willingness to learn.Follow me on Linkedin"}]}
{"messages": [{"role": "user", "content": "Discuss how learning new data processing tools can enhance a data engineer's career opportunities."}, {"role": "assistant", "content": "Let&#8217;s do some quick history:Pandas: released in January 2008Polars: released in October 2020DuckDB: released in July 2019It&#8217;s been 16 years of Pandas dominance until new options have appeared on the scene. Nowadays, we can seamlessly run SQL through python and process more data without jumping into Spark or Cloud Warehouses. I wanted to understand the difference between the 3 of them and how they perform across different file types.I&#8217;ve created this Github Repo: &#128270; File Processing Benchmark&#9888;&#65039; Disclaimer: All tests ran on Mac M1 Pro 16GB 2021. The presented conclusions might vary from other experiments.Let&#8217;s get started! &#128060; PandasPandas is often the first tool data analysts learn for Python. It's easy to use, versatile and helps you work with structured data easily. It's great for:Loading data from various sources (CSV, Excel, Databases).Cleaning and organizing messy data.Performing calculations and statistical analyses.Creating simple visualizations.This is how the syntax looks:import pandas as pd df = pd.read_csv('sales_data.csv') print(df.head()) print(df.describe()) product_summary = df.groupby('product').agg({ 'quantity': 'sum', 'price': ['mean', 'sum'], 'customer_id': 'nunique' }).reset_index() print(product_summary.sort_values(('price', 'sum'), ascending=False).head()) df['date'] = pd.to_datetime(df['date']) daily_sales = df.groupby('date')['price'].sum().plot(figsize=(10, 5), title='Daily Sales') &#129514; Test results: Slowest for CSV: Pandas is the slowest for both reading and writing CSV files, especially with larger datasets. Moderate Parquet Performance: It handles Parquet files well for small datasets. But, it slows down with larger ones.Inconsistent with Arrow: Generally faster than CSV but slower than Parquet. Performance varies widely across different dataset sizes.Good for Data Exploration: It suits users who value ease of use and flexibility over speed. But, it lags behind other tools for high-performance tasks, especially with large datasets.Thanks for reading The Pipe & The Line! This post is public so feel free to share it.Share&#128059;&#8205;&#10052;&#65039; PolarsPolars is designed to be lightning-fast and memory-efficient. Think of it as a turbocharged version of Pandas. It's great for:Working with very large datasets that may be slow in Pandas.Performing complex data manipulations quickly.Handling time-series data efficiently.This is how the syntax looks:import polars as pl df = pl.read_csv('sales_data.csv') print(df.head()) print(df.describe()) product_summary = df.groupby('product').agg([ pl.col('quantity').sum().alias('total_quantity'), pl.col('price').mean().alias('avg_price'), (pl.col('quantity') * pl.col('price')).sum().alias('total_sales'), pl.col('customer_id').n_unique().alias('unique_customers') ]).sort('total_sales', descending=True) print(product_summary.head()) daily_sales = df.select( pl.col('date').str.strptime(pl.Date, fmt='%Y-%m-%d'), (pl.col('quantity') * pl.col('price')).alias('sales') ).groupby('date').sum() print(daily_sales.head())&#129514; Test results: Superior CSV Performance: The fastest at reading and writing CSV files, for any dataset size.Consistent Arrow Performance: It has a fast, consistent speed for reading and writing Arrow files.Efficient Parquet Handling: Very fast in reading Parquet files, though write times increase more noticeably with larger datasets.Consistent Across File Types: For read and write operations, Polars performs well on all file types, especially for small and medium datasets.Ideal for Large-Scale Operations: Best choice for performance-critical tasks, especially when dealing with large datasets in various formats. High dataset writing highlight: When writing bigger Parque/Arrow datasets, it was slightly outperformed by Pandas. Overhead of Polars compared to Pandas might be higher, or this could be an outlier that can be invalidated with more tests.Thanks for reading The Pipe & The Line! Subscribe to get new content and learnings for your data career!&#129414; DuckDB DuckDB is a super-fast in-memory database engine right inside your Python program. It's designed to be:Easy to set up. Lightning-fast for data analysis tasks.Capable of handling larger-than-memory datasets efficiently.The best thing is that it lets you use SQL &#10084;&#65039; right in your Python code. You can quickly analyze large amounts of data without learning a whole new language.This is how the syntax looks:import duckdb con = duckdb.connect(database=':memory:') con.execute(\"CREATE TABLE sales AS SELECT * FROM read_csv_auto('sales_data.csv')\") print(con.execute(\"SELECT * FROM sales LIMIT 5\").df()) print(con.execute(\"DESCRIBE sales\").df()) product_summary = con.execute(\"\"\" SELECT product, SUM(quantity) as total_quantity, AVG(price) as avg_price, SUM(quantity * price) as total_sales, COUNT(DISTINCT customer_id) as unique_customers FROM sales GROUP BY product ORDER BY total_sales DESC LIMIT 5 \"\"\").df() print(product_summary)&#129514; Test results: Excellent Parquet Performance: It reads Parquet files super quickly even with bigger datasets. It performs similarly to the other when writing Parquet. Efficient CSV Performance: DuckDB excels at reading/writing CSV files for all dataset sizes. It outpaces Pandas and remains competitive with Polars with large datasets.Inconsistent Arrow Performance: It's fast for small datasets. But, it degrades for larger ones, especially in reading.Balanced Performance: It suits medium to large data operations and emonstrates good scalability across all file formats.&#129514; General conclusionsFile Format Efficiency: Parquet and Arrow are better than CSV for larger datasets.Scalability: Polars and DuckDB scale better than Pandas. They are better for larger datasets.Use Case Dependence: Polars is faster. But, DuckDB is good for exploratory analysis, it can handle huge workloads!&#128221; TL;DR&#128060; Pandas: use it for small explorations,& analysis. If possible, prioritze Parquet over CSV.&#129414; DuckDB: An amazing option to run EDAs for big datasets and use SQL without further effort. Use Parquet or even CSV over Arrow. &#128059;&#8205;&#10052;&#65039; Polars: it can take huge workloads easily, specially with Arrow and Parquet. If you enjoyed the content, hit the like &#10084;&#65039; button, share, comment, repost, and all those nice things people do when like stuff these days. Glad to know you made it to this part!Hi, I am Alejandro Aboy. I am currently working as a Data Engineer. I started in digital marketing at 19. I gained experience in website tracking, advertising, and analytics. I also founded my agency. In 2021, I found my passion for data engineering. So, I shifted my career focus, despite lacking a CS degree. I'm now pursuing this path, leveraging my diverse experience and willingness to learn.Follow me on Linkedin"}]}
{"messages": [{"role": "user", "content": "Create a practical guide on integrating SQL with Python for efficient data analysis workflows."}, {"role": "assistant", "content": "Let&#8217;s do some quick history:Pandas: released in January 2008Polars: released in October 2020DuckDB: released in July 2019It&#8217;s been 16 years of Pandas dominance until new options have appeared on the scene. Nowadays, we can seamlessly run SQL through python and process more data without jumping into Spark or Cloud Warehouses. I wanted to understand the difference between the 3 of them and how they perform across different file types.I&#8217;ve created this Github Repo: &#128270; File Processing Benchmark&#9888;&#65039; Disclaimer: All tests ran on Mac M1 Pro 16GB 2021. The presented conclusions might vary from other experiments.Let&#8217;s get started! &#128060; PandasPandas is often the first tool data analysts learn for Python. It's easy to use, versatile and helps you work with structured data easily. It's great for:Loading data from various sources (CSV, Excel, Databases).Cleaning and organizing messy data.Performing calculations and statistical analyses.Creating simple visualizations.This is how the syntax looks:import pandas as pd df = pd.read_csv('sales_data.csv') print(df.head()) print(df.describe()) product_summary = df.groupby('product').agg({ 'quantity': 'sum', 'price': ['mean', 'sum'], 'customer_id': 'nunique' }).reset_index() print(product_summary.sort_values(('price', 'sum'), ascending=False).head()) df['date'] = pd.to_datetime(df['date']) daily_sales = df.groupby('date')['price'].sum().plot(figsize=(10, 5), title='Daily Sales') &#129514; Test results: Slowest for CSV: Pandas is the slowest for both reading and writing CSV files, especially with larger datasets. Moderate Parquet Performance: It handles Parquet files well for small datasets. But, it slows down with larger ones.Inconsistent with Arrow: Generally faster than CSV but slower than Parquet. Performance varies widely across different dataset sizes.Good for Data Exploration: It suits users who value ease of use and flexibility over speed. But, it lags behind other tools for high-performance tasks, especially with large datasets.Thanks for reading The Pipe & The Line! This post is public so feel free to share it.Share&#128059;&#8205;&#10052;&#65039; PolarsPolars is designed to be lightning-fast and memory-efficient. Think of it as a turbocharged version of Pandas. It's great for:Working with very large datasets that may be slow in Pandas.Performing complex data manipulations quickly.Handling time-series data efficiently.This is how the syntax looks:import polars as pl df = pl.read_csv('sales_data.csv') print(df.head()) print(df.describe()) product_summary = df.groupby('product').agg([ pl.col('quantity').sum().alias('total_quantity'), pl.col('price').mean().alias('avg_price'), (pl.col('quantity') * pl.col('price')).sum().alias('total_sales'), pl.col('customer_id').n_unique().alias('unique_customers') ]).sort('total_sales', descending=True) print(product_summary.head()) daily_sales = df.select( pl.col('date').str.strptime(pl.Date, fmt='%Y-%m-%d'), (pl.col('quantity') * pl.col('price')).alias('sales') ).groupby('date').sum() print(daily_sales.head())&#129514; Test results: Superior CSV Performance: The fastest at reading and writing CSV files, for any dataset size.Consistent Arrow Performance: It has a fast, consistent speed for reading and writing Arrow files.Efficient Parquet Handling: Very fast in reading Parquet files, though write times increase more noticeably with larger datasets.Consistent Across File Types: For read and write operations, Polars performs well on all file types, especially for small and medium datasets.Ideal for Large-Scale Operations: Best choice for performance-critical tasks, especially when dealing with large datasets in various formats. High dataset writing highlight: When writing bigger Parque/Arrow datasets, it was slightly outperformed by Pandas. Overhead of Polars compared to Pandas might be higher, or this could be an outlier that can be invalidated with more tests.Thanks for reading The Pipe & The Line! Subscribe to get new content and learnings for your data career!&#129414; DuckDB DuckDB is a super-fast in-memory database engine right inside your Python program. It's designed to be:Easy to set up. Lightning-fast for data analysis tasks.Capable of handling larger-than-memory datasets efficiently.The best thing is that it lets you use SQL &#10084;&#65039; right in your Python code. You can quickly analyze large amounts of data without learning a whole new language.This is how the syntax looks:import duckdb con = duckdb.connect(database=':memory:') con.execute(\"CREATE TABLE sales AS SELECT * FROM read_csv_auto('sales_data.csv')\") print(con.execute(\"SELECT * FROM sales LIMIT 5\").df()) print(con.execute(\"DESCRIBE sales\").df()) product_summary = con.execute(\"\"\" SELECT product, SUM(quantity) as total_quantity, AVG(price) as avg_price, SUM(quantity * price) as total_sales, COUNT(DISTINCT customer_id) as unique_customers FROM sales GROUP BY product ORDER BY total_sales DESC LIMIT 5 \"\"\").df() print(product_summary)&#129514; Test results: Excellent Parquet Performance: It reads Parquet files super quickly even with bigger datasets. It performs similarly to the other when writing Parquet. Efficient CSV Performance: DuckDB excels at reading/writing CSV files for all dataset sizes. It outpaces Pandas and remains competitive with Polars with large datasets.Inconsistent Arrow Performance: It's fast for small datasets. But, it degrades for larger ones, especially in reading.Balanced Performance: It suits medium to large data operations and emonstrates good scalability across all file formats.&#129514; General conclusionsFile Format Efficiency: Parquet and Arrow are better than CSV for larger datasets.Scalability: Polars and DuckDB scale better than Pandas. They are better for larger datasets.Use Case Dependence: Polars is faster. But, DuckDB is good for exploratory analysis, it can handle huge workloads!&#128221; TL;DR&#128060; Pandas: use it for small explorations,& analysis. If possible, prioritze Parquet over CSV.&#129414; DuckDB: An amazing option to run EDAs for big datasets and use SQL without further effort. Use Parquet or even CSV over Arrow. &#128059;&#8205;&#10052;&#65039; Polars: it can take huge workloads easily, specially with Arrow and Parquet. If you enjoyed the content, hit the like &#10084;&#65039; button, share, comment, repost, and all those nice things people do when like stuff these days. Glad to know you made it to this part!Hi, I am Alejandro Aboy. I am currently working as a Data Engineer. I started in digital marketing at 19. I gained experience in website tracking, advertising, and analytics. I also founded my agency. In 2021, I found my passion for data engineering. So, I shifted my career focus, despite lacking a CS degree. I'm now pursuing this path, leveraging my diverse experience and willingness to learn.Follow me on Linkedin"}]}
{"messages": [{"role": "user", "content": "Discuss the long-term implications of technical debt, focusing on data management practices in engineering projects."}, {"role": "assistant", "content": "If you haven&#8217;t checked the previous post, I will suggest taking a look: &#128201;&#128104;&#127995;&#8205;&#128187; Leave the Technical Debt you would like to inherit (Part 1)&#129335;&#127995;&#8205;&#9794;&#65039; Ask: &#8220;How can I help you(r future you)?&#8221;When adding new debt, you should think wisely because this is not only for your replacement, but also for you.You can check things you&#8217;ve done 6 months ago and have no idea of what you were thinking at that time.Maybe it's because you've learned new things and improved your skills. That's usually the reason.Meme to the rescue.There are some areas we can influence to improve our legacy in any project or company:&#128296; Implementation&#128259; Maintenance&#128221; DocumentationThanks for reading The Pipe & The Line! Subscribe to get new content and learnings for your data career!&#128296; Implementation: your footprint in the codeI received the best pieces of advice from these recommendations:&#128721; Don't add new logic unless necessary. (eg) you want to improve Slack alerts by polishing the error messages with more details. Nevertheless, you can see the error by logging into the tool. Just let it be and use your time to fix the errors in the first place. &#128300; Don't over-optimize. Avoid tweaking minor things unless strictly needed. It's hard to explain that rationale later if you go too deep. &#128221; Comments, comments, comments: try any LLM based software (e.g. Github Copilot). If generated comments don't make sense, your code may not be intuitive. This could help in the validation process.&#128064; Help the eyes with patterns: This may go against the DRY (Don't Repeat Yourself) principle. But, sometimes, using code patterns or templates helps outsiders understand the code. It catches their eye. I like how talks about patterns of the DRY (Don&#8217;t repeat yourself) principle in this post:&#128259; Maintenance: your everyday missionThese are the most important points I&#8217;ve identified on this topic:&#9200; Don&#8217;t do &#8220;temporal hacks&#8221;; ask yourself:Would this give more issues or less problems?Are we trading one issue for another?Do I intend to return and correct it as soon as possible?&#128694;&#127995;&#129532; Come back and clean up:Don&#8217;t abandon your stuff, clear out the noise to reduce complexity. Use new learnings to improve things when opportunities are there. Don&#8217;t be afraid to remove, archive, or deprecate stuff if relevant. But always leave backups in case you need them. &#128202;&#128104;&#127995;&#8205;&#128187; Monitor and decide; pay attention to the details:Can we make this more readable?Can we make this faster?Would improving X make a real difference?Save the overengineering for your side projects. You are the only one touching them.I've seen engineers spend weeks on custom Python packages to ingest data from a source.Plot twist: the stakeholders were fine checking the insights directly on the tool.Think and assess your decisions before going through all the overengineering!Old classic meme. Source: the internet.Thanks for reading The Pipe & The Line! This post is public so feel free to share it.Share&#128221; Documentation: your legacy within the docsI understand way better when I have to explain them.Writing documentation gives you clarity and enhances your documentation skills. When you start putting things outside your head, context hits you with reality. My favorite documentation approaches are:&#128128; Post-mortem docs: Describe the issues and how we addressed them (e.g. SQL queries taking too much time in dbt)Explain why this happened. Keep asking \"Why?\" until you get to the root cause (5Whys technique)Explain the actions taken. Justify why it must never happen again with that solution.&#128284; Decision docs:Describe why you&#8217;ve decided to use X instead of Y (e.g. using Metabase instead of Superset for BI assets.)Cite research to support your rationale. (e.g. many engineers on Reddit complain about deployment issues with Superset)Explain the implementation plan and the caveats about the decision.&#128104;&#127995;&#8205;&#127979; Workshops:Run open Q&As and workshops to transfer knowledge.Provide context on the topic, explain the outcome and what the approaches were. (e.g. After collecting user feedback, we added caching to reduce Metabase loading times.)After running the workshop, grab feedback and write down a guide with a Q&A section to summarize the topics.By now, you can see the base pattern of documentation; of course, you can add more angles, but the baseline is: DescribeExplainAction takenBear in mind that the sooner you document, the better. Then you can organize those docs in an accessible workspace:How a nice Docs workspace could look like.Hope you&#8217;ve liked the series on technical debt. Take a look to the previous post if you haven&#8217;t yet: &#128201;&#128104;&#127995;&#8205;&#128187; Leave the Technical Debt you would like to inherit (Part 1)&#128221; TL;DR&#129335;&#127995;&#8205;&#9794;&#65039; How can I you help you(r future you)?&#128296; Implementation: your footprint in the code&#128721; Don't add new logic unless necessary&#128300; Don't over-optimize&#128221; Comments, comments, comments&#128064; Help the eyes with patterns&#128259; Maintenance: your everyday mission&#9200; Don&#8217;t do &#8220;temporal hacks&#8221;; ask yourself&#128694;&#127995;&#129532; Come back and clean up&#128202;&#128104;&#127995;&#8205;&#128187; Monitor and decide; pay attention to the details&#128221; Documentation: your legacy within the docs&#128128; Post-mortem docs&#128284; Decision docs&#128104;&#127995;&#8205;&#127979; WorkshopsIf you enjoyed the content, hit the like &#10084;&#65039; button, share, comment, repost, and all those nice things people do when like stuff these days. Glad to know you made it to this part!Hi, I am Alejandro Aboy. I am currently working as a Data Engineer. I started in digital marketing at 19. I gained experience in website tracking, advertising, and analytics. I also founded my agency. In 2021, I found my passion for data engineering. So, I shifted my career focus, despite lacking a CS degree. I'm now pursuing this path, leveraging my diverse experience and willingness to learn.Follow me on Linkedin"}]}
{"messages": [{"role": "user", "content": "Write a guide on effective communication strategies for reducing technical debt among team members in tech environments."}, {"role": "assistant", "content": "If you haven&#8217;t checked the previous post, I will suggest taking a look: &#128201;&#128104;&#127995;&#8205;&#128187; Leave the Technical Debt you would like to inherit (Part 1)&#129335;&#127995;&#8205;&#9794;&#65039; Ask: &#8220;How can I help you(r future you)?&#8221;When adding new debt, you should think wisely because this is not only for your replacement, but also for you.You can check things you&#8217;ve done 6 months ago and have no idea of what you were thinking at that time.Maybe it's because you've learned new things and improved your skills. That's usually the reason.Meme to the rescue.There are some areas we can influence to improve our legacy in any project or company:&#128296; Implementation&#128259; Maintenance&#128221; DocumentationThanks for reading The Pipe & The Line! Subscribe to get new content and learnings for your data career!&#128296; Implementation: your footprint in the codeI received the best pieces of advice from these recommendations:&#128721; Don't add new logic unless necessary. (eg) you want to improve Slack alerts by polishing the error messages with more details. Nevertheless, you can see the error by logging into the tool. Just let it be and use your time to fix the errors in the first place. &#128300; Don't over-optimize. Avoid tweaking minor things unless strictly needed. It's hard to explain that rationale later if you go too deep. &#128221; Comments, comments, comments: try any LLM based software (e.g. Github Copilot). If generated comments don't make sense, your code may not be intuitive. This could help in the validation process.&#128064; Help the eyes with patterns: This may go against the DRY (Don't Repeat Yourself) principle. But, sometimes, using code patterns or templates helps outsiders understand the code. It catches their eye. I like how talks about patterns of the DRY (Don&#8217;t repeat yourself) principle in this post:&#128259; Maintenance: your everyday missionThese are the most important points I&#8217;ve identified on this topic:&#9200; Don&#8217;t do &#8220;temporal hacks&#8221;; ask yourself:Would this give more issues or less problems?Are we trading one issue for another?Do I intend to return and correct it as soon as possible?&#128694;&#127995;&#129532; Come back and clean up:Don&#8217;t abandon your stuff, clear out the noise to reduce complexity. Use new learnings to improve things when opportunities are there. Don&#8217;t be afraid to remove, archive, or deprecate stuff if relevant. But always leave backups in case you need them. &#128202;&#128104;&#127995;&#8205;&#128187; Monitor and decide; pay attention to the details:Can we make this more readable?Can we make this faster?Would improving X make a real difference?Save the overengineering for your side projects. You are the only one touching them.I've seen engineers spend weeks on custom Python packages to ingest data from a source.Plot twist: the stakeholders were fine checking the insights directly on the tool.Think and assess your decisions before going through all the overengineering!Old classic meme. Source: the internet.Thanks for reading The Pipe & The Line! This post is public so feel free to share it.Share&#128221; Documentation: your legacy within the docsI understand way better when I have to explain them.Writing documentation gives you clarity and enhances your documentation skills. When you start putting things outside your head, context hits you with reality. My favorite documentation approaches are:&#128128; Post-mortem docs: Describe the issues and how we addressed them (e.g. SQL queries taking too much time in dbt)Explain why this happened. Keep asking \"Why?\" until you get to the root cause (5Whys technique)Explain the actions taken. Justify why it must never happen again with that solution.&#128284; Decision docs:Describe why you&#8217;ve decided to use X instead of Y (e.g. using Metabase instead of Superset for BI assets.)Cite research to support your rationale. (e.g. many engineers on Reddit complain about deployment issues with Superset)Explain the implementation plan and the caveats about the decision.&#128104;&#127995;&#8205;&#127979; Workshops:Run open Q&As and workshops to transfer knowledge.Provide context on the topic, explain the outcome and what the approaches were. (e.g. After collecting user feedback, we added caching to reduce Metabase loading times.)After running the workshop, grab feedback and write down a guide with a Q&A section to summarize the topics.By now, you can see the base pattern of documentation; of course, you can add more angles, but the baseline is: DescribeExplainAction takenBear in mind that the sooner you document, the better. Then you can organize those docs in an accessible workspace:How a nice Docs workspace could look like.Hope you&#8217;ve liked the series on technical debt. Take a look to the previous post if you haven&#8217;t yet: &#128201;&#128104;&#127995;&#8205;&#128187; Leave the Technical Debt you would like to inherit (Part 1)&#128221; TL;DR&#129335;&#127995;&#8205;&#9794;&#65039; How can I you help you(r future you)?&#128296; Implementation: your footprint in the code&#128721; Don't add new logic unless necessary&#128300; Don't over-optimize&#128221; Comments, comments, comments&#128064; Help the eyes with patterns&#128259; Maintenance: your everyday mission&#9200; Don&#8217;t do &#8220;temporal hacks&#8221;; ask yourself&#128694;&#127995;&#129532; Come back and clean up&#128202;&#128104;&#127995;&#8205;&#128187; Monitor and decide; pay attention to the details&#128221; Documentation: your legacy within the docs&#128128; Post-mortem docs&#128284; Decision docs&#128104;&#127995;&#8205;&#127979; WorkshopsIf you enjoyed the content, hit the like &#10084;&#65039; button, share, comment, repost, and all those nice things people do when like stuff these days. Glad to know you made it to this part!Hi, I am Alejandro Aboy. I am currently working as a Data Engineer. I started in digital marketing at 19. I gained experience in website tracking, advertising, and analytics. I also founded my agency. In 2021, I found my passion for data engineering. So, I shifted my career focus, despite lacking a CS degree. I'm now pursuing this path, leveraging my diverse experience and willingness to learn.Follow me on Linkedin"}]}
{"messages": [{"role": "user", "content": "Create a checklist for evaluating existing technical debt and prioritizing it for future projects in data engineering."}, {"role": "assistant", "content": "If you haven&#8217;t checked the previous post, I will suggest taking a look: &#128201;&#128104;&#127995;&#8205;&#128187; Leave the Technical Debt you would like to inherit (Part 1)&#129335;&#127995;&#8205;&#9794;&#65039; Ask: &#8220;How can I help you(r future you)?&#8221;When adding new debt, you should think wisely because this is not only for your replacement, but also for you.You can check things you&#8217;ve done 6 months ago and have no idea of what you were thinking at that time.Maybe it's because you've learned new things and improved your skills. That's usually the reason.Meme to the rescue.There are some areas we can influence to improve our legacy in any project or company:&#128296; Implementation&#128259; Maintenance&#128221; DocumentationThanks for reading The Pipe & The Line! Subscribe to get new content and learnings for your data career!&#128296; Implementation: your footprint in the codeI received the best pieces of advice from these recommendations:&#128721; Don't add new logic unless necessary. (eg) you want to improve Slack alerts by polishing the error messages with more details. Nevertheless, you can see the error by logging into the tool. Just let it be and use your time to fix the errors in the first place. &#128300; Don't over-optimize. Avoid tweaking minor things unless strictly needed. It's hard to explain that rationale later if you go too deep. &#128221; Comments, comments, comments: try any LLM based software (e.g. Github Copilot). If generated comments don't make sense, your code may not be intuitive. This could help in the validation process.&#128064; Help the eyes with patterns: This may go against the DRY (Don't Repeat Yourself) principle. But, sometimes, using code patterns or templates helps outsiders understand the code. It catches their eye. I like how talks about patterns of the DRY (Don&#8217;t repeat yourself) principle in this post:&#128259; Maintenance: your everyday missionThese are the most important points I&#8217;ve identified on this topic:&#9200; Don&#8217;t do &#8220;temporal hacks&#8221;; ask yourself:Would this give more issues or less problems?Are we trading one issue for another?Do I intend to return and correct it as soon as possible?&#128694;&#127995;&#129532; Come back and clean up:Don&#8217;t abandon your stuff, clear out the noise to reduce complexity. Use new learnings to improve things when opportunities are there. Don&#8217;t be afraid to remove, archive, or deprecate stuff if relevant. But always leave backups in case you need them. &#128202;&#128104;&#127995;&#8205;&#128187; Monitor and decide; pay attention to the details:Can we make this more readable?Can we make this faster?Would improving X make a real difference?Save the overengineering for your side projects. You are the only one touching them.I've seen engineers spend weeks on custom Python packages to ingest data from a source.Plot twist: the stakeholders were fine checking the insights directly on the tool.Think and assess your decisions before going through all the overengineering!Old classic meme. Source: the internet.Thanks for reading The Pipe & The Line! This post is public so feel free to share it.Share&#128221; Documentation: your legacy within the docsI understand way better when I have to explain them.Writing documentation gives you clarity and enhances your documentation skills. When you start putting things outside your head, context hits you with reality. My favorite documentation approaches are:&#128128; Post-mortem docs: Describe the issues and how we addressed them (e.g. SQL queries taking too much time in dbt)Explain why this happened. Keep asking \"Why?\" until you get to the root cause (5Whys technique)Explain the actions taken. Justify why it must never happen again with that solution.&#128284; Decision docs:Describe why you&#8217;ve decided to use X instead of Y (e.g. using Metabase instead of Superset for BI assets.)Cite research to support your rationale. (e.g. many engineers on Reddit complain about deployment issues with Superset)Explain the implementation plan and the caveats about the decision.&#128104;&#127995;&#8205;&#127979; Workshops:Run open Q&As and workshops to transfer knowledge.Provide context on the topic, explain the outcome and what the approaches were. (e.g. After collecting user feedback, we added caching to reduce Metabase loading times.)After running the workshop, grab feedback and write down a guide with a Q&A section to summarize the topics.By now, you can see the base pattern of documentation; of course, you can add more angles, but the baseline is: DescribeExplainAction takenBear in mind that the sooner you document, the better. Then you can organize those docs in an accessible workspace:How a nice Docs workspace could look like.Hope you&#8217;ve liked the series on technical debt. Take a look to the previous post if you haven&#8217;t yet: &#128201;&#128104;&#127995;&#8205;&#128187; Leave the Technical Debt you would like to inherit (Part 1)&#128221; TL;DR&#129335;&#127995;&#8205;&#9794;&#65039; How can I you help you(r future you)?&#128296; Implementation: your footprint in the code&#128721; Don't add new logic unless necessary&#128300; Don't over-optimize&#128221; Comments, comments, comments&#128064; Help the eyes with patterns&#128259; Maintenance: your everyday mission&#9200; Don&#8217;t do &#8220;temporal hacks&#8221;; ask yourself&#128694;&#127995;&#129532; Come back and clean up&#128202;&#128104;&#127995;&#8205;&#128187; Monitor and decide; pay attention to the details&#128221; Documentation: your legacy within the docs&#128128; Post-mortem docs&#128284; Decision docs&#128104;&#127995;&#8205;&#127979; WorkshopsIf you enjoyed the content, hit the like &#10084;&#65039; button, share, comment, repost, and all those nice things people do when like stuff these days. Glad to know you made it to this part!Hi, I am Alejandro Aboy. I am currently working as a Data Engineer. I started in digital marketing at 19. I gained experience in website tracking, advertising, and analytics. I also founded my agency. In 2021, I found my passion for data engineering. So, I shifted my career focus, despite lacking a CS degree. I'm now pursuing this path, leveraging my diverse experience and willingness to learn.Follow me on Linkedin"}]}
{"messages": [{"role": "user", "content": "Explain how to use automated tools to assess code quality and maintainability in data engineering projects."}, {"role": "assistant", "content": "I've been talking about how to catch code issues when writing or merging pull requests.Most of the time, you would follow these steps:Make it work.Optimize it.Make it readable.Make it easy to understand for outsiders.I&#8217;ve been talking about the latter in: You might have code reviews by another coworker, but some things are hard to catch. Some questions might come up when running these checks:How do we measure complexity from an unbiased point of view?How do you really know if it&#8217;s hard or easy to maintain?There is a way to start.&#128104;&#127995;&#8205;&#128187; Checking your code with Radon & Xenon&#128073; What is Radon?Radon measures complexity and maintainability. In other words, it will check things like length, nested logics, readability, and how easy it is to maintain it.It will provide a score for these 2 metrics, both grading ranges from A (simplest) to F (most complex):Cyclomatic Complexity (CC)Measures how complex a function is by counting the number of decisions or branches (e.g., if, for, while). Lower complexity means simpler, easier-to-read code. Maintainability Index (MI)Tells you how easy the code is to maintain. Uses the length of your code, the number of comments, and its complexity to calculate this score.&#128073; What is Xenon?Xenon builds on Radon by enforcing complexity rules. You set a max complexity (like Grade B). Xenon checks if your code is within that limit.Thanks for reading The Pipe & The Line! Subscribe for free to receive new posts and support my work.How do you use them?I&#8217;ve created a Github Repo to simulate the process: &#128104;&#127995;&#8205;&#128187; Code Quality Checker with Radon and Xenon&#128104;&#127995;&#8205;&#128187; LocallyInstall both packages:pip install radon xenonCheck the Cyclomatic Complexity (CC) and select the path. In this case, we are using the root:radon cc ./ -sThis will output the grade of complexity (A-F) and the score for each method/function (1-100):The closer to A and 1, the better.Check the Maintainability Index (MI). It will grade the maintainability (A-F) of each file:radon mi ./The closer to A, the better.This will output the enforced grade. In this case, we will raise anything above B.xenon --max-absolute B ./You can modify the complexity threshold for Xenon by changing the --max-absolute flag in your GitHub Actions workflow or when running it locally.Thanks for reading The Pipe & The Line! Subscribe to get new content and learnings for your data career!&#128044; Docker & Github Actions I&#8217;ve created a Dockerfile and a Github workflow file to showcase it.This CI/CD snippet could be better. It could parse the output and send it via Slack. But, GitHub notifications might be enough.I have left a sample pull request with failed checks so you can see how it works!&#128165; Bonus: fixing code applying with Radon/Xenon with AI!Instead of just pasting code in ChatGPT or Claude and asking, \"make it more readable\", we can try something else.In this example, I have used Cursor:Asking Cursor (Claude 3.5) to suggest fixes based on MI and CC metrics.This is the answer from Claude 3.5 Sonnet&#129392;:Claude&#8217;s output after prompting.The result of the code after applying the suggestions:Output of Radon/Xenon after applying suggestions by Claude.&#128221; TL;DR&#128104;&#127995;&#8205;&#128187; Checking your code with Radon & XenonRadon measures complexity and maintainability. Xenon enforces complexity rules with your own thresolds. &#128064; How do you use them?&#128104;&#127995;&#8205;&#128187; Locally by installing package and running the commands.&#128044; Docker & Github Actions to avoid dependencies issues.&#128165; Fixing code with Radon/Xenon and AI!If you enjoyed the content, hit the like &#10084;&#65039; button, share, comment, repost, and all those nice things people do when like stuff these days. Glad to know you made it to this part!Hi, I am Alejandro Aboy. I am currently working as a Data Engineer. I started in digital marketing at 19. I gained experience in website tracking, advertising, and analytics. I also founded my agency. In 2021, I found my passion for data engineering. So, I shifted my career focus, despite lacking a CS degree. I'm now pursuing this path, leveraging my diverse experience and willingness to learn.Follow me on Linkedin"}]}
{"messages": [{"role": "user", "content": "Discuss the importance of code readability and collaboration in a data engineering team and how to achieve it."}, {"role": "assistant", "content": "I've been talking about how to catch code issues when writing or merging pull requests.Most of the time, you would follow these steps:Make it work.Optimize it.Make it readable.Make it easy to understand for outsiders.I&#8217;ve been talking about the latter in: You might have code reviews by another coworker, but some things are hard to catch. Some questions might come up when running these checks:How do we measure complexity from an unbiased point of view?How do you really know if it&#8217;s hard or easy to maintain?There is a way to start.&#128104;&#127995;&#8205;&#128187; Checking your code with Radon & Xenon&#128073; What is Radon?Radon measures complexity and maintainability. In other words, it will check things like length, nested logics, readability, and how easy it is to maintain it.It will provide a score for these 2 metrics, both grading ranges from A (simplest) to F (most complex):Cyclomatic Complexity (CC)Measures how complex a function is by counting the number of decisions or branches (e.g., if, for, while). Lower complexity means simpler, easier-to-read code. Maintainability Index (MI)Tells you how easy the code is to maintain. Uses the length of your code, the number of comments, and its complexity to calculate this score.&#128073; What is Xenon?Xenon builds on Radon by enforcing complexity rules. You set a max complexity (like Grade B). Xenon checks if your code is within that limit.Thanks for reading The Pipe & The Line! Subscribe for free to receive new posts and support my work.How do you use them?I&#8217;ve created a Github Repo to simulate the process: &#128104;&#127995;&#8205;&#128187; Code Quality Checker with Radon and Xenon&#128104;&#127995;&#8205;&#128187; LocallyInstall both packages:pip install radon xenonCheck the Cyclomatic Complexity (CC) and select the path. In this case, we are using the root:radon cc ./ -sThis will output the grade of complexity (A-F) and the score for each method/function (1-100):The closer to A and 1, the better.Check the Maintainability Index (MI). It will grade the maintainability (A-F) of each file:radon mi ./The closer to A, the better.This will output the enforced grade. In this case, we will raise anything above B.xenon --max-absolute B ./You can modify the complexity threshold for Xenon by changing the --max-absolute flag in your GitHub Actions workflow or when running it locally.Thanks for reading The Pipe & The Line! Subscribe to get new content and learnings for your data career!&#128044; Docker & Github Actions I&#8217;ve created a Dockerfile and a Github workflow file to showcase it.This CI/CD snippet could be better. It could parse the output and send it via Slack. But, GitHub notifications might be enough.I have left a sample pull request with failed checks so you can see how it works!&#128165; Bonus: fixing code applying with Radon/Xenon with AI!Instead of just pasting code in ChatGPT or Claude and asking, \"make it more readable\", we can try something else.In this example, I have used Cursor:Asking Cursor (Claude 3.5) to suggest fixes based on MI and CC metrics.This is the answer from Claude 3.5 Sonnet&#129392;:Claude&#8217;s output after prompting.The result of the code after applying the suggestions:Output of Radon/Xenon after applying suggestions by Claude.&#128221; TL;DR&#128104;&#127995;&#8205;&#128187; Checking your code with Radon & XenonRadon measures complexity and maintainability. Xenon enforces complexity rules with your own thresolds. &#128064; How do you use them?&#128104;&#127995;&#8205;&#128187; Locally by installing package and running the commands.&#128044; Docker & Github Actions to avoid dependencies issues.&#128165; Fixing code with Radon/Xenon and AI!If you enjoyed the content, hit the like &#10084;&#65039; button, share, comment, repost, and all those nice things people do when like stuff these days. Glad to know you made it to this part!Hi, I am Alejandro Aboy. I am currently working as a Data Engineer. I started in digital marketing at 19. I gained experience in website tracking, advertising, and analytics. I also founded my agency. In 2021, I found my passion for data engineering. So, I shifted my career focus, despite lacking a CS degree. I'm now pursuing this path, leveraging my diverse experience and willingness to learn.Follow me on Linkedin"}]}
{"messages": [{"role": "user", "content": "Create a step-by-step guide on integrating code complexity checks into a CI/CD pipeline for data engineering workflows."}, {"role": "assistant", "content": "I've been talking about how to catch code issues when writing or merging pull requests.Most of the time, you would follow these steps:Make it work.Optimize it.Make it readable.Make it easy to understand for outsiders.I&#8217;ve been talking about the latter in: You might have code reviews by another coworker, but some things are hard to catch. Some questions might come up when running these checks:How do we measure complexity from an unbiased point of view?How do you really know if it&#8217;s hard or easy to maintain?There is a way to start.&#128104;&#127995;&#8205;&#128187; Checking your code with Radon & Xenon&#128073; What is Radon?Radon measures complexity and maintainability. In other words, it will check things like length, nested logics, readability, and how easy it is to maintain it.It will provide a score for these 2 metrics, both grading ranges from A (simplest) to F (most complex):Cyclomatic Complexity (CC)Measures how complex a function is by counting the number of decisions or branches (e.g., if, for, while). Lower complexity means simpler, easier-to-read code. Maintainability Index (MI)Tells you how easy the code is to maintain. Uses the length of your code, the number of comments, and its complexity to calculate this score.&#128073; What is Xenon?Xenon builds on Radon by enforcing complexity rules. You set a max complexity (like Grade B). Xenon checks if your code is within that limit.Thanks for reading The Pipe & The Line! Subscribe for free to receive new posts and support my work.How do you use them?I&#8217;ve created a Github Repo to simulate the process: &#128104;&#127995;&#8205;&#128187; Code Quality Checker with Radon and Xenon&#128104;&#127995;&#8205;&#128187; LocallyInstall both packages:pip install radon xenonCheck the Cyclomatic Complexity (CC) and select the path. In this case, we are using the root:radon cc ./ -sThis will output the grade of complexity (A-F) and the score for each method/function (1-100):The closer to A and 1, the better.Check the Maintainability Index (MI). It will grade the maintainability (A-F) of each file:radon mi ./The closer to A, the better.This will output the enforced grade. In this case, we will raise anything above B.xenon --max-absolute B ./You can modify the complexity threshold for Xenon by changing the --max-absolute flag in your GitHub Actions workflow or when running it locally.Thanks for reading The Pipe & The Line! Subscribe to get new content and learnings for your data career!&#128044; Docker & Github Actions I&#8217;ve created a Dockerfile and a Github workflow file to showcase it.This CI/CD snippet could be better. It could parse the output and send it via Slack. But, GitHub notifications might be enough.I have left a sample pull request with failed checks so you can see how it works!&#128165; Bonus: fixing code applying with Radon/Xenon with AI!Instead of just pasting code in ChatGPT or Claude and asking, \"make it more readable\", we can try something else.In this example, I have used Cursor:Asking Cursor (Claude 3.5) to suggest fixes based on MI and CC metrics.This is the answer from Claude 3.5 Sonnet&#129392;:Claude&#8217;s output after prompting.The result of the code after applying the suggestions:Output of Radon/Xenon after applying suggestions by Claude.&#128221; TL;DR&#128104;&#127995;&#8205;&#128187; Checking your code with Radon & XenonRadon measures complexity and maintainability. Xenon enforces complexity rules with your own thresolds. &#128064; How do you use them?&#128104;&#127995;&#8205;&#128187; Locally by installing package and running the commands.&#128044; Docker & Github Actions to avoid dependencies issues.&#128165; Fixing code with Radon/Xenon and AI!If you enjoyed the content, hit the like &#10084;&#65039; button, share, comment, repost, and all those nice things people do when like stuff these days. Glad to know you made it to this part!Hi, I am Alejandro Aboy. I am currently working as a Data Engineer. I started in digital marketing at 19. I gained experience in website tracking, advertising, and analytics. I also founded my agency. In 2021, I found my passion for data engineering. So, I shifted my career focus, despite lacking a CS degree. I'm now pursuing this path, leveraging my diverse experience and willingness to learn.Follow me on Linkedin"}]}
{"messages": [{"role": "user", "content": "Discuss the implications of technical debt in data pipelines and suggest best practices for managing it."}, {"role": "assistant", "content": "The thing is: everything you create is somehow debt. You must consider the type and its impact before merging that PR or publishing that dashboard's custom logic.You could make mistakes, improve stuff or take decisions based on complex scenarios. &#8220;Debt&#8221; is something that should be taken with responsibilty because it&#8217;s your legacy. I will describe some experience-based scenarios and list some actions to address them the best we can.&#128288; There are 2 sides of technical debtYou can approach the problem from two perspectives:New in the job:You start reading docs, the codebase, checking dashboards, and getting to know stakeholders.Probably not everything will &#8220;click&#8221; in the first months.There are many things that are not intuitive.Old in the job:You have been working for some time or leaving a job in the short term.You have introduced many changes: improvements,\"bandages\" or \"I will fix this later.\" logics.You've worked in a context that pushed you to make risky or cautious decisions.Based on these scenarios, we will break down from different angles.Some &#8220;art&#8221; to represent the situation.Thanks for reading The Pipe & The Line! Subscribe to get new content and learnings for your data career!&#128064; When analyzing debt, what are the relevant angles?The ContextYou should set a criterion to judge the reasons for decisions. I see four variables that affected them:ResourcesHeadcountGrowth expectationsCulture&#129297; ResourcesHaving too many resources to spend. (e.g.) All your services are working in Cloud, nothing open-source or self-managed.Trying to cut as much as possible. (e.g.) Fighting against maintenance issues to avoid going with a Cloud solution. &#129730; HeadcountHaving 200 more employees. (e.g.) The BI tool has 100 reports, but only 3 are being used. Having 50 employees after rounds of layoffs. (e.g.) Abandoned documentation or too manyassets to maintain.&#128640; Growth expectationsThinking growth will scale 10 times the actual growth they ended up having. (e.g.) Building custom logic to handle more customers that never came. &#129395; CultureDifferent management with a different mindset. More conservative, more aggressive, more risky, you name it.Also a culture that encourages knowledge base camps and recurrent documentation might make a good differenceLast but not least, different technical profiles:Having many techy people, usually results in many pieces all over the place.Having weak or strong technical leads usually results in different types of debt.Having recycled profiles that learned through doing without mentorship.An example of the reality many companies are facing. has summarized this situation with great empathy:Thanks for reading The Pipe & The Line! This post is public so feel free to share it.Share&#128182; Technical debt analysis: it&#8217;s time to pay the interestSome frameworks can help you classify debt by analyzing logics.The Technical Debt Quadrant: You could be making decisions and accepting the consequences. Or, you might lack the experience to make good choices.You could be making decisions and taking precautions, you could try to cover for bad outcomes. Or, you could just go ahead without much thought.You group decisions based on Reckless-Prudent and Deliberate-Inadvertent. Inspired by: https://martinfowler.com/bliki/TechnicalDebtQuadrant.htmlInterest Probability Framework (Managing Technical Debt - Book):It tries to predict if the debt will cause future problems, also seeks to know how painful and soon those effects will be. With all these ideas in mind, you&#8217;ll know what&#8217;s the best course of action.You have 3 main concepts within this framework:Interest accrual: how the \"interest\" increases over time. Same as a bank loan, where the longer you wait to pay it off, the more you owe. Technical debt becomes more expensive to address the longer it remains unfixed.Interest amount: effort required to address the issue if it causes problems. This could be measured in time, resources, or even lost opportunities. It tends to grow over time. Time sensitivity: how quickly the interest amount increases if the technical debt is not addressed. Some types of technical debt are more time-sensitive than others, meaning the cost of not fixing them grows rapidly.Visualizing.Thanks for reading The Pipe & The Line! Subscribe for free to receive new posts and support my work.Example: Outdated Python CodeImagine you have an old data pipeline. It still works, but is hard to maintain due to outdated libraries and code.Interest Accrual: Delaying the upgrade may cause new issues. The old libraries may become unsupported or incompatible with newer systems. Interest Amount: The effort to fix these issues and upgrade might grow. More dependencies may break, and new features may be harder to integrate.Time sensitivity: If those packages version are deprecated soon or added packages are incompatible, it will speed up the technical debt effect. This is purely experimental, but it works for me; you can adapt as much as you like. This combined approach can help self-evaluate the technical debt:Template to analyze tech debt.\"Increasing caching frequency from every 30 minutes to every minute&#8221; is Prudent/Deliberate because it was a decision taken with predictability in mind. So it can buy some time until new customers come and more optimization can be added.&#8220;Added an incremental logic, but schema changes will break the workflow&#8221; is Prudent/Inadvertent because the engineer added that logic without considering the impact of not handling schema changes, and no predictable outcome could be expected from it. The Interest Probability Framework helps you find the scope of the technical debt effect. The Quadrant category helps you take the right actions.Acting on a decision made without enough expertise is not the same as acting on one made under team pressure.Sometimes you can tweak the analysis for a different angle.For example, in this article, and break down technical debt to positively impact business value by thinking into:Faster time to market. Improved customer experience. Greater team efficiency. Engineering LeadershipEngineer&#8217;s guide to convincing your Product Manager to prioritize technical debtRead more10 months ago &#183; 60 likes &#183; 12 comments &#183; Gregor Ojstersek and Robert TaIn the next post I will be writing about how to help the ones from the future: not only your replacements or backups, but also yourself. &#128221; TL;DR&#128288; There are 2 sides of technical debtBe empathetic when joining a new companyLeave the best legacy while you work at the company&#128064; When analyzing debt, what are the relevant angles?&#129297; Resources: How the availability of money impacts decisions.&#129730; Headcount: How the amount of employees influences actions.&#128640; Growth expectations: How growth defines tech projects.&#129395; Culture: How leaders' mindsets affect contributors.&#128182; Technical debt analysis: it&#8217;s time to pay the interestThe Technical Debt Quadrant: Categorize your debt by the human approach taken at the moment.Interest Probability Framework: Analyzing debt based on its effect over time, effort to solve it, and how soon that effect will impact.Use debt to drive business value.If you enjoyed the content, hit the like &#10084;&#65039; button, share, comment, repost, and all those nice things people do when like stuff these days. Glad to know you made it to this part!Hi, I am Alejandro Aboy. I am currently working as a Data Engineer. I started in digital marketing at 19. I gained experience in website tracking, advertising, and analytics. I also founded my agency. In 2021, I found my passion for data engineering. So, I shifted my career focus, despite lacking a CS degree. I'm now pursuing this path, leveraging my diverse experience and willingness to learn.Follow me on Linkedin"}]}
{"messages": [{"role": "user", "content": "Create a guide on how to evaluate technical debt when merging code changes in collaborative projects."}, {"role": "assistant", "content": "The thing is: everything you create is somehow debt. You must consider the type and its impact before merging that PR or publishing that dashboard's custom logic.You could make mistakes, improve stuff or take decisions based on complex scenarios. &#8220;Debt&#8221; is something that should be taken with responsibilty because it&#8217;s your legacy. I will describe some experience-based scenarios and list some actions to address them the best we can.&#128288; There are 2 sides of technical debtYou can approach the problem from two perspectives:New in the job:You start reading docs, the codebase, checking dashboards, and getting to know stakeholders.Probably not everything will &#8220;click&#8221; in the first months.There are many things that are not intuitive.Old in the job:You have been working for some time or leaving a job in the short term.You have introduced many changes: improvements,\"bandages\" or \"I will fix this later.\" logics.You've worked in a context that pushed you to make risky or cautious decisions.Based on these scenarios, we will break down from different angles.Some &#8220;art&#8221; to represent the situation.Thanks for reading The Pipe & The Line! Subscribe to get new content and learnings for your data career!&#128064; When analyzing debt, what are the relevant angles?The ContextYou should set a criterion to judge the reasons for decisions. I see four variables that affected them:ResourcesHeadcountGrowth expectationsCulture&#129297; ResourcesHaving too many resources to spend. (e.g.) All your services are working in Cloud, nothing open-source or self-managed.Trying to cut as much as possible. (e.g.) Fighting against maintenance issues to avoid going with a Cloud solution. &#129730; HeadcountHaving 200 more employees. (e.g.) The BI tool has 100 reports, but only 3 are being used. Having 50 employees after rounds of layoffs. (e.g.) Abandoned documentation or too manyassets to maintain.&#128640; Growth expectationsThinking growth will scale 10 times the actual growth they ended up having. (e.g.) Building custom logic to handle more customers that never came. &#129395; CultureDifferent management with a different mindset. More conservative, more aggressive, more risky, you name it.Also a culture that encourages knowledge base camps and recurrent documentation might make a good differenceLast but not least, different technical profiles:Having many techy people, usually results in many pieces all over the place.Having weak or strong technical leads usually results in different types of debt.Having recycled profiles that learned through doing without mentorship.An example of the reality many companies are facing. has summarized this situation with great empathy:Thanks for reading The Pipe & The Line! This post is public so feel free to share it.Share&#128182; Technical debt analysis: it&#8217;s time to pay the interestSome frameworks can help you classify debt by analyzing logics.The Technical Debt Quadrant: You could be making decisions and accepting the consequences. Or, you might lack the experience to make good choices.You could be making decisions and taking precautions, you could try to cover for bad outcomes. Or, you could just go ahead without much thought.You group decisions based on Reckless-Prudent and Deliberate-Inadvertent. Inspired by: https://martinfowler.com/bliki/TechnicalDebtQuadrant.htmlInterest Probability Framework (Managing Technical Debt - Book):It tries to predict if the debt will cause future problems, also seeks to know how painful and soon those effects will be. With all these ideas in mind, you&#8217;ll know what&#8217;s the best course of action.You have 3 main concepts within this framework:Interest accrual: how the \"interest\" increases over time. Same as a bank loan, where the longer you wait to pay it off, the more you owe. Technical debt becomes more expensive to address the longer it remains unfixed.Interest amount: effort required to address the issue if it causes problems. This could be measured in time, resources, or even lost opportunities. It tends to grow over time. Time sensitivity: how quickly the interest amount increases if the technical debt is not addressed. Some types of technical debt are more time-sensitive than others, meaning the cost of not fixing them grows rapidly.Visualizing.Thanks for reading The Pipe & The Line! Subscribe for free to receive new posts and support my work.Example: Outdated Python CodeImagine you have an old data pipeline. It still works, but is hard to maintain due to outdated libraries and code.Interest Accrual: Delaying the upgrade may cause new issues. The old libraries may become unsupported or incompatible with newer systems. Interest Amount: The effort to fix these issues and upgrade might grow. More dependencies may break, and new features may be harder to integrate.Time sensitivity: If those packages version are deprecated soon or added packages are incompatible, it will speed up the technical debt effect. This is purely experimental, but it works for me; you can adapt as much as you like. This combined approach can help self-evaluate the technical debt:Template to analyze tech debt.\"Increasing caching frequency from every 30 minutes to every minute&#8221; is Prudent/Deliberate because it was a decision taken with predictability in mind. So it can buy some time until new customers come and more optimization can be added.&#8220;Added an incremental logic, but schema changes will break the workflow&#8221; is Prudent/Inadvertent because the engineer added that logic without considering the impact of not handling schema changes, and no predictable outcome could be expected from it. The Interest Probability Framework helps you find the scope of the technical debt effect. The Quadrant category helps you take the right actions.Acting on a decision made without enough expertise is not the same as acting on one made under team pressure.Sometimes you can tweak the analysis for a different angle.For example, in this article, and break down technical debt to positively impact business value by thinking into:Faster time to market. Improved customer experience. Greater team efficiency. Engineering LeadershipEngineer&#8217;s guide to convincing your Product Manager to prioritize technical debtRead more10 months ago &#183; 60 likes &#183; 12 comments &#183; Gregor Ojstersek and Robert TaIn the next post I will be writing about how to help the ones from the future: not only your replacements or backups, but also yourself. &#128221; TL;DR&#128288; There are 2 sides of technical debtBe empathetic when joining a new companyLeave the best legacy while you work at the company&#128064; When analyzing debt, what are the relevant angles?&#129297; Resources: How the availability of money impacts decisions.&#129730; Headcount: How the amount of employees influences actions.&#128640; Growth expectations: How growth defines tech projects.&#129395; Culture: How leaders' mindsets affect contributors.&#128182; Technical debt analysis: it&#8217;s time to pay the interestThe Technical Debt Quadrant: Categorize your debt by the human approach taken at the moment.Interest Probability Framework: Analyzing debt based on its effect over time, effort to solve it, and how soon that effect will impact.Use debt to drive business value.If you enjoyed the content, hit the like &#10084;&#65039; button, share, comment, repost, and all those nice things people do when like stuff these days. Glad to know you made it to this part!Hi, I am Alejandro Aboy. I am currently working as a Data Engineer. I started in digital marketing at 19. I gained experience in website tracking, advertising, and analytics. I also founded my agency. In 2021, I found my passion for data engineering. So, I shifted my career focus, despite lacking a CS degree. I'm now pursuing this path, leveraging my diverse experience and willingness to learn.Follow me on Linkedin"}]}
{"messages": [{"role": "user", "content": "Share career insights on how to communicate technical debt to stakeholders effectively and advocate for improvements."}, {"role": "assistant", "content": "The thing is: everything you create is somehow debt. You must consider the type and its impact before merging that PR or publishing that dashboard's custom logic.You could make mistakes, improve stuff or take decisions based on complex scenarios. &#8220;Debt&#8221; is something that should be taken with responsibilty because it&#8217;s your legacy. I will describe some experience-based scenarios and list some actions to address them the best we can.&#128288; There are 2 sides of technical debtYou can approach the problem from two perspectives:New in the job:You start reading docs, the codebase, checking dashboards, and getting to know stakeholders.Probably not everything will &#8220;click&#8221; in the first months.There are many things that are not intuitive.Old in the job:You have been working for some time or leaving a job in the short term.You have introduced many changes: improvements,\"bandages\" or \"I will fix this later.\" logics.You've worked in a context that pushed you to make risky or cautious decisions.Based on these scenarios, we will break down from different angles.Some &#8220;art&#8221; to represent the situation.Thanks for reading The Pipe & The Line! Subscribe to get new content and learnings for your data career!&#128064; When analyzing debt, what are the relevant angles?The ContextYou should set a criterion to judge the reasons for decisions. I see four variables that affected them:ResourcesHeadcountGrowth expectationsCulture&#129297; ResourcesHaving too many resources to spend. (e.g.) All your services are working in Cloud, nothing open-source or self-managed.Trying to cut as much as possible. (e.g.) Fighting against maintenance issues to avoid going with a Cloud solution. &#129730; HeadcountHaving 200 more employees. (e.g.) The BI tool has 100 reports, but only 3 are being used. Having 50 employees after rounds of layoffs. (e.g.) Abandoned documentation or too manyassets to maintain.&#128640; Growth expectationsThinking growth will scale 10 times the actual growth they ended up having. (e.g.) Building custom logic to handle more customers that never came. &#129395; CultureDifferent management with a different mindset. More conservative, more aggressive, more risky, you name it.Also a culture that encourages knowledge base camps and recurrent documentation might make a good differenceLast but not least, different technical profiles:Having many techy people, usually results in many pieces all over the place.Having weak or strong technical leads usually results in different types of debt.Having recycled profiles that learned through doing without mentorship.An example of the reality many companies are facing. has summarized this situation with great empathy:Thanks for reading The Pipe & The Line! This post is public so feel free to share it.Share&#128182; Technical debt analysis: it&#8217;s time to pay the interestSome frameworks can help you classify debt by analyzing logics.The Technical Debt Quadrant: You could be making decisions and accepting the consequences. Or, you might lack the experience to make good choices.You could be making decisions and taking precautions, you could try to cover for bad outcomes. Or, you could just go ahead without much thought.You group decisions based on Reckless-Prudent and Deliberate-Inadvertent. Inspired by: https://martinfowler.com/bliki/TechnicalDebtQuadrant.htmlInterest Probability Framework (Managing Technical Debt - Book):It tries to predict if the debt will cause future problems, also seeks to know how painful and soon those effects will be. With all these ideas in mind, you&#8217;ll know what&#8217;s the best course of action.You have 3 main concepts within this framework:Interest accrual: how the \"interest\" increases over time. Same as a bank loan, where the longer you wait to pay it off, the more you owe. Technical debt becomes more expensive to address the longer it remains unfixed.Interest amount: effort required to address the issue if it causes problems. This could be measured in time, resources, or even lost opportunities. It tends to grow over time. Time sensitivity: how quickly the interest amount increases if the technical debt is not addressed. Some types of technical debt are more time-sensitive than others, meaning the cost of not fixing them grows rapidly.Visualizing.Thanks for reading The Pipe & The Line! Subscribe for free to receive new posts and support my work.Example: Outdated Python CodeImagine you have an old data pipeline. It still works, but is hard to maintain due to outdated libraries and code.Interest Accrual: Delaying the upgrade may cause new issues. The old libraries may become unsupported or incompatible with newer systems. Interest Amount: The effort to fix these issues and upgrade might grow. More dependencies may break, and new features may be harder to integrate.Time sensitivity: If those packages version are deprecated soon or added packages are incompatible, it will speed up the technical debt effect. This is purely experimental, but it works for me; you can adapt as much as you like. This combined approach can help self-evaluate the technical debt:Template to analyze tech debt.\"Increasing caching frequency from every 30 minutes to every minute&#8221; is Prudent/Deliberate because it was a decision taken with predictability in mind. So it can buy some time until new customers come and more optimization can be added.&#8220;Added an incremental logic, but schema changes will break the workflow&#8221; is Prudent/Inadvertent because the engineer added that logic without considering the impact of not handling schema changes, and no predictable outcome could be expected from it. The Interest Probability Framework helps you find the scope of the technical debt effect. The Quadrant category helps you take the right actions.Acting on a decision made without enough expertise is not the same as acting on one made under team pressure.Sometimes you can tweak the analysis for a different angle.For example, in this article, and break down technical debt to positively impact business value by thinking into:Faster time to market. Improved customer experience. Greater team efficiency. Engineering LeadershipEngineer&#8217;s guide to convincing your Product Manager to prioritize technical debtRead more10 months ago &#183; 60 likes &#183; 12 comments &#183; Gregor Ojstersek and Robert TaIn the next post I will be writing about how to help the ones from the future: not only your replacements or backups, but also yourself. &#128221; TL;DR&#128288; There are 2 sides of technical debtBe empathetic when joining a new companyLeave the best legacy while you work at the company&#128064; When analyzing debt, what are the relevant angles?&#129297; Resources: How the availability of money impacts decisions.&#129730; Headcount: How the amount of employees influences actions.&#128640; Growth expectations: How growth defines tech projects.&#129395; Culture: How leaders' mindsets affect contributors.&#128182; Technical debt analysis: it&#8217;s time to pay the interestThe Technical Debt Quadrant: Categorize your debt by the human approach taken at the moment.Interest Probability Framework: Analyzing debt based on its effect over time, effort to solve it, and how soon that effect will impact.Use debt to drive business value.If you enjoyed the content, hit the like &#10084;&#65039; button, share, comment, repost, and all those nice things people do when like stuff these days. Glad to know you made it to this part!Hi, I am Alejandro Aboy. I am currently working as a Data Engineer. I started in digital marketing at 19. I gained experience in website tracking, advertising, and analytics. I also founded my agency. In 2021, I found my passion for data engineering. So, I shifted my career focus, despite lacking a CS degree. I'm now pursuing this path, leveraging my diverse experience and willingness to learn.Follow me on Linkedin"}]}
{"messages": [{"role": "user", "content": "Explain how to create a custom data generator for testing machine learning algorithms effectively."}, {"role": "assistant", "content": "When you begin working in data, you will likely come across Kaggle.Kaggle is fine. It's the entry point for most of us, especially when learning to be a Data Analyst or Scientist.But, at some point, you memorize datasets. Or, you want to work with certain, dynamic data points.That&#8217;s why I started creating the V1 of this repo. Github Repository:&#128202;&#129302; Fake Data Generator for your demo projectsGraphic view of the repo.This repo might come in handy if:Your machine learning models might need more data points to improve their scores.Your data engineering projects might more realistic workloads to test their robustness.Your analytics workflows at work.Even if you work in frontend or backend and you need more realistic data to test your applications.You want to benchmark some tools, like DuckDB or Polars. The typical 1000-row dataset isn't giving the results you expect.&#128221; Use cases:The documentation shows that this repo provides a DuckDB instance to run queries. You can also export the .csv files to:Process data with Pandas, Polars or pySpark.Practice Data Viz with Matplotlib, Seaborn, plotly or Dash. Train your ML models.Practice ETL skills with Orchestrator using simulated datasets.Practice GDPR compliant techniques for data anonimization. &#128694;&#127995; Quick WalkthroughTo get started, you can generate 1000, 10000, 50000, 100000 rows per each table. You know your machine resources, you know how far you can push. Also, if you like the .csv files to be available for further processing, you can run:make csvsLet me walk you through it: Generate users and product dataThanks for reading The Pipe & The Line! Subscribe to get new content and learnings for your data career!The next function expects products and user IDs to create a relational output. This will be pretty useful for the DuckDB section, or even if you want to use the .csv files with PandasYou will spot this 2 lines getting the corresponding IDs: ... user_ids = [user[\"user_id\"] for user in user_profiles] ... product_ids = [product[\"product_id\"] for product in products]The good thing about this code is its flexibility. You can add new features, like extra dimensions, varied price ranges, or even a dataset for tracking product IDs (SKUs). The possibilities are endless!A final view of the dataset ERD (Entity Relationship Diagram).&#128064; How this repo can be improved?As mentioned, opportunities are limitless:Add nulls to practice data cleansing.Add outliers to practice data normalization techniquies.Add JSON-based columns to practice more complex data transformations.Add a method to send the .csv to a S3 bucket to be ingested by another tool. I will try adding new use cases from time to time! Hope you liked it! If you enjoyed the content, hit the like &#10084;&#65039; button, share, comment, repost, and all those nice things people do when like stuff these days. Glad to know you made it to this part!Hi, I am Alejandro Aboy. I am currently working as a Data Engineer. I started in digital marketing at 19. I gained experience in website tracking, advertising, and analytics. I also founded my agency. In 2021, I found my passion for data engineering. So, I shifted my career focus, despite lacking a CS degree. I'm now pursuing this path, leveraging my diverse experience and willingness to learn.Follow me on Linkedin"}]}
{"messages": [{"role": "user", "content": "Outline the benefits of using synthetic data in real-world machine learning projects for career advancement."}, {"role": "assistant", "content": "When you begin working in data, you will likely come across Kaggle.Kaggle is fine. It's the entry point for most of us, especially when learning to be a Data Analyst or Scientist.But, at some point, you memorize datasets. Or, you want to work with certain, dynamic data points.That&#8217;s why I started creating the V1 of this repo. Github Repository:&#128202;&#129302; Fake Data Generator for your demo projectsGraphic view of the repo.This repo might come in handy if:Your machine learning models might need more data points to improve their scores.Your data engineering projects might more realistic workloads to test their robustness.Your analytics workflows at work.Even if you work in frontend or backend and you need more realistic data to test your applications.You want to benchmark some tools, like DuckDB or Polars. The typical 1000-row dataset isn't giving the results you expect.&#128221; Use cases:The documentation shows that this repo provides a DuckDB instance to run queries. You can also export the .csv files to:Process data with Pandas, Polars or pySpark.Practice Data Viz with Matplotlib, Seaborn, plotly or Dash. Train your ML models.Practice ETL skills with Orchestrator using simulated datasets.Practice GDPR compliant techniques for data anonimization. &#128694;&#127995; Quick WalkthroughTo get started, you can generate 1000, 10000, 50000, 100000 rows per each table. You know your machine resources, you know how far you can push. Also, if you like the .csv files to be available for further processing, you can run:make csvsLet me walk you through it: Generate users and product dataThanks for reading The Pipe & The Line! Subscribe to get new content and learnings for your data career!The next function expects products and user IDs to create a relational output. This will be pretty useful for the DuckDB section, or even if you want to use the .csv files with PandasYou will spot this 2 lines getting the corresponding IDs: ... user_ids = [user[\"user_id\"] for user in user_profiles] ... product_ids = [product[\"product_id\"] for product in products]The good thing about this code is its flexibility. You can add new features, like extra dimensions, varied price ranges, or even a dataset for tracking product IDs (SKUs). The possibilities are endless!A final view of the dataset ERD (Entity Relationship Diagram).&#128064; How this repo can be improved?As mentioned, opportunities are limitless:Add nulls to practice data cleansing.Add outliers to practice data normalization techniquies.Add JSON-based columns to practice more complex data transformations.Add a method to send the .csv to a S3 bucket to be ingested by another tool. I will try adding new use cases from time to time! Hope you liked it! If you enjoyed the content, hit the like &#10084;&#65039; button, share, comment, repost, and all those nice things people do when like stuff these days. Glad to know you made it to this part!Hi, I am Alejandro Aboy. I am currently working as a Data Engineer. I started in digital marketing at 19. I gained experience in website tracking, advertising, and analytics. I also founded my agency. In 2021, I found my passion for data engineering. So, I shifted my career focus, despite lacking a CS degree. I'm now pursuing this path, leveraging my diverse experience and willingness to learn.Follow me on Linkedin"}]}
{"messages": [{"role": "user", "content": "Provide a step-by-step guide on integrating a fake data generator into a data pipeline for demos."}, {"role": "assistant", "content": "When you begin working in data, you will likely come across Kaggle.Kaggle is fine. It's the entry point for most of us, especially when learning to be a Data Analyst or Scientist.But, at some point, you memorize datasets. Or, you want to work with certain, dynamic data points.That&#8217;s why I started creating the V1 of this repo. Github Repository:&#128202;&#129302; Fake Data Generator for your demo projectsGraphic view of the repo.This repo might come in handy if:Your machine learning models might need more data points to improve their scores.Your data engineering projects might more realistic workloads to test their robustness.Your analytics workflows at work.Even if you work in frontend or backend and you need more realistic data to test your applications.You want to benchmark some tools, like DuckDB or Polars. The typical 1000-row dataset isn't giving the results you expect.&#128221; Use cases:The documentation shows that this repo provides a DuckDB instance to run queries. You can also export the .csv files to:Process data with Pandas, Polars or pySpark.Practice Data Viz with Matplotlib, Seaborn, plotly or Dash. Train your ML models.Practice ETL skills with Orchestrator using simulated datasets.Practice GDPR compliant techniques for data anonimization. &#128694;&#127995; Quick WalkthroughTo get started, you can generate 1000, 10000, 50000, 100000 rows per each table. You know your machine resources, you know how far you can push. Also, if you like the .csv files to be available for further processing, you can run:make csvsLet me walk you through it: Generate users and product dataThanks for reading The Pipe & The Line! Subscribe to get new content and learnings for your data career!The next function expects products and user IDs to create a relational output. This will be pretty useful for the DuckDB section, or even if you want to use the .csv files with PandasYou will spot this 2 lines getting the corresponding IDs: ... user_ids = [user[\"user_id\"] for user in user_profiles] ... product_ids = [product[\"product_id\"] for product in products]The good thing about this code is its flexibility. You can add new features, like extra dimensions, varied price ranges, or even a dataset for tracking product IDs (SKUs). The possibilities are endless!A final view of the dataset ERD (Entity Relationship Diagram).&#128064; How this repo can be improved?As mentioned, opportunities are limitless:Add nulls to practice data cleansing.Add outliers to practice data normalization techniquies.Add JSON-based columns to practice more complex data transformations.Add a method to send the .csv to a S3 bucket to be ingested by another tool. I will try adding new use cases from time to time! Hope you liked it! If you enjoyed the content, hit the like &#10084;&#65039; button, share, comment, repost, and all those nice things people do when like stuff these days. Glad to know you made it to this part!Hi, I am Alejandro Aboy. I am currently working as a Data Engineer. I started in digital marketing at 19. I gained experience in website tracking, advertising, and analytics. I also founded my agency. In 2021, I found my passion for data engineering. So, I shifted my career focus, despite lacking a CS degree. I'm now pursuing this path, leveraging my diverse experience and willingness to learn.Follow me on Linkedin"}]}
{"messages": [{"role": "user", "content": "Explain a complex data engineering concept in simple terms without jargon, focusing on practical applications."}, {"role": "assistant", "content": "If you work in the data space, mainly as a Data Engineer, you hear about CDC everywhere.When you research on the topic, you are likely to find explanation like this one:CDC is a DMT that facilitates real-time replication by ensuring ACID and identifying DML operations on OLTP systems. It leverages RDBMS transaction logs or delta comparisons to synchronize downstream DWs, BI tools, and MDM frameworks in real-time or near real-time. Even for Emimem, that&#8217;s too much vocabulary. We will try understanding what&#8217;s CDC without the need of a dictionary or robotic-to-human translator. That&#8217;s why I am writing the type of blog post I would have liked to find when I was starting my research. &#128288; What&#8217;s the CDC definition?CDC is software that captures changes in a Source Database and sends those to a Target Database.If you look up more definitions, some people in the data space like over-hyping the concept and they end up looking like this:Meme Service at your disposal.But remember to keep it simple: CDC is a software that captures changes in a Source Database and sends those into a Target Database.Don&#8217;t confuse this with ETL (Extract, Transform, Load). ETL/ELT is a 3-step process in which you extract, load, and transform data. If we allow some flexibility in definitions, we can say that, due to its functions, CDC covers the EL part of ETL/ELT.You usually do batch processing instead of CDC when doing ETL, the main differences being:ETL can load every day, every hour, every week, but not in real-time.ETL can load incrementally or full reloads, but you are not change-sensitive.ETL can load great workloads, CDC is catching tiny bits of information.Think of it as a mirror.Thanks for reading The Pipe & The Line! Subscribe to get new content and learnings for your data career!&#128064; When is it relevant to use CDC?There are tons of use-cases, but the most usual ones are:You have a product/backend database. You don't want to overload it with Analytics queries.You have fraud detection algorithms that need real-time data to be available. You want to fire alerts based on real-time platform actions. Here&#8217;s a graphic representation of with and without CDC:All the pressure on one DB.Redistributing pressure into 2 DBs.&#127941; Main concepts you need to understand CDCForget about the fancy keywords I was mentioning. Here&#8217;s the list of the relevant things:Source & Target DatabaseChangesCapture Type&#127919; Source & Target DatabaseThe WHO of the data replication process.They are also known as:Publisher & SubscriberProducer & Consumer (Kafka influence)Source Node & Target NodePush & PullBut they are all the same; don&#8217;t worry!Think of it as a conversation: Bad joke, please keep reading &#129315;Jokes aside, there is one part catching and sending, while the other part is receiving. All these actions end up with them being exactly the same. &#128258; ChangesThe WHAT of the data replication process.First, consider the name: Change Data Capture. It means capturing data changes.Change: something was A and now is B. What&#8217;s a data change?UPDATEINSERTDELETEThese operations alter the data stored in database tables.Compatible operations for CDC.What&#8217;s not a data change?CREATE TABLEALTER TABLETRUNCATE TABLECDC captures changes in the data itself (the contents of the rows in a table), not changes to the database schema or structure.TRUNCATE TABLE is a special case despite it deletes all rows in a table. It's treated as a bulk operation that resets the table's data state, not as a series of individual DELETE operations.Incompatible operations for CDC.&#129708; Capture TypeThe HOW of the data replication process. The most common types are the following:Query-based (a.k.a Audit Columns): You can look for changes by querying &#8220;LAST_UPDATED&#8221; or &#8220;DATE_MODIFIED&#8221; columns. You will get the new rows after those greatest values.Unless you keep track of deletions, this process will ignore them.Trigger-based: Every time you update, insert, or delete a row, you trigger a replication workflow.You will hurt your database's performance by making it slower.Log-based (a.k.a Transaction Log, WAL, binlog, redo log, etc):Databases save logs of the recorded actions for recovery purposes in case of any issue. Reading those logs doesn't involve scanning the database for changes. So, performance remains stable. However, operations like ALTER or TRUNCATE aren't recorded in the CDC logs.For example, in PostgreSQL we have WAL (Write-Ahead-Logs). WAL helps for backups, recovering from an event, streaming replication, among other things. It will insert an identified known as LSN (Log Sequence Number), which is saving the position of the replication process. Just like the checkpoint save when playing games. Credits to https://www.crunchydata.com/blog/postgres-wal-files-and-sequuence-numbersThanks for reading The Pipe & The Line! Subscribe to get new content and learnings for your data career!&#129683; What are the tools to implement CDC?There are tons of tools. Cloud based, open source, database extensions and many more. To list some of them:Cloud BasedEstuaryFivetranAirbyte (also open source)AWS Database Migration ServiceOpen Source & Database ExtensionsDebezium with Kafkapglogical plugin (PostgreSQL)MySQL triggersI will try to cover some PostgreSQL use cases in upcoming posts. &#128221; TL;DR&#128288; What&#8217;s CDC definition? CDC is a software that captures changes in a Source Database and sends those into a Target Database.&#128064; When is relevant to use CDC?Relief one database from taking too much workload or enabling real-time data for machine learning use-cases. &#127941; Main concepts you need to understand CDC&#127919; Source & Target Database: the publisher and the susbcriber&#128258; Changes: INSERT, UPDATE, DELETE&#129708; Capture Type: Log, Trigger, Query Based&#129683; What are the tools to implement CDC?Cloud: Estuary, Fivetran, Airbyte, AWS DMSOpen Source: Debezium, pglogical, MySQL triggersIf you enjoyed the content, hit the like &#10084;&#65039; button, share, comment, repost, and all those nice things people do when like stuff these days. Glad to know you made it to this part!Hi, I am Alejandro Aboy. I am currently working as a Data Engineer. I started in digital marketing at 19. I gained experience in website tracking, advertising, and analytics. I also founded my agency. In 2021, I found my passion for data engineering. So, I shifted my career focus, despite lacking a CS degree. I'm now pursuing this path, leveraging my diverse experience and willingness to learn.Follow me on Linkedin"}]}
{"messages": [{"role": "user", "content": "Write a guide for data engineers on how to implement Change Data Capture in real-world projects."}, {"role": "assistant", "content": "If you work in the data space, mainly as a Data Engineer, you hear about CDC everywhere.When you research on the topic, you are likely to find explanation like this one:CDC is a DMT that facilitates real-time replication by ensuring ACID and identifying DML operations on OLTP systems. It leverages RDBMS transaction logs or delta comparisons to synchronize downstream DWs, BI tools, and MDM frameworks in real-time or near real-time. Even for Emimem, that&#8217;s too much vocabulary. We will try understanding what&#8217;s CDC without the need of a dictionary or robotic-to-human translator. That&#8217;s why I am writing the type of blog post I would have liked to find when I was starting my research. &#128288; What&#8217;s the CDC definition?CDC is software that captures changes in a Source Database and sends those to a Target Database.If you look up more definitions, some people in the data space like over-hyping the concept and they end up looking like this:Meme Service at your disposal.But remember to keep it simple: CDC is a software that captures changes in a Source Database and sends those into a Target Database.Don&#8217;t confuse this with ETL (Extract, Transform, Load). ETL/ELT is a 3-step process in which you extract, load, and transform data. If we allow some flexibility in definitions, we can say that, due to its functions, CDC covers the EL part of ETL/ELT.You usually do batch processing instead of CDC when doing ETL, the main differences being:ETL can load every day, every hour, every week, but not in real-time.ETL can load incrementally or full reloads, but you are not change-sensitive.ETL can load great workloads, CDC is catching tiny bits of information.Think of it as a mirror.Thanks for reading The Pipe & The Line! Subscribe to get new content and learnings for your data career!&#128064; When is it relevant to use CDC?There are tons of use-cases, but the most usual ones are:You have a product/backend database. You don't want to overload it with Analytics queries.You have fraud detection algorithms that need real-time data to be available. You want to fire alerts based on real-time platform actions. Here&#8217;s a graphic representation of with and without CDC:All the pressure on one DB.Redistributing pressure into 2 DBs.&#127941; Main concepts you need to understand CDCForget about the fancy keywords I was mentioning. Here&#8217;s the list of the relevant things:Source & Target DatabaseChangesCapture Type&#127919; Source & Target DatabaseThe WHO of the data replication process.They are also known as:Publisher & SubscriberProducer & Consumer (Kafka influence)Source Node & Target NodePush & PullBut they are all the same; don&#8217;t worry!Think of it as a conversation: Bad joke, please keep reading &#129315;Jokes aside, there is one part catching and sending, while the other part is receiving. All these actions end up with them being exactly the same. &#128258; ChangesThe WHAT of the data replication process.First, consider the name: Change Data Capture. It means capturing data changes.Change: something was A and now is B. What&#8217;s a data change?UPDATEINSERTDELETEThese operations alter the data stored in database tables.Compatible operations for CDC.What&#8217;s not a data change?CREATE TABLEALTER TABLETRUNCATE TABLECDC captures changes in the data itself (the contents of the rows in a table), not changes to the database schema or structure.TRUNCATE TABLE is a special case despite it deletes all rows in a table. It's treated as a bulk operation that resets the table's data state, not as a series of individual DELETE operations.Incompatible operations for CDC.&#129708; Capture TypeThe HOW of the data replication process. The most common types are the following:Query-based (a.k.a Audit Columns): You can look for changes by querying &#8220;LAST_UPDATED&#8221; or &#8220;DATE_MODIFIED&#8221; columns. You will get the new rows after those greatest values.Unless you keep track of deletions, this process will ignore them.Trigger-based: Every time you update, insert, or delete a row, you trigger a replication workflow.You will hurt your database's performance by making it slower.Log-based (a.k.a Transaction Log, WAL, binlog, redo log, etc):Databases save logs of the recorded actions for recovery purposes in case of any issue. Reading those logs doesn't involve scanning the database for changes. So, performance remains stable. However, operations like ALTER or TRUNCATE aren't recorded in the CDC logs.For example, in PostgreSQL we have WAL (Write-Ahead-Logs). WAL helps for backups, recovering from an event, streaming replication, among other things. It will insert an identified known as LSN (Log Sequence Number), which is saving the position of the replication process. Just like the checkpoint save when playing games. Credits to https://www.crunchydata.com/blog/postgres-wal-files-and-sequuence-numbersThanks for reading The Pipe & The Line! Subscribe to get new content and learnings for your data career!&#129683; What are the tools to implement CDC?There are tons of tools. Cloud based, open source, database extensions and many more. To list some of them:Cloud BasedEstuaryFivetranAirbyte (also open source)AWS Database Migration ServiceOpen Source & Database ExtensionsDebezium with Kafkapglogical plugin (PostgreSQL)MySQL triggersI will try to cover some PostgreSQL use cases in upcoming posts. &#128221; TL;DR&#128288; What&#8217;s CDC definition? CDC is a software that captures changes in a Source Database and sends those into a Target Database.&#128064; When is relevant to use CDC?Relief one database from taking too much workload or enabling real-time data for machine learning use-cases. &#127941; Main concepts you need to understand CDC&#127919; Source & Target Database: the publisher and the susbcriber&#128258; Changes: INSERT, UPDATE, DELETE&#129708; Capture Type: Log, Trigger, Query Based&#129683; What are the tools to implement CDC?Cloud: Estuary, Fivetran, Airbyte, AWS DMSOpen Source: Debezium, pglogical, MySQL triggersIf you enjoyed the content, hit the like &#10084;&#65039; button, share, comment, repost, and all those nice things people do when like stuff these days. Glad to know you made it to this part!Hi, I am Alejandro Aboy. I am currently working as a Data Engineer. I started in digital marketing at 19. I gained experience in website tracking, advertising, and analytics. I also founded my agency. In 2021, I found my passion for data engineering. So, I shifted my career focus, despite lacking a CS degree. I'm now pursuing this path, leveraging my diverse experience and willingness to learn.Follow me on Linkedin"}]}
{"messages": [{"role": "user", "content": "Share career tips for aspiring data engineers, emphasizing skills needed to understand and utilize CDC effectively."}, {"role": "assistant", "content": "If you work in the data space, mainly as a Data Engineer, you hear about CDC everywhere.When you research on the topic, you are likely to find explanation like this one:CDC is a DMT that facilitates real-time replication by ensuring ACID and identifying DML operations on OLTP systems. It leverages RDBMS transaction logs or delta comparisons to synchronize downstream DWs, BI tools, and MDM frameworks in real-time or near real-time. Even for Emimem, that&#8217;s too much vocabulary. We will try understanding what&#8217;s CDC without the need of a dictionary or robotic-to-human translator. That&#8217;s why I am writing the type of blog post I would have liked to find when I was starting my research. &#128288; What&#8217;s the CDC definition?CDC is software that captures changes in a Source Database and sends those to a Target Database.If you look up more definitions, some people in the data space like over-hyping the concept and they end up looking like this:Meme Service at your disposal.But remember to keep it simple: CDC is a software that captures changes in a Source Database and sends those into a Target Database.Don&#8217;t confuse this with ETL (Extract, Transform, Load). ETL/ELT is a 3-step process in which you extract, load, and transform data. If we allow some flexibility in definitions, we can say that, due to its functions, CDC covers the EL part of ETL/ELT.You usually do batch processing instead of CDC when doing ETL, the main differences being:ETL can load every day, every hour, every week, but not in real-time.ETL can load incrementally or full reloads, but you are not change-sensitive.ETL can load great workloads, CDC is catching tiny bits of information.Think of it as a mirror.Thanks for reading The Pipe & The Line! Subscribe to get new content and learnings for your data career!&#128064; When is it relevant to use CDC?There are tons of use-cases, but the most usual ones are:You have a product/backend database. You don't want to overload it with Analytics queries.You have fraud detection algorithms that need real-time data to be available. You want to fire alerts based on real-time platform actions. Here&#8217;s a graphic representation of with and without CDC:All the pressure on one DB.Redistributing pressure into 2 DBs.&#127941; Main concepts you need to understand CDCForget about the fancy keywords I was mentioning. Here&#8217;s the list of the relevant things:Source & Target DatabaseChangesCapture Type&#127919; Source & Target DatabaseThe WHO of the data replication process.They are also known as:Publisher & SubscriberProducer & Consumer (Kafka influence)Source Node & Target NodePush & PullBut they are all the same; don&#8217;t worry!Think of it as a conversation: Bad joke, please keep reading &#129315;Jokes aside, there is one part catching and sending, while the other part is receiving. All these actions end up with them being exactly the same. &#128258; ChangesThe WHAT of the data replication process.First, consider the name: Change Data Capture. It means capturing data changes.Change: something was A and now is B. What&#8217;s a data change?UPDATEINSERTDELETEThese operations alter the data stored in database tables.Compatible operations for CDC.What&#8217;s not a data change?CREATE TABLEALTER TABLETRUNCATE TABLECDC captures changes in the data itself (the contents of the rows in a table), not changes to the database schema or structure.TRUNCATE TABLE is a special case despite it deletes all rows in a table. It's treated as a bulk operation that resets the table's data state, not as a series of individual DELETE operations.Incompatible operations for CDC.&#129708; Capture TypeThe HOW of the data replication process. The most common types are the following:Query-based (a.k.a Audit Columns): You can look for changes by querying &#8220;LAST_UPDATED&#8221; or &#8220;DATE_MODIFIED&#8221; columns. You will get the new rows after those greatest values.Unless you keep track of deletions, this process will ignore them.Trigger-based: Every time you update, insert, or delete a row, you trigger a replication workflow.You will hurt your database's performance by making it slower.Log-based (a.k.a Transaction Log, WAL, binlog, redo log, etc):Databases save logs of the recorded actions for recovery purposes in case of any issue. Reading those logs doesn't involve scanning the database for changes. So, performance remains stable. However, operations like ALTER or TRUNCATE aren't recorded in the CDC logs.For example, in PostgreSQL we have WAL (Write-Ahead-Logs). WAL helps for backups, recovering from an event, streaming replication, among other things. It will insert an identified known as LSN (Log Sequence Number), which is saving the position of the replication process. Just like the checkpoint save when playing games. Credits to https://www.crunchydata.com/blog/postgres-wal-files-and-sequuence-numbersThanks for reading The Pipe & The Line! Subscribe to get new content and learnings for your data career!&#129683; What are the tools to implement CDC?There are tons of tools. Cloud based, open source, database extensions and many more. To list some of them:Cloud BasedEstuaryFivetranAirbyte (also open source)AWS Database Migration ServiceOpen Source & Database ExtensionsDebezium with Kafkapglogical plugin (PostgreSQL)MySQL triggersI will try to cover some PostgreSQL use cases in upcoming posts. &#128221; TL;DR&#128288; What&#8217;s CDC definition? CDC is a software that captures changes in a Source Database and sends those into a Target Database.&#128064; When is relevant to use CDC?Relief one database from taking too much workload or enabling real-time data for machine learning use-cases. &#127941; Main concepts you need to understand CDC&#127919; Source & Target Database: the publisher and the susbcriber&#128258; Changes: INSERT, UPDATE, DELETE&#129708; Capture Type: Log, Trigger, Query Based&#129683; What are the tools to implement CDC?Cloud: Estuary, Fivetran, Airbyte, AWS DMSOpen Source: Debezium, pglogical, MySQL triggersIf you enjoyed the content, hit the like &#10084;&#65039; button, share, comment, repost, and all those nice things people do when like stuff these days. Glad to know you made it to this part!Hi, I am Alejandro Aboy. I am currently working as a Data Engineer. I started in digital marketing at 19. I gained experience in website tracking, advertising, and analytics. I also founded my agency. In 2021, I found my passion for data engineering. So, I shifted my career focus, despite lacking a CS degree. I'm now pursuing this path, leveraging my diverse experience and willingness to learn.Follow me on Linkedin"}]}
{"messages": [{"role": "user", "content": "Explain how to effectively communicate data needs and expectations with end users to enhance collaboration."}, {"role": "assistant", "content": "If you work in data, especially in a small team, you know the common fear. It's not due to a lack of experience or knowledge. We fear it because it leads to endless problems.But we shouldn&#8217;t fear it&#8230;There is hope at the end of the pipeline (see what I did there?). The usual advice is to \"have initiative, be proactive, and be one step ahead.\" but it's hard to follow because context defines 99% of it.You can find nice templates and frameworks, but simple questions for end users are better, in my experience.As a technical contributor, you might dislike talking with non-technical coworkers in your company. But you'll need to learn to speak non-technical at some point.If you ask them the right questions, they'll make better, fewer or even more qualified requests next time. Let&#8217;s get started. &#127916;Setting the scene I am assuming you will be talking with someone you don&#8217;t usually interact with. Even for small companies, it is likely that the other person will feel less open at the beginning and you will have to break the ice. My best pieces of advice for this process:Provide an agenda in advance and be straightforward about the main goal: you want to help THEM. &#8220;Hi End User, how are you doing?\"I've been planning my projects for next quarter. My backlog shows that many of your team's Metabase dashboards are loading slowly.I&#8217;ve booked a meeting for next week to chat about the dashboards you use, the metrics you track, and your usual business questions. We'll also discuss anything else that could help to improve your daily workflow.Thanks!&#8221;If you are a talker like me, this is not the moment to shine. Start the conversation but let it be to build trust. If possible, don't interrupt. Use an AI note taker or get consent to record the meeting. But pay attention. It's frustrating to discuss your issues while hearing keyboards in the background (even if muted). There is an exception: if you catch an opportunity that AI won&#8217;t, ask for a minute to write it down.Setting the scene steps.&#10067;Asking questions that will make a difference &#128197; What are your daily tasks?Most of the time, they don&#8217;t explicitly know what they need, and that is fine. If they don&#8217;t know, ask about their daily tasks. A guided tour through their work assets is sometimes enough. That will open new points for the next steps of the conversation.If you build dashboards, make them share their screens and show you.If you build automated emails with insights, ask them what they take from those.&#129354; What are your daily challenges? After the last step, ask what they cannot do because of their daily tasks. That will let you know what you could improve.There&#8217;s no need to go with the typical question &#8220;What keeps you up at night?&#8221;. These are my favorites:&#8220;What is taking up most of your time when doing X or Y?&#8221; &#8220;How do you proceed when X or Y happens?&#8221; &#8220;Is there one impactful thing you should be doing more, but something is pulling you down? If yes, what&#8217;s that thing?\"Thanks for reading The Pipe & The Line! Subscribe to get new content and learnings for your data career!&#128165; Let&#8217;s talk about painTry to understand what they are not saying. Most of the time, if someone complains about something, they are saying something else. For example:\"I have no time to check my customers one by one. It makes me unable to anticipate whether a customer will churn.\"I have no time; remove the manual task from them if possible.Check my customers: what's the criterion? Ask for them and develop something that checks it at scale.Unable to anticipate: are we supposed to do this before a specific event or time in the year? What's the ideal frequency?Whether a customer will churn: that&#8217;s a metric. Usually, all complaints are disguising a blocker, a metric, a target and the impact of the affected process. This might not apply 100% of the time, but paying attention to these concepts could help. &#128214; Were you aware of&#8230;? It's funny when someone complains about a missing thing that was available for over 6 months but never used.It's up to you, my data friend. We often don't share available assets with users but you should announce new data products. Also, regularly update others on improvements and maintenance.This will make a big difference, and the effort is quite low. Some actionable items for this point:Use clear names for assets. If everyone refers to it as &#8220;User Growth&#8221;, don't name the dashboard \"Account Activations\".Spend time writing good descriptions as if someone coming from Mars was going to read it, be detailed but don&#8217;t rewrite the bible.Use emojis to catch attention, never gets old. &#128587;&#127995;&#8205;&#9792;&#65039;&#128587;&#127995;&#8205;&#9794;&#65039; But never forget: if you communicate to provide awareness and they don&#8217;t pay attention, don&#8217;t insist. Whenever they need it, they will come back asking for it. &#128587;&#127995;&#8205;&#9794;&#65039; What do you need?Just like the heading title. Simple. But hold on, don&#8217;t get me wrong. This question usually works with very specific individuals. Not all people know what they need. If they do, they may struggle to express themselvers. This can make it hard for others to understand the issue.But for others, it could be the best way to start the conversation. Since it&#8217;s not the usual flow, I've left the not-so-obvious point for the end.&#9888;&#65039; DANGER &#9888;&#65039;: Sometimes this would be the same as asking &#8220;What would you write in a Christmas Card?&#8221; and it could create an undesired expectations cocktail. &#129315;Think of it as a funnel where you keep digging and digging&#8230;Thank you for reading The Pipe & The Line. If you&#8217;ve liked it so far, feel free to share it!Share&#128121; The Final Boss: the &#8220;one quick report&#8221; avalancheIn past experiences working in Analytics, I&#8217;ve watched this show from the 1st row, the usual sequence being:Requester: \"Hey, we need this quick report to see how many monthly users we have per country.\"Data person: &#8220;Hey, I can have it for tomorrow; I need to wrap up some other things first.&#8221;Requester: &#8220;I need a quick .csv for a meeting taking place in 2 hours.\"Data person: \"(5&#8217; later) Okay, there you go.\"Requester: &#8220;Can you also add monthly actions?&#8221;Data person: &#8220;Done.&#8221;Requester: &#8220;Can you also add the client type and the weather forecast for tomorrow?&#8221;Data person: &#8220;We don&#8217;t have that kind of logic.&#8221;Requester: \"Oh, okay, I don&#8217;t need it anyway. Thanks.&#8221;If this were a film, we could name it: &#8220;Harry Potter and the Mystery of why they cannot ask for everything all at once.\"The real question we should always ask, and fight for, is: Data person: &#8220;Hey, what&#8217;s the use case for it? What&#8217;s the deadline? Who&#8217;s going to use it? What questions do we want to answer?&#8221;You must balance flexibility, speed, and realistic scenarios. Otherwise, someone might blame the Data team for artificial bottlenecks. Thanks for reading The Pipe & The Line! Subscribe for free to receive new data engineering stuff and useful learnings for your data career!&#129533; My approach to killing the monster in the long termCatch patterns. If you always get the same request at the end of the month, automate it right away. Don&#8217;t be lazy. Don&#8217;t build without context. Just sending .csv exports won&#8217;t be enough; there&#8217;s always a bigger picture that you might be missing if you only react. Context is king. Set up metadata reporting. Know who is using what and at what frequency. This provides abundant context without requiring direct inquiry on end users. All-in Housekeeping. If you pay attention to what&#8217;s used and what&#8217;s not, you know what&#8217;s irrelevant and needs to be removed. Probably in this process you will find mutiple one-shot reports that ended up in analytics oblivion. Test your audience. What happens if you archive an not so unused report without announcing it? Please, archive without deleting. The ghosts might return and some of those reports could help in the future. Remove what&#8217;s causing noise. They will end up using only what they need. Incredibly, when we have 20 dashboards we can check, we check none of them. But having 3 might lift usage and adoption. &#128221; TL;DR&#127916;Setting the scene Provide an agendaLet them talkUse an AI Note Taker or only takes notes of things AI won&#8217;t catch. Eg. A Data Product opportunity&#10067;Asking questions that will make a difference &#128197; What are your daily tasks? Get more context on their workflows.&#129354; What are your daily challenges? Find blockers that can be solved with Data. &#128165; Let&#8217;s talk about pain. Understand what is not being said.&#128214; Were you aware of&#8230;? Let them know about the data assets they can use periodically.&#128587;&#127995;&#8205;&#9794;&#65039; What do you need? Straightforward option for the ones that clearly know what they need.&#128121; The Final Boss: the &#8220;one quick report&#8221; avalanche&#129533; My approach to killing the monster in the long-termCatch patterns and solve them at scaleDon&#8217;t build without context. Try not working blindy by doing one quick requests all the time. Set up metadata reporting. To scout heavy users and get the best out of them. All-in Housekeeping. To really know what they use and what they don&#8217;t.Test your audience. Try breaking change for forgotten assets, what could happen? Archive without deleting. Remove what&#8217;s causing noise. It&#8217;s more likely they use 2 when they have only 2 than 2 when they have 20 dashboards. If you enjoyed the content, hit the like &#10084;&#65039; button, share, comment, repost, and all those nice things people do when like stuff these days. Glad to know you made it to this part! Hi, I am Alejandro Aboy. I am currently working as a Data Engineer. I started in digital marketing at 19. I gained experience in website tracking, advertising, and analytics. I also founded my agency. In 2021, I found my passion for data engineering. So, I shifted my career focus, despite lacking a CS degree. I'm now pursuing this path, leveraging my diverse experience and willingness to learn.Follow me on Linkedin"}]}
{"messages": [{"role": "user", "content": "Discuss strategies for building a data culture within small teams to empower proactive problem-solving."}, {"role": "assistant", "content": "If you work in data, especially in a small team, you know the common fear. It's not due to a lack of experience or knowledge. We fear it because it leads to endless problems.But we shouldn&#8217;t fear it&#8230;There is hope at the end of the pipeline (see what I did there?). The usual advice is to \"have initiative, be proactive, and be one step ahead.\" but it's hard to follow because context defines 99% of it.You can find nice templates and frameworks, but simple questions for end users are better, in my experience.As a technical contributor, you might dislike talking with non-technical coworkers in your company. But you'll need to learn to speak non-technical at some point.If you ask them the right questions, they'll make better, fewer or even more qualified requests next time. Let&#8217;s get started. &#127916;Setting the scene I am assuming you will be talking with someone you don&#8217;t usually interact with. Even for small companies, it is likely that the other person will feel less open at the beginning and you will have to break the ice. My best pieces of advice for this process:Provide an agenda in advance and be straightforward about the main goal: you want to help THEM. &#8220;Hi End User, how are you doing?\"I've been planning my projects for next quarter. My backlog shows that many of your team's Metabase dashboards are loading slowly.I&#8217;ve booked a meeting for next week to chat about the dashboards you use, the metrics you track, and your usual business questions. We'll also discuss anything else that could help to improve your daily workflow.Thanks!&#8221;If you are a talker like me, this is not the moment to shine. Start the conversation but let it be to build trust. If possible, don't interrupt. Use an AI note taker or get consent to record the meeting. But pay attention. It's frustrating to discuss your issues while hearing keyboards in the background (even if muted). There is an exception: if you catch an opportunity that AI won&#8217;t, ask for a minute to write it down.Setting the scene steps.&#10067;Asking questions that will make a difference &#128197; What are your daily tasks?Most of the time, they don&#8217;t explicitly know what they need, and that is fine. If they don&#8217;t know, ask about their daily tasks. A guided tour through their work assets is sometimes enough. That will open new points for the next steps of the conversation.If you build dashboards, make them share their screens and show you.If you build automated emails with insights, ask them what they take from those.&#129354; What are your daily challenges? After the last step, ask what they cannot do because of their daily tasks. That will let you know what you could improve.There&#8217;s no need to go with the typical question &#8220;What keeps you up at night?&#8221;. These are my favorites:&#8220;What is taking up most of your time when doing X or Y?&#8221; &#8220;How do you proceed when X or Y happens?&#8221; &#8220;Is there one impactful thing you should be doing more, but something is pulling you down? If yes, what&#8217;s that thing?\"Thanks for reading The Pipe & The Line! Subscribe to get new content and learnings for your data career!&#128165; Let&#8217;s talk about painTry to understand what they are not saying. Most of the time, if someone complains about something, they are saying something else. For example:\"I have no time to check my customers one by one. It makes me unable to anticipate whether a customer will churn.\"I have no time; remove the manual task from them if possible.Check my customers: what's the criterion? Ask for them and develop something that checks it at scale.Unable to anticipate: are we supposed to do this before a specific event or time in the year? What's the ideal frequency?Whether a customer will churn: that&#8217;s a metric. Usually, all complaints are disguising a blocker, a metric, a target and the impact of the affected process. This might not apply 100% of the time, but paying attention to these concepts could help. &#128214; Were you aware of&#8230;? It's funny when someone complains about a missing thing that was available for over 6 months but never used.It's up to you, my data friend. We often don't share available assets with users but you should announce new data products. Also, regularly update others on improvements and maintenance.This will make a big difference, and the effort is quite low. Some actionable items for this point:Use clear names for assets. If everyone refers to it as &#8220;User Growth&#8221;, don't name the dashboard \"Account Activations\".Spend time writing good descriptions as if someone coming from Mars was going to read it, be detailed but don&#8217;t rewrite the bible.Use emojis to catch attention, never gets old. &#128587;&#127995;&#8205;&#9792;&#65039;&#128587;&#127995;&#8205;&#9794;&#65039; But never forget: if you communicate to provide awareness and they don&#8217;t pay attention, don&#8217;t insist. Whenever they need it, they will come back asking for it. &#128587;&#127995;&#8205;&#9794;&#65039; What do you need?Just like the heading title. Simple. But hold on, don&#8217;t get me wrong. This question usually works with very specific individuals. Not all people know what they need. If they do, they may struggle to express themselvers. This can make it hard for others to understand the issue.But for others, it could be the best way to start the conversation. Since it&#8217;s not the usual flow, I've left the not-so-obvious point for the end.&#9888;&#65039; DANGER &#9888;&#65039;: Sometimes this would be the same as asking &#8220;What would you write in a Christmas Card?&#8221; and it could create an undesired expectations cocktail. &#129315;Think of it as a funnel where you keep digging and digging&#8230;Thank you for reading The Pipe & The Line. If you&#8217;ve liked it so far, feel free to share it!Share&#128121; The Final Boss: the &#8220;one quick report&#8221; avalancheIn past experiences working in Analytics, I&#8217;ve watched this show from the 1st row, the usual sequence being:Requester: \"Hey, we need this quick report to see how many monthly users we have per country.\"Data person: &#8220;Hey, I can have it for tomorrow; I need to wrap up some other things first.&#8221;Requester: &#8220;I need a quick .csv for a meeting taking place in 2 hours.\"Data person: \"(5&#8217; later) Okay, there you go.\"Requester: &#8220;Can you also add monthly actions?&#8221;Data person: &#8220;Done.&#8221;Requester: &#8220;Can you also add the client type and the weather forecast for tomorrow?&#8221;Data person: &#8220;We don&#8217;t have that kind of logic.&#8221;Requester: \"Oh, okay, I don&#8217;t need it anyway. Thanks.&#8221;If this were a film, we could name it: &#8220;Harry Potter and the Mystery of why they cannot ask for everything all at once.\"The real question we should always ask, and fight for, is: Data person: &#8220;Hey, what&#8217;s the use case for it? What&#8217;s the deadline? Who&#8217;s going to use it? What questions do we want to answer?&#8221;You must balance flexibility, speed, and realistic scenarios. Otherwise, someone might blame the Data team for artificial bottlenecks. Thanks for reading The Pipe & The Line! Subscribe for free to receive new data engineering stuff and useful learnings for your data career!&#129533; My approach to killing the monster in the long termCatch patterns. If you always get the same request at the end of the month, automate it right away. Don&#8217;t be lazy. Don&#8217;t build without context. Just sending .csv exports won&#8217;t be enough; there&#8217;s always a bigger picture that you might be missing if you only react. Context is king. Set up metadata reporting. Know who is using what and at what frequency. This provides abundant context without requiring direct inquiry on end users. All-in Housekeeping. If you pay attention to what&#8217;s used and what&#8217;s not, you know what&#8217;s irrelevant and needs to be removed. Probably in this process you will find mutiple one-shot reports that ended up in analytics oblivion. Test your audience. What happens if you archive an not so unused report without announcing it? Please, archive without deleting. The ghosts might return and some of those reports could help in the future. Remove what&#8217;s causing noise. They will end up using only what they need. Incredibly, when we have 20 dashboards we can check, we check none of them. But having 3 might lift usage and adoption. &#128221; TL;DR&#127916;Setting the scene Provide an agendaLet them talkUse an AI Note Taker or only takes notes of things AI won&#8217;t catch. Eg. A Data Product opportunity&#10067;Asking questions that will make a difference &#128197; What are your daily tasks? Get more context on their workflows.&#129354; What are your daily challenges? Find blockers that can be solved with Data. &#128165; Let&#8217;s talk about pain. Understand what is not being said.&#128214; Were you aware of&#8230;? Let them know about the data assets they can use periodically.&#128587;&#127995;&#8205;&#9794;&#65039; What do you need? Straightforward option for the ones that clearly know what they need.&#128121; The Final Boss: the &#8220;one quick report&#8221; avalanche&#129533; My approach to killing the monster in the long-termCatch patterns and solve them at scaleDon&#8217;t build without context. Try not working blindy by doing one quick requests all the time. Set up metadata reporting. To scout heavy users and get the best out of them. All-in Housekeeping. To really know what they use and what they don&#8217;t.Test your audience. Try breaking change for forgotten assets, what could happen? Archive without deleting. Remove what&#8217;s causing noise. It&#8217;s more likely they use 2 when they have only 2 than 2 when they have 20 dashboards. If you enjoyed the content, hit the like &#10084;&#65039; button, share, comment, repost, and all those nice things people do when like stuff these days. Glad to know you made it to this part! Hi, I am Alejandro Aboy. I am currently working as a Data Engineer. I started in digital marketing at 19. I gained experience in website tracking, advertising, and analytics. I also founded my agency. In 2021, I found my passion for data engineering. So, I shifted my career focus, despite lacking a CS degree. I'm now pursuing this path, leveraging my diverse experience and willingness to learn.Follow me on Linkedin"}]}
{"messages": [{"role": "user", "content": "Create a step-by-step guide for developing data assets that involve user feedback throughout the process."}, {"role": "assistant", "content": "If you work in data, especially in a small team, you know the common fear. It's not due to a lack of experience or knowledge. We fear it because it leads to endless problems.But we shouldn&#8217;t fear it&#8230;There is hope at the end of the pipeline (see what I did there?). The usual advice is to \"have initiative, be proactive, and be one step ahead.\" but it's hard to follow because context defines 99% of it.You can find nice templates and frameworks, but simple questions for end users are better, in my experience.As a technical contributor, you might dislike talking with non-technical coworkers in your company. But you'll need to learn to speak non-technical at some point.If you ask them the right questions, they'll make better, fewer or even more qualified requests next time. Let&#8217;s get started. &#127916;Setting the scene I am assuming you will be talking with someone you don&#8217;t usually interact with. Even for small companies, it is likely that the other person will feel less open at the beginning and you will have to break the ice. My best pieces of advice for this process:Provide an agenda in advance and be straightforward about the main goal: you want to help THEM. &#8220;Hi End User, how are you doing?\"I've been planning my projects for next quarter. My backlog shows that many of your team's Metabase dashboards are loading slowly.I&#8217;ve booked a meeting for next week to chat about the dashboards you use, the metrics you track, and your usual business questions. We'll also discuss anything else that could help to improve your daily workflow.Thanks!&#8221;If you are a talker like me, this is not the moment to shine. Start the conversation but let it be to build trust. If possible, don't interrupt. Use an AI note taker or get consent to record the meeting. But pay attention. It's frustrating to discuss your issues while hearing keyboards in the background (even if muted). There is an exception: if you catch an opportunity that AI won&#8217;t, ask for a minute to write it down.Setting the scene steps.&#10067;Asking questions that will make a difference &#128197; What are your daily tasks?Most of the time, they don&#8217;t explicitly know what they need, and that is fine. If they don&#8217;t know, ask about their daily tasks. A guided tour through their work assets is sometimes enough. That will open new points for the next steps of the conversation.If you build dashboards, make them share their screens and show you.If you build automated emails with insights, ask them what they take from those.&#129354; What are your daily challenges? After the last step, ask what they cannot do because of their daily tasks. That will let you know what you could improve.There&#8217;s no need to go with the typical question &#8220;What keeps you up at night?&#8221;. These are my favorites:&#8220;What is taking up most of your time when doing X or Y?&#8221; &#8220;How do you proceed when X or Y happens?&#8221; &#8220;Is there one impactful thing you should be doing more, but something is pulling you down? If yes, what&#8217;s that thing?\"Thanks for reading The Pipe & The Line! Subscribe to get new content and learnings for your data career!&#128165; Let&#8217;s talk about painTry to understand what they are not saying. Most of the time, if someone complains about something, they are saying something else. For example:\"I have no time to check my customers one by one. It makes me unable to anticipate whether a customer will churn.\"I have no time; remove the manual task from them if possible.Check my customers: what's the criterion? Ask for them and develop something that checks it at scale.Unable to anticipate: are we supposed to do this before a specific event or time in the year? What's the ideal frequency?Whether a customer will churn: that&#8217;s a metric. Usually, all complaints are disguising a blocker, a metric, a target and the impact of the affected process. This might not apply 100% of the time, but paying attention to these concepts could help. &#128214; Were you aware of&#8230;? It's funny when someone complains about a missing thing that was available for over 6 months but never used.It's up to you, my data friend. We often don't share available assets with users but you should announce new data products. Also, regularly update others on improvements and maintenance.This will make a big difference, and the effort is quite low. Some actionable items for this point:Use clear names for assets. If everyone refers to it as &#8220;User Growth&#8221;, don't name the dashboard \"Account Activations\".Spend time writing good descriptions as if someone coming from Mars was going to read it, be detailed but don&#8217;t rewrite the bible.Use emojis to catch attention, never gets old. &#128587;&#127995;&#8205;&#9792;&#65039;&#128587;&#127995;&#8205;&#9794;&#65039; But never forget: if you communicate to provide awareness and they don&#8217;t pay attention, don&#8217;t insist. Whenever they need it, they will come back asking for it. &#128587;&#127995;&#8205;&#9794;&#65039; What do you need?Just like the heading title. Simple. But hold on, don&#8217;t get me wrong. This question usually works with very specific individuals. Not all people know what they need. If they do, they may struggle to express themselvers. This can make it hard for others to understand the issue.But for others, it could be the best way to start the conversation. Since it&#8217;s not the usual flow, I've left the not-so-obvious point for the end.&#9888;&#65039; DANGER &#9888;&#65039;: Sometimes this would be the same as asking &#8220;What would you write in a Christmas Card?&#8221; and it could create an undesired expectations cocktail. &#129315;Think of it as a funnel where you keep digging and digging&#8230;Thank you for reading The Pipe & The Line. If you&#8217;ve liked it so far, feel free to share it!Share&#128121; The Final Boss: the &#8220;one quick report&#8221; avalancheIn past experiences working in Analytics, I&#8217;ve watched this show from the 1st row, the usual sequence being:Requester: \"Hey, we need this quick report to see how many monthly users we have per country.\"Data person: &#8220;Hey, I can have it for tomorrow; I need to wrap up some other things first.&#8221;Requester: &#8220;I need a quick .csv for a meeting taking place in 2 hours.\"Data person: \"(5&#8217; later) Okay, there you go.\"Requester: &#8220;Can you also add monthly actions?&#8221;Data person: &#8220;Done.&#8221;Requester: &#8220;Can you also add the client type and the weather forecast for tomorrow?&#8221;Data person: &#8220;We don&#8217;t have that kind of logic.&#8221;Requester: \"Oh, okay, I don&#8217;t need it anyway. Thanks.&#8221;If this were a film, we could name it: &#8220;Harry Potter and the Mystery of why they cannot ask for everything all at once.\"The real question we should always ask, and fight for, is: Data person: &#8220;Hey, what&#8217;s the use case for it? What&#8217;s the deadline? Who&#8217;s going to use it? What questions do we want to answer?&#8221;You must balance flexibility, speed, and realistic scenarios. Otherwise, someone might blame the Data team for artificial bottlenecks. Thanks for reading The Pipe & The Line! Subscribe for free to receive new data engineering stuff and useful learnings for your data career!&#129533; My approach to killing the monster in the long termCatch patterns. If you always get the same request at the end of the month, automate it right away. Don&#8217;t be lazy. Don&#8217;t build without context. Just sending .csv exports won&#8217;t be enough; there&#8217;s always a bigger picture that you might be missing if you only react. Context is king. Set up metadata reporting. Know who is using what and at what frequency. This provides abundant context without requiring direct inquiry on end users. All-in Housekeeping. If you pay attention to what&#8217;s used and what&#8217;s not, you know what&#8217;s irrelevant and needs to be removed. Probably in this process you will find mutiple one-shot reports that ended up in analytics oblivion. Test your audience. What happens if you archive an not so unused report without announcing it? Please, archive without deleting. The ghosts might return and some of those reports could help in the future. Remove what&#8217;s causing noise. They will end up using only what they need. Incredibly, when we have 20 dashboards we can check, we check none of them. But having 3 might lift usage and adoption. &#128221; TL;DR&#127916;Setting the scene Provide an agendaLet them talkUse an AI Note Taker or only takes notes of things AI won&#8217;t catch. Eg. A Data Product opportunity&#10067;Asking questions that will make a difference &#128197; What are your daily tasks? Get more context on their workflows.&#129354; What are your daily challenges? Find blockers that can be solved with Data. &#128165; Let&#8217;s talk about pain. Understand what is not being said.&#128214; Were you aware of&#8230;? Let them know about the data assets they can use periodically.&#128587;&#127995;&#8205;&#9794;&#65039; What do you need? Straightforward option for the ones that clearly know what they need.&#128121; The Final Boss: the &#8220;one quick report&#8221; avalanche&#129533; My approach to killing the monster in the long-termCatch patterns and solve them at scaleDon&#8217;t build without context. Try not working blindy by doing one quick requests all the time. Set up metadata reporting. To scout heavy users and get the best out of them. All-in Housekeeping. To really know what they use and what they don&#8217;t.Test your audience. Try breaking change for forgotten assets, what could happen? Archive without deleting. Remove what&#8217;s causing noise. It&#8217;s more likely they use 2 when they have only 2 than 2 when they have 20 dashboards. If you enjoyed the content, hit the like &#10084;&#65039; button, share, comment, repost, and all those nice things people do when like stuff these days. Glad to know you made it to this part! Hi, I am Alejandro Aboy. I am currently working as a Data Engineer. I started in digital marketing at 19. I gained experience in website tracking, advertising, and analytics. I also founded my agency. In 2021, I found my passion for data engineering. So, I shifted my career focus, despite lacking a CS degree. I'm now pursuing this path, leveraging my diverse experience and willingness to learn.Follow me on Linkedin"}]}
